{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ea515a-03df-4412-abd3-820fd69dee05",
   "metadata": {},
   "source": [
    "# 08f - Vertex AI > Training > Training Pipelines - With Custom Container\n",
    "\n",
    "# <IN ACTIVE DEVELOPMENT - NOT COMPLETE>\n",
    "\n",
    "Dev Notes:\n",
    "- Python Kernel\n",
    "- Orchestrates Vertex AI Services Sequentially\n",
    "\n",
    "Workflow:\n",
    "- code to script\n",
    "- script to container\n",
    "- model to Vertex AI Model Registry\n",
    "- predictions with Vertex Ai: Batch and Online\n",
    "\n",
    "Next Steps:\n",
    "- Pipeline to conduct steps\n",
    "- use [reticulate](https://rstudio.github.io/reticulate/) for an R centric workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b23e63-6a03-4b11-839e-5a2a9ea0ef6e",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d2426-f159-4d70-930b-0865e123ba9d",
   "metadata": {},
   "source": [
    "### Package Installs (if needed)\n",
    "\n",
    "This notebook uses the Python Clients for\n",
    "- Google Service Usage\n",
    "    - to enable APIs (Artifact Registry and Cloud Build)\n",
    "- Artifact Registry\n",
    "    - to create repositories for Python packages and Docker containers\n",
    "- Cloud Build\n",
    "    - To build custom Docker containers\n",
    "\n",
    "The cells below check to see if the required Python libraries are installed.  If any are not it will print a message to do the install with the associated pip command to use.  These installs must be completed before continuing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a2cfa5-76a8-4cf7-a7dd-77a9406a3ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.cloud.service_usage_v1\n",
    "except ImportError:\n",
    "    print('You need to pip install google-cloud-service-usage')\n",
    "    !pip install google-cloud-service-usage -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "474394ea-08ea-4327-b287-1c5c84108b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.cloud.artifactregistry_v1\n",
    "except ImportError:\n",
    "    print('You need to pip install google-cloud-artifact-registry')\n",
    "    !pip install google-cloud-artifact-registry -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c8ceb7-ad0e-438b-8191-8568eec8a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.cloud.devtools.cloudbuild\n",
    "except ImportError:\n",
    "    print('You need to pip install google-cloud-build')\n",
    "    !pip install google-cloud-build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e76d56-99cc-4f29-98c1-edaf6bf569d0",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d854258-fc2d-4dce-b65f-aaa666917230",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7f9f258-f67d-4b6d-998b-09ba384974ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statmike-mlops-349915'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9126be55-8d72-4012-9ece-8d2f52a85ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "EXPERIMENT = '08f'\n",
    "SERIES = '08'\n",
    "\n",
    "# source data\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = 'fraud'\n",
    "BQ_TABLE = 'fraud_prepped'\n",
    "\n",
    "# Resources\n",
    "TRAIN_COMPUTE = 'n1-standard-4'\n",
    "DEPLOY_COMPUTE = 'n1-standard-4'\n",
    "\n",
    "# Model Training\n",
    "VAR_TARGET = 'Class'\n",
    "VAR_OMIT = 'transaction_id' # add more variables to the string with space delimiters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1341dda-9153-457e-be08-20a95c0abe86",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2120141-22d6-49c3-9b28-25f103b08b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "import os, shutil, glob\n",
    "import pkg_resources\n",
    "from IPython.display import Markdown as md\n",
    "from google.cloud import service_usage_v1\n",
    "from google.cloud.devtools import cloudbuild_v1\n",
    "from google.cloud import artifactregistry_v1\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e46d467-96e2-446a-84f1-16e181210000",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c00fa76-a5ab-4c96-bb4c-a34a37ecf615",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "bq = bigquery.Client()\n",
    "gcs = storage.Client()\n",
    "su_client = service_usage_v1.ServiceUsageClient()\n",
    "ar_client = artifactregistry_v1.ArtifactRegistryClient()\n",
    "cb_client = cloudbuild_v1.CloudBuildClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc3615-1571-4e65-b3d1-293cc09112bf",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e79c9215-4558-4e28-af76-4373b4929fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET = PROJECT_ID\n",
    "URI = f\"gs://{BUCKET}/{SERIES}/{EXPERIMENT}\"\n",
    "DIR = f\"temp/{EXPERIMENT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcc5c459-13db-42df-8366-8cecd7474485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1026793852137-compute@developer.gserviceaccount.com'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f54c8-7364-46c7-b8db-feb922de4631",
   "metadata": {},
   "source": [
    "List the service accounts current roles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85e56b6c-36b2-45ba-9aa0-baf887c58636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROLE\n",
      "roles/bigquery.admin\n",
      "roles/owner\n",
      "roles/run.admin\n",
      "roles/storage.objectAdmin\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects get-iam-policy $PROJECT_ID --filter=\"bindings.members:$SERVICE_ACCOUNT\" --format='table(bindings.role)' --flatten=\"bindings[].members\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f63bb4a-be89-41dc-81e4-71ec5d538861",
   "metadata": {},
   "source": [
    ">Note: If the resulting list is missing [roles/storage.objectAdmin](https://cloud.google.com/storage/docs/access-control/iam-roles) then [revisit the setup notebook](../00%20-%20Setup/00%20-%20Environment%20Setup.ipynb#permissions) and add this permission to the service account with the provided instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f2704-1aa0-4921-925b-3e2859de5563",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64f7a268-9d00-47aa-9a00-362650e8add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {DIR}\n",
    "!mkdir -p {DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1040a55-82cb-4c34-a3ee-7af94d07e73e",
   "metadata": {},
   "source": [
    "Experiment Tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a86cee3-c059-48de-8a43-ccf99ff0fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMEWORK = 'r'\n",
    "TASK = 'classification'\n",
    "MODEL_TYPE = 'logistic_regression'\n",
    "EXPERIMENT_NAME = f'experiment-{SERIES}-{EXPERIMENT}-{FRAMEWORK}-{TASK}-{MODEL_TYPE}'\n",
    "RUN_NAME = f'run-{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5af49-1e70-4349-a740-7989ea528e24",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Enable APIs\n",
    "\n",
    "Using Cloud Build and Artifact Registry requires enabling these APIs for the Google Cloud Project.\n",
    "\n",
    "Options for enabeling these.  In this notebook option 2 is used.\n",
    " 1. Use the APIs & Services page in the console: https://console.cloud.google.com/apis\n",
    "     - `+ Enable APIs and Services`\n",
    "     - Search for Cloud Build and Enable\n",
    "     - Search for Artifact Registry and Enable\n",
    " 2. Use [Google Service Usage](https://cloud.google.com/service-usage/docs) API from Python\n",
    "     - [Python Client For Service Usage](https://github.com/googleapis/python-service-usage)\n",
    "     - [Python Client Library Documentation](https://cloud.google.com/python/docs/reference/serviceusage/latest)\n",
    "     \n",
    "The following code cells use the Service Usage Client to:\n",
    "- get the state of the service\n",
    "- if 'DISABLED':\n",
    "    - Try enabling the service and return the state after trying\n",
    "- if 'ENABLED' print the state for confirmation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941628d-6ddf-4e3c-ba5d-8cea56ec6780",
   "metadata": {},
   "source": [
    "#### Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57c9e824-ae7a-4fac-99e7-2e4d4bcfa3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact Registry already enabled for project: statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "artifactregistry = su_client.get_service(\n",
    "    request = service_usage_v1.GetServiceRequest(\n",
    "        name = f'projects/{PROJECT_ID}/services/artifactregistry.googleapis.com'\n",
    "    )\n",
    ").state.name\n",
    "\n",
    "\n",
    "if artifactregistry == 'DISABLED':\n",
    "    print(f'Artifact Registry is currently {artifactregistry} for project: {PROJECT_ID}')\n",
    "    print(f'Trying to Enable...')\n",
    "    operation = su_client.enable_service(\n",
    "        request = service_usage_v1.EnableServiceRequest(\n",
    "            name = f'projects/{PROJECT_ID}/services/artifactregistry.googleapis.com'\n",
    "        )\n",
    "    )\n",
    "    response = operation.result()\n",
    "    if response.service.state.name == 'ENABLED':\n",
    "        print(f'Artifact Registry is now enabled for project: {PROJECT_ID}')\n",
    "    else:\n",
    "        print(response)\n",
    "else:\n",
    "    print(f'Artifact Registry already enabled for project: {PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e3f6bf-c4ec-4315-84e7-5e2f473d5e4d",
   "metadata": {},
   "source": [
    "#### Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de495fb8-0ef8-41bc-b028-d544911a373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloud Build already enabled for project: statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "cloudbuild = su_client.get_service(\n",
    "    request = service_usage_v1.GetServiceRequest(\n",
    "        name = f'projects/{PROJECT_ID}/services/cloudbuild.googleapis.com'\n",
    "    )\n",
    ").state.name\n",
    "\n",
    "\n",
    "if cloudbuild == 'DISABLED':\n",
    "    print(f'Cloud Build is currently {cloudbuild} for project: {PROJECT_ID}')\n",
    "    print(f'Trying to Enable...')\n",
    "    operation = su_client.enable_service(\n",
    "        request = service_usage_v1.EnableServiceRequest(\n",
    "            name = f'projects/{PROJECT_ID}/services/cloudbuild.googleapis.com'\n",
    "        )\n",
    "    )\n",
    "    response = operation.result()\n",
    "    if response.service.state.name == 'ENABLED':\n",
    "        print(f'Cloud Build is now enabled for project: {PROJECT_ID}')\n",
    "    else:\n",
    "        print(response)\n",
    "else:\n",
    "    print(f'Cloud Build already enabled for project: {PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7edfb9b-c4ba-446c-b7f1-31f2152df7d0",
   "metadata": {},
   "source": [
    "---\n",
    "## Training & Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbff3cf5-6752-4a17-8de6-98d7b3aa7c18",
   "metadata": {},
   "source": [
    "### R Script for Training\n",
    "\n",
    "This notebook trains the same R model from [08 - Vertex AI Custom Model - R - in Notebook](./08%20-%20Vertex%20AI%20Custom%20Model%20-%20R%20-%20in%20Notebook.ipynb) by first modifying and saving the training code to an R script as shown in [08 - Vertex Ai Custom Model - R - Notebook to Script](08%20-%20Vertex%20AI%20Custom%20Model%20-%20R%20-%20Notebook%20to%20Script.ipynb) which stores the script in [`./code/train.R`](./code/train.R).\n",
    "\n",
    "**Review the script:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c7959c5-1bb0-40e8-9dbc-d4212b3b52f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```R\n",
       "\n",
       "\n",
       "# library import\n",
       "library(bigrquery)\n",
       "library(dplyr)\n",
       "\n",
       "# inputs\n",
       "args <- commandArgs(trailingOnly = TRUE)\n",
       "project_id <- args[1]\n",
       "region <- args[2]\n",
       "experiment <- args[3]\n",
       "series <- args[4]\n",
       "bq_project <- args[5]\n",
       "bq_dataset <- args[6]\n",
       "bq_table <- args[7]\n",
       "var_target <- args[8]\n",
       "var_omit <- args[9]\n",
       "\n",
       "# data source\n",
       "get_data <- function(s){\n",
       "    query = sprintf('SELECT * EXCEPT(%s, splits) FROM `%s.%s.%s` WHERE splits = \"%s\"', var_omit, bq_project, bq_dataset, bq_table, s)\n",
       "    table <- bq_project_query(bq_project, query)\n",
       "    ds <- bq_table_download(table)\n",
       "    return(ds)\n",
       "}\n",
       "train <- get_data(\"TRAIN\")\n",
       "test <- get_data(\"TEST\")\n",
       "\n",
       "# logistic regression model\n",
       "model <- glm(\n",
       "    Class ~ .,\n",
       "    data = train,\n",
       "    family = binomial)\n",
       "\n",
       "# predictions for evaluation\n",
       "preds <- predict(model, test, type = \"response\")\n",
       "\n",
       "# evaluate\n",
       "actual <- test[, var_target]\n",
       "names(actual) <- 'actual'\n",
       "pred <- tibble(round(preds))\n",
       "names(pred) <- 'pred'\n",
       "results <- cbind(actual, pred)\n",
       "cm <- table(results)\n",
       "\n",
       "# save model to file\n",
       "saveRDS(model, \"model.rds\")\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCRIPT_PATH = './code/train.R'\n",
    "\n",
    "with open(SCRIPT_PATH, 'r') as file:\n",
    "    data = file.read()\n",
    "md(f\"```R\\n\\n{data}\\n```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c40e87-ec43-435d-a3c5-1ac64e268f6f",
   "metadata": {},
   "source": [
    "Make a copy of the script in the notebooks temp folder and append code for saving to GCS model directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e42e4f07-e9b3-4c9f-829e-433977cec8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./temp/08f/train.R'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copyfile(SCRIPT_PATH, f'./{DIR}/train.R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b562480-2a71-44cb-bc03-35cb936fa178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./temp/08f/train.R\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a './temp/08f/train.R'\n",
    "\n",
    "# use Vertex AI Training Pre-Defined Environment Variables to Write to GCS\n",
    "Sys.getenv()\n",
    "system2('gsutil', c('cp', 'model.rds', Sys.getenv('AIP_MODEL_DIR')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e4fd6-d155-4799-b2e4-dbbd00dc7149",
   "metadata": {},
   "source": [
    "### R Script for Serving\n",
    "\n",
    "To serve the model, another script that uses [plumber](https://www.rplumber.io/) is created:\n",
    "\n",
    "**Review the script:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8139e2f4-1e9c-4391-978b-740fb5b69d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./temp/08f/serve.R\n"
     ]
    }
   ],
   "source": [
    "%%writefile './temp/08f/serve.R'\n",
    "\n",
    "# library import\n",
    "library(plumber)\n",
    "\n",
    "# use Vertex AI Training Pre-Defined Environment Variables to Read from GCS\n",
    "Sys.getenv()\n",
    "system2('gsutil', c('cp', '-r', Sys.getenv('AIP_STORAGE_URI'), '.'))\n",
    "\n",
    "# import model\n",
    "model <- readRDS('artifacts/model.rds')\n",
    "\n",
    "# prediction route function\n",
    "predict_route <- function(req, res){\n",
    "    print(\"Processing Prediction Request...\")\n",
    "    df <- as.data.frame(req$body$instances)\n",
    "    preds <- predict(model, df, type = \"response\")\n",
    "    return(list(predictions = preds))\n",
    "}\n",
    "\n",
    "# serving\n",
    "print(\"Start Serving Process...\")\n",
    "pr() %>%\n",
    "    pr_get(Sys.getenv(\"AIP_HEALTH_ROUTE\"), function() \"OK\") %>%\n",
    "    pr_post(Sys.getenv(\"AIP_PREDICT_ROUTE\"), predict_route) %>%\n",
    "    pr_run(host = \"0.0.0.0\", port=as.integer(Sys.getenv(\"AIP_HTTP_PORT\", 8080)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84edff-2733-4bc4-9046-c884ae3d8af4",
   "metadata": {},
   "source": [
    "### Creating a Custom Container with Cloud Build\n",
    "\n",
    "Cloud Build creates and manages the build on GCP.  The API creates a build by providing:\n",
    "- location of the source\n",
    "- instructions\n",
    "- location to store the built artifacts\n",
    "\n",
    "The instruction part of Cloud Build has options:\n",
    "- Dockerfile\n",
    "- Build Config file (YAML or JSON)\n",
    "- Cloud Native Buildpacks\n",
    "\n",
    "This notebook uses the approach of using the Python Client for Cloud Build and not referencing any local files.  For that reason, the first step is creating a Dockerfile for the workflow and storing it in GCS. The next step is running Cloud Build and using the client to specify the Build config rather than a config file.  The steps of the build config start with getting the code (git clone, or copy from GCS) and copying the Dockerfile.  \n",
    "\n",
    "There are many workflows for creating containers with ML training code.  Many of the most common ones are explored in the tips notebook [Python Custom Containers](../Tips/Python%20Custom%20Containers.ipynb).  The method used here is the simplest - copy the training code directly into the container.  The other methods include packaging the training code as a Python Distribution and using `pip install` in from GCS, GitHub and even Artifact Registry as a private repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d9715-b173-45a0-9c81-07d06931d214",
   "metadata": {},
   "source": [
    "#### Create the Dockerfile\n",
    "A basic dockerfile thats take the base image and copies the code in and add additional installs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7cbba22-9a30-49b7-9f26-be61455aa635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./temp/08f/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile './temp/08f/Dockerfile'\n",
    "FROM gcr.io/deeplearning-platform-release/r-cpu.4-1:latest\n",
    "\n",
    "WORKDIR /root\n",
    "\n",
    "# copy requirements and install them\n",
    "COPY train.R /root/train.R\n",
    "COPY serve.R /root/serve.R\n",
    "\n",
    "RUN apt-get update\n",
    "RUN apt_get install gfortran -yy\n",
    "RUN R -e 'install.packages(c{\"plumber\"})'\n",
    "\n",
    "EXPOSE 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e1240b-8dd0-4a69-baf3-2463ac066b81",
   "metadata": {},
   "source": [
    "#### Store Resources in Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c60bbd7-4f9b-49b0-890b-4a6d218a9fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.R', 'serve.R', '.ipynb_checkpoints', 'Dockerfile']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "46e23aa6-2a6d-426c-bc63-741c79a0f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = gcs.lookup_bucket(PROJECT_ID)\n",
    "SOURCEPATH = f'{SERIES}/{EXPERIMENT}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d2b4366a-7d27-49a9-b1d7-dc0aebbd1c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.R\n",
      "serve.R\n",
      "Dockerfile\n"
     ]
    }
   ],
   "source": [
    "for file in [f for f in os.listdir(DIR) if not f.startswith('.')]:\n",
    "    print(file)\n",
    "    blob = bucket.blob(f'{SOURCEPATH}/{file}')\n",
    "    blob.upload_from_filename(f'{DIR}/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c24d32d8-2fed-4b21-859f-0f80d5eca80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Blob: statmike-mlops-349915, 08/08f/Dockerfile, 1665948395160783>,\n",
       " <Blob: statmike-mlops-349915, 08/08f/serve.R, 1665948395117160>,\n",
       " <Blob: statmike-mlops-349915, 08/08f/train.R, 1665948395032856>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bucket.list_blobs(prefix = SOURCEPATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea615428-46bc-4477-b11f-afba7a77c5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the bucket directly here:\n",
      "https://console.cloud.google.com/storage/browser/statmike-mlops-349915/08/08f;tab=objects&project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "print(f\"View the bucket directly here:\\nhttps://console.cloud.google.com/storage/browser/{PROJECT_ID}/{SOURCEPATH};tab=objects&project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe928aa4-a6aa-4ae7-a8e9-8b410762c433",
   "metadata": {},
   "source": [
    "#### Setup Artifact Registry\n",
    "\n",
    "Artifact registry organizes artifacts with repositories.  Each repository contains packages and is designated to hold a partifcular format of package: Docker images, Python Packages and [others](https://cloud.google.com/artifact-registry/docs/supported-formats#package)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09ed5f6-1d69-4fdb-83b9-c782d884d30a",
   "metadata": {},
   "source": [
    "##### List Repositories\n",
    "\n",
    "This may be empty if no repositories have been created for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be78242d-ed78-410f-9942-485e72114418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915\n",
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-docker\n",
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-python\n"
     ]
    }
   ],
   "source": [
    "for repo in ar_client.list_repositories(parent = f'projects/{PROJECT_ID}/locations/{REGION}'):\n",
    "    print(repo.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4563bd94-37e1-4b40-a023-1ce4d2ecfe05",
   "metadata": {},
   "source": [
    "#### Create Docker Image Repository\n",
    "\n",
    "Create an Artifact Registry Repository to hold Docker Images created by this notebook.  First, check to see if it is already created by a previous run and retrieve it if it has.  Otherwise, create!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f4399f2-28f3-44af-af27-2f0a0965623e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved existing repo: projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-docker\n"
     ]
    }
   ],
   "source": [
    "docker_repo = None\n",
    "for repo in ar_client.list_repositories(parent = f'projects/{PROJECT_ID}/locations/{REGION}'):\n",
    "    if f'{PROJECT_ID}-docker' in repo.name:\n",
    "        docker_repo = repo\n",
    "        print(f'Retrieved existing repo: {docker_repo.name}')\n",
    "\n",
    "if not docker_repo:\n",
    "    operation = ar_client.create_repository(\n",
    "        request = artifactregistry_v1.CreateRepositoryRequest(\n",
    "            parent = f'projects/{PROJECT_ID}/locations/{REGION}',\n",
    "            repository_id = f'{PROJECT_ID}-docker',\n",
    "            repository = artifactregistry_v1.Repository(\n",
    "                description = f'A repository for the {EXPERIMENT} experiment that holds docker images.',\n",
    "                name = f'{PROJECT_ID}-docker',\n",
    "                format_ = artifactregistry_v1.Repository.Format.DOCKER,\n",
    "                labels = {'series': SERIES, 'experiment': EXPERIMENT}\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print('Creating Repository ...')\n",
    "    docker_repo = operation.result()\n",
    "    print(f'Completed creating repo: {docker_repo.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d2d6bcb-564a-4ce3-92b0-4aa90db6ea7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-docker',\n",
       " 'DOCKER')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docker_repo.name, docker_repo.format_.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f41f170c-3c6e-4c01-9b30-e444862915c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOSITORY = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{docker_repo.name.split('/')[-1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0142cfef-3a9f-4b09-b241-a89e36a736db",
   "metadata": {},
   "source": [
    "#### Build Custom Container\n",
    "Use the Cloud Build client to construct and run the build instructions.  Here the files collected in GCS are copied to the build instance, then the Docker build in run in the folder with the `Dockerfile`.  The resulting image is pushed to Artifact Registry (setup above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea41e5d7-79c4-43f9-bfe9-29077e5bc5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the build config with empty list of steps - these will be added sequentially\n",
    "build = cloudbuild_v1.Build(\n",
    "    steps = []\n",
    ")\n",
    "# retrieve the source\n",
    "build.steps.append(\n",
    "    {\n",
    "        'name': 'gcr.io/cloud-builders/gsutil',\n",
    "        'args': ['cp', '-r', f'gs://{PROJECT_ID}/{SOURCEPATH}/*', '/workspace']\n",
    "    }\n",
    ")\n",
    "# docker build\n",
    "build.steps.append(\n",
    "    {\n",
    "        'name': 'gcr.io/cloud-builders/docker',\n",
    "        'args': ['build', '-t', f'{REPOSITORY}/{EXPERIMENT}_trainer', '/workspace']\n",
    "    }    \n",
    ")\n",
    "# docker push\n",
    "build.images = [f\"{REPOSITORY}/{EXPERIMENT}_trainer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3317c94-4f2e-4469-943b-9e77cf922e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "steps {\n",
       "  name: \"gcr.io/cloud-builders/gsutil\"\n",
       "  args: \"cp\"\n",
       "  args: \"-r\"\n",
       "  args: \"gs://statmike-mlops-349915/05/05f/training/*\"\n",
       "  args: \"/workspace\"\n",
       "}\n",
       "steps {\n",
       "  name: \"gcr.io/cloud-builders/docker\"\n",
       "  args: \"build\"\n",
       "  args: \"-t\"\n",
       "  args: \"us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915-docker/05f_trainer\"\n",
       "  args: \"/workspace\"\n",
       "}\n",
       "images: \"us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915-docker/05f_trainer\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6284d793-3425-4721-884d-cec6892a88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "operation = cb_client.create_build(\n",
    "    project_id = PROJECT_ID,\n",
    "    build = build\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6f00a84-9d23-4529-a088-25629533fe3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Status.SUCCESS: 3>,\n",
       " images: \"us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915-docker/05f_trainer\")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = operation.result()\n",
    "response.status, response.artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d60548a9-79ad-4f75-af4a-2c520806f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review the Custom Container with Artifact Registry in the Google Cloud Console:\n",
      "https://console.cloud.google.com/artifacts/docker/statmike-mlops-349915/us-central1/statmike-mlops-349915-docker?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "print(f\"Review the Custom Container with Artifact Registry in the Google Cloud Console:\\nhttps://console.cloud.google.com/artifacts/docker/{PROJECT_ID}/{REGION}/{PROJECT_ID}-docker?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f02643-fe90-42ab-ab42-ced198a1daf7",
   "metadata": {},
   "source": [
    "### Setup Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67dd3c91-4823-445e-8703-2c130e3af30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "    \"--var_target=\" + VAR_TARGET,\n",
    "    \"--var_omit=\" + VAR_OMIT,\n",
    "    \"--project_id=\" + PROJECT_ID,\n",
    "    \"--bq_project=\" + BQ_PROJECT,\n",
    "    \"--bq_dataset=\" + BQ_DATASET,\n",
    "    \"--bq_table=\" + BQ_TABLE,\n",
    "    \"--region=\" + REGION,\n",
    "    \"--experiment=\" + EXPERIMENT,\n",
    "    \"--series=\" + SERIES,\n",
    "    \"--experiment_name=\" + EXPERIMENT_NAME,\n",
    "    \"--run_name=\" + RUN_NAME\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac76cf49-06ee-44ec-8090-00664013779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingJob = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name = f'{SERIES}_{EXPERIMENT}_{TIMESTAMP}',\n",
    "    container_uri = f\"{REPOSITORY}/{EXPERIMENT}_trainer\",\n",
    "    model_serving_container_image_uri = DEPLOY_IMAGE,\n",
    "    staging_bucket = f\"{URI}/models/{TIMESTAMP}\",\n",
    "    labels = {'series' : f'{SERIES}', 'experiment' : f'{EXPERIMENT}', 'experiment_name' : f'{EXPERIMENT_NAME}', 'run_name' : f'{RUN_NAME}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bf6289-26fc-4762-9555-5341adff89ce",
   "metadata": {},
   "source": [
    "### Run Training Job AND Upload The Model\n",
    "The training job will automatically upload the model to the Vertex AI Model Registry and return the link to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3453c91-81d2-4b43-84ca-2e0dd91d97c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a new model, creating in model registry\n",
      "Training Output directory:\n",
      "gs://statmike-mlops-349915/05/05f/models/20220927190441 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2939962975911411712?project=1026793852137\n",
      "CustomContainerTrainingJob projects/1026793852137/locations/us-central1/trainingPipelines/2939962975911411712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7560497863919140864?project=1026793852137\n",
      "View tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+1026793852137+locations+us-central1+tensorboards+7179142426307592192+experiments+7560497863919140864\n",
      "CustomContainerTrainingJob projects/1026793852137/locations/us-central1/trainingPipelines/2939962975911411712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/1026793852137/locations/us-central1/trainingPipelines/2939962975911411712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/1026793852137/locations/us-central1/trainingPipelines/2939962975911411712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/1026793852137/locations/us-central1/trainingPipelines/2939962975911411712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/1026793852137/locations/us-central1/trainingPipelines/2939962975911411712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/1026793852137/locations/us-central1/trainingPipelines/2939962975911411712 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob run completed. Resource name: projects/1026793852137/locations/us-central1/trainingPipelines/2939962975911411712\n",
      "Model available at projects/1026793852137/locations/us-central1/models/model_05_05f\n"
     ]
    }
   ],
   "source": [
    "modelmatch = aiplatform.Model.list(filter = f'display_name={SERIES}_{EXPERIMENT} AND labels.series={SERIES} AND labels.experiment={EXPERIMENT}')\n",
    "if modelmatch:\n",
    "    print(\"Model Already in Registry:\")\n",
    "    if RUN_NAME in modelmatch[0].version_aliases:\n",
    "        print(\"This version already loaded, no action taken.\")\n",
    "        model = aiplatform.Model(model_name = modelmatch[0].resource_name)\n",
    "    else:\n",
    "        print('Loading model as new default version.')\n",
    "        model = trainingJob.run(\n",
    "            model_display_name = f'{SERIES}_{EXPERIMENT}',\n",
    "            model_labels = {'series' : f'{SERIES}', 'experiment' : f'{EXPERIMENT}', 'experiment_name' : f'{EXPERIMENT_NAME}', 'run_name' : f'{RUN_NAME}'},\n",
    "            model_id = f'model_{SERIES}_{EXPERIMENT}',\n",
    "            parent_model = modelmatch[0].resource_name,\n",
    "            is_default_version = True,\n",
    "            model_version_aliases = [RUN_NAME],\n",
    "            model_version_description = RUN_NAME,\n",
    "            base_output_dir = f\"{URI}/models/{TIMESTAMP}\",\n",
    "            service_account = SERVICE_ACCOUNT,\n",
    "            args = CMDARGS,\n",
    "            replica_count = 1,\n",
    "            machine_type = TRAIN_COMPUTE,\n",
    "            accelerator_count = 0,\n",
    "            tensorboard = tb.resource_name\n",
    "        )\n",
    "else:\n",
    "    print('This is a new model, creating in model registry')\n",
    "    model = trainingJob.run(\n",
    "        model_display_name = f'{SERIES}_{EXPERIMENT}',\n",
    "        model_labels = {'series' : f'{SERIES}', 'experiment' : f'{EXPERIMENT}', 'experiment_name' : f'{EXPERIMENT_NAME}', 'run_name' : f'{RUN_NAME}'},\n",
    "        model_id = f'model_{SERIES}_{EXPERIMENT}',\n",
    "        is_default_version = True,\n",
    "        model_version_aliases = [RUN_NAME],\n",
    "        model_version_description = RUN_NAME,\n",
    "        base_output_dir = f\"{URI}/models/{TIMESTAMP}\",\n",
    "        service_account = SERVICE_ACCOUNT,\n",
    "        args = CMDARGS,\n",
    "        replica_count = 1,\n",
    "        machine_type = TRAIN_COMPUTE,\n",
    "        accelerator_count = 0,\n",
    "        tensorboard = tb.resource_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d56ab8-9495-4b15-8daa-e5e0104398fc",
   "metadata": {},
   "source": [
    "Get the backing Custom Job for the Training Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d2a7c97-f99c-420a-90af-50a0d9dc3cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientPL = aiplatform.gapic.PipelineServiceClient(client_options = {'api_endpoint': f'{REGION}-aiplatform.googleapis.com'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8cf4b02a-053c-4aff-b06d-92a41d934404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "backingCustomJob = MessageToDict(clientPL.get_training_pipeline(name = trainingJob.resource_name)._pb)['trainingTaskMetadata']['backingCustomJob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e605d5f-6e5c-4b34-a0e1-82e438e5d59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('projects/1026793852137/locations/us-central1/customJobs/7560497863919140864',\n",
       " '05_05f_20220927190441-custom-job')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customJob = aiplatform.CustomJob.get(backingCustomJob)\n",
    "customJob.resource_name, customJob.display_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ff790-091a-4d70-bbe5-be2286656698",
   "metadata": {},
   "source": [
    "Create hyperlinks to job and tensorboard here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c0dcbef-1fcf-4476-b78a-56b18cbc8566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review the Training Pipeline here:\n",
      "https://console.cloud.google.com/vertex-ai/training/training-pipelines?project=statmike-mlops-349915\n",
      "Review the Custom Job here:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/training/7560497863919140864/cpu?cloudshell=false&project=statmike-mlops-349915\n",
      "Review the TensorBoard From the Job here:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+1026793852137+locations+us-central1+tensorboards+7179142426307592192+experiments+7560497863919140864\n",
      "Review the model in the Vertex AI Model Registry:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/models/model_05_05f?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "job_link = f\"https://console.cloud.google.com/vertex-ai/locations/{REGION}/training/{customJob.resource_name.split('/')[-1]}/cpu?cloudshell=false&project={PROJECT_ID}\"\n",
    "board_link = f\"https://{REGION}.tensorboard.googleusercontent.com/experiment/{tb.resource_name.replace('/', '+')}+experiments+{customJob.name.split('/')[-1]}\"\n",
    "\n",
    "print(f'Review the Training Pipeline here:\\nhttps://console.cloud.google.com/vertex-ai/training/training-pipelines?project={PROJECT_ID}')\n",
    "print(f'Review the Custom Job here:\\n{job_link}')\n",
    "print(f'Review the TensorBoard From the Job here:\\n{board_link}')\n",
    "print(f'Review the model in the Vertex AI Model Registry:\\nhttps://console.cloud.google.com/vertex-ai/locations/{REGION}/models/{model.name}?project={PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004b5d6-bec7-4fef-b297-ff32782af14a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Progress (Coming Soon)\n",
    "## 04g - Vertex AI > Pipelines - Forecasting Tournament with Kubeflow (KFP) and BQML + AutoML + Prophet\n",
    "\n",
    "\n",
    "### Prerequisites:\n",
    "-  04 - Time Series Forecasting - Data Review in BigQuery\n",
    "\n",
    "### Overview:\n",
    "-  \n",
    "\n",
    "### Resources:\n",
    "-  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Vertex AI - Conceptual Flow\n",
    "\n",
    "<img src=\"architectures/slides/04g_arch.png\">\n",
    "\n",
    "---\n",
    "## Vertex AI - Workflow\n",
    "\n",
    "<img src=\"architectures/slides/04g_console.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DEV NOTES\n",
    "\n",
    "KFP\n",
    "- prepare data: raw > forecast (Transform, add splits)\n",
    "- Launch Forecasting\n",
    "    - ARIMA+\n",
    "        - Output to BigQuery\n",
    "        - Post-Process Predictions\n",
    "    - AutoML\n",
    "        - Parallel CW scenarios [32, 16, 8, 4, 2, 1, 0]\n",
    "            - Launch Vertex AI AutoML Forecasting Training Job\n",
    "                - Output to BigQuery\n",
    "            - Post-Process Predictions\n",
    "    - Prophet\n",
    "        - Parallel Scenarios (yearly flag):\n",
    "            - Launch Vertex AI Training Jobs - use 04f container\n",
    "                - Output to BigQuery\n",
    "            - Post Process Predictions\n",
    "- Custom Metrics - Use combined predictions table from the individual post-processing\n",
    "- Prepare Forecast Table (Best by Series)\n",
    "\n",
    "\n",
    "The AutoML component will need to run 7 concurrent jobs\n",
    "The default limit is 5 per: https://cloud.google.com/vertex-ai/docs/quotas#model_quotas\n",
    "I updated this to 10 with IAM > Quota using this instructions: https://cloud.google.com/docs/quota#requesting_higher_quota\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='statmike-mlops'\n",
    "REGION = 'us-central1'\n",
    "DATANAME = 'citibikes'\n",
    "NOTEBOOK = '04g'\n",
    "\n",
    "# Used for Prophet Custom Forecasting Jobs\n",
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/base-cpu'\n",
    "TRAIN_COMPUTE = 'n1-standard-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "from typing import NamedTuple\n",
    "import kfp # used for dsl.pipeline\n",
    "import kfp.v2.dsl as dsl # used for dsl.component, dsl.Output, dsl.Input, dsl.Artifact, dsl.Model, ...\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "bigquery = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET = PROJECT_ID\n",
    "URI = f\"gs://{BUCKET}/{DATANAME}/models/{NOTEBOOK}\"\n",
    "DIR = f\"temp/{NOTEBOOK}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give service account roles/storage.objectAdmin permissions\n",
    "# Console > IMA > Select Account <projectnumber>-compute@developer.gserviceaccount.com > edit - give role\n",
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {DIR}\n",
    "!mkdir -p {DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Custom Components (KFP)\n",
    "\n",
    "Vertex AI Pipelines are made up of components that run independently with inputs and outputs that connect to form a graph - the pipeline.  For this notebook workflow the following custom components are used to orchestrate different forcasting approaches (BigQuery ML ARIMA+, Prophet, and Vertex AI AutoML Forecasting) and the different scenearios for each of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "This component prepares the data for forcasting and add the split for Train/Validate/Test sets.  This follows the logic used in notebok `04`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image = \"python:3.9\",\n",
    "    packages_to_install = ['pandas','pyarrow','google-cloud-bigquery']\n",
    ")\n",
    "def forecast_prep(\n",
    "    project: str,\n",
    "    notebook: str,\n",
    "    bqsource: str\n",
    ") -> NamedTuple('Source', [('Dataset', str), ('Table', str), ('TournamentTable', str), ('SourceQuery', str), ('TournamentQuery', str)]):\n",
    "\n",
    "    # Setup\n",
    "    from google.cloud import bigquery\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    bigquery = bigquery.Client(project = project)\n",
    "    \n",
    "    # parameters\n",
    "    bqdataset = f\"{notebook}_tournament\"\n",
    "    bqtable = 'source'\n",
    "    bqmain = 'tournament'\n",
    "    sources = namedtuple('Source', ['Dataset', 'Table', 'TournamentTable', 'SourceQuery', 'TournamentQuery'])\n",
    "    \n",
    "    # Create Schema\n",
    "    query = f\"\"\"\n",
    "        CREATE SCHEMA IF NOT EXIST `{project}.{bqdataset}`\n",
    "        OPTIONS(location = 'US', labels = [('notebook','{notebook}')])\n",
    "    \"\"\"\n",
    "    job = bigquery.query(query = query)\n",
    "    job.results()\n",
    "    \n",
    "    # Plan Cutoff dates\n",
    "    query = f\"\"\"\n",
    "        WITH\n",
    "            ALLDATES AS(\n",
    "                SELECT EXTRACT(DATE from starttime) as date\n",
    "                FROM `{bqsource}`\n",
    "                WHERE start_station_name LIKE '%Central Park%'\n",
    "            ),\n",
    "            KEYS AS(\n",
    "                SELECT \n",
    "                    MIN(date) as start_date,\n",
    "                    MAX(date) - CAST(0.025 * DATE_DIFF(MAX(date), MIN(date), DAY) AS INT64) as val_start,\n",
    "                    MAX(date) - CAST(0.0125 * DATE_DIFF(MAX(date), MIN(date), DAY) AS INT64) test_start,\n",
    "                    MAX(date) as end_date\n",
    "                FROM ALLDATES  \n",
    "            )\n",
    "        SELECT *, DATE_DIFF(end_date, test_start, DAY)+1 as forecast_horizon\n",
    "        FROM KEYS    \n",
    "    \"\"\"\n",
    "    keyDates = bigquery.query(query = query).to_dataframe()\n",
    "    \n",
    "    # Prepare Source\n",
    "    querySource = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{project}.{bqdataset}.{bqtable}` AS\n",
    "        WITH\n",
    "            DAYS AS(\n",
    "                SELECT\n",
    "                   start_station_name,\n",
    "                   EXTRACT(DATE from starttime) AS date,\n",
    "                   COUNT(*) AS num_trips\n",
    "                FROM `{bqsource}`\n",
    "                WHERE start_station_name LIKE '%Central Park%'\n",
    "                GROUP BY start_station_name, date\n",
    "            )\n",
    "        SELECT *,\n",
    "           CASE\n",
    "               WHEN date < DATE({keyDates['val_start'][0].strftime('%Y, %m, %d')}) THEN \"TRAIN\"\n",
    "               WHEN date < DATE({keyDates['test_start'][0].strftime('%Y, %m, %d')}) THEN \"VALIDATE\"\n",
    "               ELSE \"TEST\"\n",
    "           END AS splits\n",
    "        FROM DAYS\n",
    "    \"\"\"\n",
    "    job = bigquery.query(query = querySource)\n",
    "    job.result()\n",
    "    \n",
    "    # Prepare Common Output Table\n",
    "    queryOutput = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{project}.{bqdataset}.{bqmain}`\n",
    "        (platform STRING, method STRING, scenario STRING, start_station_name STRING, date DATE, num_trips INT64, yhat FLOAT64, yhat_lower FLOAT64, yhat_upper FLOAT64)\n",
    "    \"\"\"\n",
    "    job = bq.query(query = queryOutput)\n",
    "    job.result()\n",
    "    \n",
    "    return sources(bqdataset, bqtable, bqmain, querySource, queryOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigQuery ML ARIMA+\n",
    "This component fits a forecasting model using BigQuery ML model type ARIMA+.  This follows the logic used in notebook `04a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image = \"python:3.9\",\n",
    "    packages_to_install = ['pandas','pyarrow','google-cloud-bigquery']\n",
    ")\n",
    "def forecast_bqarima(\n",
    "    project: str,\n",
    "    notebook: str,\n",
    "    bqdataset: str,\n",
    "    bqtable: str,\n",
    "    bqmain: str\n",
    ") -> NamedTuple('Source', [('Output', str), ('Model', str), ('ModelQuery', str)])\n",
    "\n",
    "    # Setup\n",
    "    from google.cloud import bigquery\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    bigquery = bigquery.Client(project = project)\n",
    "    \n",
    "    # parameters\n",
    "    bqsource = f\"{project}.{bqdataset}.{bqtable}\"\n",
    "    bqmodel = f\"{project}.{bqdataset}.{notebook}_arimaplus\"\n",
    "    bqoutput = f\"{project}.{bqdataset}.{notebook}_forecast_arimaplus\"\n",
    "    sources = namedtuple('Source', ['Output', 'Model', 'ModelQuery'])\n",
    "    \n",
    "    # Retrieve Key Dates from Source\n",
    "    query = f\"\"\"\n",
    "        WITH\n",
    "            SPLIT AS (\n",
    "                SELECT splits, min(date) as mindate, max(date) as maxdate\n",
    "                FROM `{bqsource}`\n",
    "                GROUP BY splits\n",
    "            ),\n",
    "            TRAIN AS (\n",
    "                SELECT mindate as start_date\n",
    "                FROM SPLIT\n",
    "                WHERE splits ='TRAIN'\n",
    "            ),\n",
    "            VAL AS (\n",
    "                SELECT mindate as val_start\n",
    "                FROM SPLIT\n",
    "                WHERE splits = 'VALIDATE'\n",
    "            ),\n",
    "            TEST AS (\n",
    "                SELECT mindate as test_start, maxdate as end_date, DATE_DIFF(maxdate, mindate, DAY)+1 as forecast_horizon\n",
    "                FROM SPLIT\n",
    "                WHERE splits = 'TEST'\n",
    "            )\n",
    "        SELECT * EXCEPT(pos) FROM\n",
    "        (SELECT *, ROW_NUMBER() OVER() pos FROM TRAIN)\n",
    "        JOIN (SELECT *, ROW_NUMBER() OVER() pos FROM VAL)\n",
    "        USING (pos)\n",
    "        JOIN (SELECT *, ROW_NUMBER() OVER() pos FROM TEST)\n",
    "        USING (pos)\n",
    "    \"\"\"\n",
    "    keyDates = bigquery.query(query = query).to_dataframe()\n",
    "    keyDates\n",
    "    \n",
    "    # Create Model: ARIMA_PLUS\n",
    "    queryARIMA = f\"\"\"\n",
    "        CREATE OR REPLACE MODEL `{bqmodel}`\n",
    "        OPTIONS\n",
    "          (model_type = 'ARIMA_PLUS',\n",
    "           time_series_timestamp_col = 'date',\n",
    "           time_series_data_col = 'num_trips',\n",
    "           time_series_id_col = 'start_station_name',\n",
    "           auto_arima_max_order = 5,\n",
    "           holiday_region = 'US',\n",
    "           horizon = {keyDates['forecast_horizon'][0]}\n",
    "          ) AS\n",
    "        SELECT start_station_name, date, num_trips\n",
    "        FROM `{bqsource}`\n",
    "        WHERE splits in ('TRAIN','VALIDATE')\n",
    "    \"\"\"\n",
    "    job = bigquery.query(query = queryARIMA)\n",
    "    job.result()\n",
    "    \n",
    "    # Create Raw Output\n",
    "    query = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{bqoutput}` AS\n",
    "        WITH\n",
    "            FORECAST AS (\n",
    "                SELECT\n",
    "                    start_station_name, \n",
    "                    EXTRACT(DATE from time_series_timestamp) as date,\n",
    "                    time_series_adjusted_data as forecast_value,\n",
    "                    time_series_type,\n",
    "                    prediction_interval_lower_bound,\n",
    "                    prediction_interval_upper_bound\n",
    "                FROM ML.EXPLAIN_FORECAST(MODEL `{bqmodel}`, STRUCT({keyDates['forecast_horizon'][0]} AS horizon, 0.95 AS confidence_level))\n",
    "                WHERE time_series_type = 'forecast'\n",
    "            ),\n",
    "            ACTUAL AS (\n",
    "                SELECT start_station_name, date, sum(num_trips) as actual_value\n",
    "                FROM `{bqsource}`\n",
    "                WHERE splits = 'TEST'\n",
    "                GROUP BY start_station_name, date\n",
    "            )\n",
    "        SELECT *\n",
    "        FROM FORECAST\n",
    "        INNER JOIN ACTUAL\n",
    "        USING (start_station_name, date)\n",
    "        ORDER BY start_station_name, time_series_type  \n",
    "    \"\"\"\n",
    "    job = bigquery.query(query = query)\n",
    "    job.result()\n",
    "    \n",
    "    # Insert Output for Tournament\n",
    "    query = f\"\"\"\n",
    "        INSERT INTO `{project}.{bqdataset}.{bqmain}`\n",
    "        SELECT\n",
    "            'BigQuery' as platform,\n",
    "            'ARIMA_PLUS' as method,\n",
    "            'automatic' as scenario,\n",
    "            start_station_name,\n",
    "            date,\n",
    "            num_trips,\n",
    "            forecast_value as yhat,\n",
    "            prediction_interval_lower_bound as yhat_lower,\n",
    "            prediction_interval_upper_bound as yhat_upper\n",
    "        FROM `{bqoutput}`\n",
    "        ORDER by start_station_name, date\n",
    "    \"\"\"\n",
    "    job = bigquery.query(query = query)\n",
    "    job.result()\n",
    "    \n",
    "    return sources(bqoutput, bqmodel, queryARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex AI AutoML Forecasting\n",
    "This component fits a forecasting model using Vertex AI AutoML Forecasting.  This follows the logic used in notebooks `04c` and `04d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex AI Training Custom Jobs for Forecasting with Prophet\n",
    "This component fits a forecasting model using a Prophet script in a custom container (built in `04f`) to fit forecasting.  This follows the logic used in notebook `04f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metrics and Champion Selection\n",
    "This component calculates custom metrics for all time series and all methods to select the best method per series and prepare a champion prediction file in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline (KFP) Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "    BQ_SOURCE = 'bigquery-public-data.new_york.citibike_trips'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vertex AI Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

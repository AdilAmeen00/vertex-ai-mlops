{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Progress (Coming Soon)\n",
    "## 04g - Vertex AI > Pipelines - Forecasting Tournament with Kubeflow (KFP) and BQML + AutoML + Prophet\n",
    "\n",
    "\n",
    "### Prerequisites:\n",
    "-  04 - Time Series Forecasting - Data Review in BigQuery\n",
    "\n",
    "### Overview:\n",
    "-  \n",
    "\n",
    "### Resources:\n",
    "-  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Vertex AI - Conceptual Flow\n",
    "\n",
    "<img src=\"architectures/slides/04g_arch.png\">\n",
    "\n",
    "---\n",
    "## Vertex AI - Workflow\n",
    "\n",
    "<img src=\"architectures/slides/04g_console.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DEV NOTES\n",
    "\n",
    "KFP\n",
    "- prepare data: raw > forecast (Transform, add splits) and create dataset (for automl)\n",
    "- Launch Forecasting\n",
    "    - ARIMA+\n",
    "        - fit\n",
    "            - Output to BigQuery\n",
    "            - Post-Process Predictions\n",
    "        - Calc Metrics\n",
    "    - AutoML\n",
    "        - Parallel CW scenarios [32, 16, 8, 4, 2, 1, 0]\n",
    "            - fit\n",
    "                - Launch Vertex AI AutoML Forecasting Training Job\n",
    "                    - Output to BigQuery\n",
    "                - Post-Process Predictions\n",
    "            - Calc Metrics\n",
    "    - Prophet\n",
    "        - Parallel Scenarios (yearly flag):\n",
    "            - fit\n",
    "                - Launch Vertex AI Training Jobs - use 04f container\n",
    "                    - Output to BigQuery\n",
    "                - Post Process Predictions\n",
    "- Create Best Method - Use combined predictions table from the individual post-processing\n",
    "- Create Best Series\n",
    "\n",
    "The AutoML component will need to run 7 concurrent jobs\n",
    "The default limit is 5 per: https://cloud.google.com/vertex-ai/docs/quotas#model_quotas\n",
    "I updated this to 10 with IAM > Quota using this instructions: https://cloud.google.com/docs/quota#requesting_higher_quota\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='statmike-demo3'\n",
    "REGION = 'us-central1'\n",
    "DATANAME = 'citibikes'\n",
    "NOTEBOOK = '04g'\n",
    "\n",
    "# Used for Prophet Custom Forecasting Jobs\n",
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/base-cpu'\n",
    "BQ_SOURCE = 'bigquery-public-data.new_york.citibike_trips'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "from typing import NamedTuple\n",
    "import kfp # used for dsl.pipeline\n",
    "import kfp.v2.dsl as dsl # used for dsl.component, dsl.Output, dsl.Input, dsl.Artifact, dsl.Model, ...\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "bigquery = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = PROJECT_ID\n",
    "URI = f\"gs://{BUCKET}/{DATANAME}/models/{NOTEBOOK}\"\n",
    "DIR = f\"temp/{NOTEBOOK}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'715288179162-compute@developer.gserviceaccount.com'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give service account roles/storage.objectAdmin permissions\n",
    "# Console > IMA > Select Account <projectnumber>-compute@developer.gserviceaccount.com > edit - give role\n",
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {DIR}\n",
    "!mkdir -p {DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Custom Components (KFP)\n",
    "\n",
    "Vertex AI Pipelines are made up of components that run independently with inputs and outputs that connect to form a graph - the pipeline.  For this notebook workflow the following custom components are used to orchestrate different forcasting approaches (BigQuery ML ARIMA+, Prophet, and Vertex AI AutoML Forecasting) and the different scenearios for each of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "This component prepares the data for forcasting and add the split for Train/Validate/Test sets.  This follows the logic used in notebok `04`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image = \"python:3.9\",\n",
    "    packages_to_install = ['pandas','pyarrow','google-cloud-bigquery', 'google-cloud-aiplatform']\n",
    ")\n",
    "def forecast_prep(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    notebook: str,\n",
    "    bqsource: str,\n",
    ") -> NamedTuple('source', [('fh', int), ('bqdataset', str), ('bqtable', str), ('bqmain', str), ('bqtable_query', str), ('bqmain_query', str), ('dataset_resource_name', str)]):\n",
    "\n",
    "    # Setup\n",
    "    from collections import namedtuple\n",
    "    sources = namedtuple('source', ['fh', 'bqdataset', 'bqtable', 'bqmain', 'bqtable_query', 'bqmain_query', 'dataset_resource_name'])\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    bigquery = bigquery.Client(project = project)\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project, location=region)\n",
    "    \n",
    "    # parameters\n",
    "    bqdataset = f\"{notebook}_tournament\"\n",
    "    bqtable = f'{project}.{bqdataset}.source'\n",
    "    bqmain = f'{project}.{bqdataset}.tournament'\n",
    "    \n",
    "    # Create Schema\n",
    "    query = f\"\"\"\n",
    "        CREATE SCHEMA IF NOT EXISTS `{project}.{bqdataset}`\n",
    "        OPTIONS(location = 'US', labels = [('notebook','{notebook}')])\n",
    "    \"\"\"\n",
    "    job = bigquery.query(query = query)\n",
    "    job.result()\n",
    "    \n",
    "    # Plan Cutoff dates\n",
    "    query = f\"\"\"\n",
    "        WITH\n",
    "            ALLDATES AS(\n",
    "                SELECT EXTRACT(DATE from starttime) as date\n",
    "                FROM `{bqsource}`\n",
    "                WHERE start_station_name LIKE '%Central Park%'\n",
    "            ),\n",
    "            KEYS AS(\n",
    "                SELECT \n",
    "                    MIN(date) as start_date,\n",
    "                    MAX(date) - CAST(0.025 * DATE_DIFF(MAX(date), MIN(date), DAY) AS INT64) as val_start,\n",
    "                    MAX(date) - CAST(0.0125 * DATE_DIFF(MAX(date), MIN(date), DAY) AS INT64) test_start,\n",
    "                    MAX(date) as end_date\n",
    "                FROM ALLDATES  \n",
    "            )\n",
    "        SELECT *, DATE_DIFF(end_date, test_start, DAY)+1 as forecast_horizon\n",
    "        FROM KEYS    \n",
    "    \"\"\"\n",
    "    keyDates = bigquery.query(query = query).to_dataframe()\n",
    "    \n",
    "    # Prepare Source\n",
    "    queryTable = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{bqtable}` AS\n",
    "        WITH\n",
    "            DAYS AS(\n",
    "                SELECT\n",
    "                   start_station_name,\n",
    "                   EXTRACT(DATE from starttime) AS date,\n",
    "                   COUNT(*) AS num_trips\n",
    "                FROM `{bqsource}`\n",
    "                WHERE start_station_name LIKE '%Central Park%'\n",
    "                GROUP BY start_station_name, date\n",
    "            )\n",
    "        SELECT *,\n",
    "           CASE\n",
    "               WHEN date < DATE({keyDates['val_start'][0].strftime('%Y, %m, %d')}) THEN \"TRAIN\"\n",
    "               WHEN date < DATE({keyDates['test_start'][0].strftime('%Y, %m, %d')}) THEN \"VALIDATE\"\n",
    "               ELSE \"TEST\"\n",
    "           END AS splits\n",
    "        FROM DAYS\n",
    "    \"\"\"\n",
    "    job = bigquery.query(query = queryTable)\n",
    "    job.result()\n",
    "    \n",
    "    # Prepare Common Output Table\n",
    "    queryOutput = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{bqmain}`\n",
    "        (platform STRING, method STRING, scenario STRING, start_station_name STRING, date DATE, num_trips INT64, yhat FLOAT64, yhat_lower FLOAT64, yhat_upper FLOAT64)\n",
    "    \"\"\"\n",
    "    job = bigquery.query(query = queryOutput)\n",
    "    job.result()\n",
    "    \n",
    "    # create Vertex AI Dataset linking to bqtable\n",
    "    dataset = aiplatform.TimeSeriesDataset.create(\n",
    "        display_name = f'{notebook}_source', \n",
    "        bq_source = f'bq://{bqtable}',\n",
    "        labels = {'notebook':f'{notebook}'}\n",
    "    )\n",
    "    \n",
    "    return sources(keyDates['forecast_horizon'][0].item(), bqdataset, bqtable, bqmain, queryTable, queryOutput, dataset.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigQuery ML ARIMA+\n",
    "This component fits a forecasting model using BigQuery ML model type ARIMA+.  This follows the logic used in notebook `04a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image = \"python:3.9\",\n",
    "    packages_to_install = ['pandas','pyarrow','google-cloud-bigquery']\n",
    ")\n",
    "def forecast_bqarima(\n",
    "    project: str,\n",
    "    notebook: str,\n",
    "    bqtable: str,\n",
    "    bqmain: str,\n",
    "    fh: int\n",
    ") -> NamedTuple('source', [('bqoutput', str), ('bqmodel', str), ('bq_model_query', str), ('platform', str), ('method', str), ('scenario', str)]):\n",
    "\n",
    "    # Setup\n",
    "    from collections import namedtuple\n",
    "    sources = namedtuple('source', ['bqoutput', 'bqmodel', 'bq_model_query', 'platform', 'method', 'scenario'])\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    bigquery = bigquery.Client(project = project)\n",
    "    \n",
    "    # parameters\n",
    "    table = bqtable.split('.')[-1]\n",
    "    bqmodel = f\"{bqtable[:-(len(table)+1)]}.{notebook}_arimaplus\"\n",
    "    bqoutput = f\"{bqtable[:-(len(table)+1)]}.{notebook}_forecast_arimaplus\"\n",
    "    platform = 'BigQuery' \n",
    "    method = 'ARIMA_PLUS'\n",
    "    scenario = 'automatic'\n",
    "    \n",
    "    # Create Model: ARIMA_PLUS\n",
    "    queryARIMA = f\"\"\"\n",
    "        CREATE OR REPLACE MODEL `{bqmodel}`\n",
    "        OPTIONS\n",
    "          (model_type = 'ARIMA_PLUS',\n",
    "           time_series_timestamp_col = 'date',\n",
    "           time_series_data_col = 'num_trips',\n",
    "           time_series_id_col = 'start_station_name',\n",
    "           auto_arima_max_order = 5,\n",
    "           holiday_region = 'US',\n",
    "           horizon = {fh}\n",
    "          ) AS\n",
    "        SELECT start_station_name, date, num_trips\n",
    "        FROM `{bqtable}`\n",
    "        WHERE splits in ('TRAIN','VALIDATE')\n",
    "    \"\"\"\n",
    "    job = bigquery.query(query = queryARIMA)\n",
    "    job.result()\n",
    "    \n",
    "    # Create Raw Output\n",
    "    query = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE `{bqoutput}` AS\n",
    "        WITH\n",
    "            FORECAST AS (\n",
    "                SELECT\n",
    "                    start_station_name, \n",
    "                    EXTRACT(DATE from time_series_timestamp) as date,\n",
    "                    time_series_adjusted_data as forecast_value,\n",
    "                    time_series_type,\n",
    "                    prediction_interval_lower_bound,\n",
    "                    prediction_interval_upper_bound\n",
    "                FROM ML.EXPLAIN_FORECAST(MODEL `{bqmodel}`, STRUCT({fh} AS horizon, 0.95 AS confidence_level))\n",
    "                WHERE time_series_type = 'forecast'\n",
    "            ),\n",
    "            ACTUAL AS (\n",
    "                SELECT start_station_name, date, sum(num_trips) as num_trips\n",
    "                FROM `{bqtable}`\n",
    "                WHERE splits = 'TEST'\n",
    "                GROUP BY start_station_name, date\n",
    "            )\n",
    "        SELECT *\n",
    "        FROM FORECAST\n",
    "        INNER JOIN ACTUAL\n",
    "        USING (start_station_name, date)\n",
    "        ORDER BY start_station_name, time_series_type  \n",
    "    \"\"\"\n",
    "    job = bigquery.query(query = query)\n",
    "    job.result()\n",
    "    \n",
    "    # Insert Output for Tournament (first remove prior run if present)\n",
    "    query = f\"\"\"\n",
    "        DELETE `{bqmain}`\n",
    "        WHERE platform = '{platform}' and method = '{method}' and scenario = '{scenario}'\n",
    "    \"\"\"\n",
    "    job = bigquery.query(query)\n",
    "    job.result()\n",
    "    query = f\"\"\"\n",
    "        INSERT INTO `{bqmain}`\n",
    "        SELECT\n",
    "            '{platform}' as platform,\n",
    "            '{method}' as method,\n",
    "            '{scenario}' as scenario,\n",
    "            start_station_name,\n",
    "            date,\n",
    "            num_trips,\n",
    "            forecast_value as yhat,\n",
    "            prediction_interval_lower_bound as yhat_lower,\n",
    "            prediction_interval_upper_bound as yhat_upper\n",
    "        FROM `{bqoutput}`\n",
    "        ORDER by start_station_name, date\n",
    "    \"\"\"\n",
    "    job = bigquery.query(query = query)\n",
    "    job.result()\n",
    "    \n",
    "    return sources(bqoutput, bqmodel, queryARIMA, platform, method, scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex AI AutoML Forecasting\n",
    "This component fits a forecasting model using Vertex AI AutoML Forecasting.  This follows the logic used in notebooks `04c` and `04d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image = \"python:3.9\",\n",
    "    packages_to_install = ['google-cloud-bigquery','google-cloud-aiplatform']\n",
    ")\n",
    "def forecast_automl(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    notebook: str,\n",
    "    bqtable: str,\n",
    "    bqmain: str,\n",
    "    ds: str, # vertex ai dataset.resource_name\n",
    "    cw: int, # context window\n",
    "    fh: int # forecast horizon\n",
    ") -> NamedTuple('source', [('bqoutput', str), ('model', str), ('platform', str), ('method', str), ('scenario', str)]):\n",
    "\n",
    "    # Setup\n",
    "    from collections import namedtuple\n",
    "    sources = namedtuple('source', ['bqoutput', 'model', 'platform', 'method', 'scenario'])\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    bigquery = bigquery.Client(project = project)\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project, location=region)\n",
    "    \n",
    "    # parameters\n",
    "    table = bqtable.split('.')[-1]\n",
    "    bqoutput = f\"{bqtable[:-(len(table)+1)]}.{notebook}_forecast_automl_{cw}\"\n",
    "    platform = 'Vertex AI' \n",
    "    method = 'AutoML'\n",
    "    scenario = f'cw={cw}'\n",
    "    \n",
    "    # dataset\n",
    "    dataset = aiplatform.TimeSeriesDataset(ds)\n",
    "    \n",
    "    # Delete output table from BigQuery (in case previously run created it)\n",
    "    query = f\"\"\"\n",
    "        DROP TABLE IF EXISTS `{bqoutput}`\n",
    "    \"\"\"\n",
    "    job = bigquery.query(query = query)\n",
    "    job.result()\n",
    "    \n",
    "    # Create Job\n",
    "    column_specs = list(set(dataset.column_names) - set(['splits','start_station_name']))\n",
    "    column_specs = dict.fromkeys(column_specs, 'auto')\n",
    "    column_specs\n",
    "    \n",
    "    forecast_job = aiplatform.AutoMLForecastingTrainingJob(\n",
    "        display_name = f'{notebook}_automl_{cw}',\n",
    "        optimization_objective = \"minimize-rmse\",\n",
    "        column_specs = column_specs,\n",
    "        labels = {'notebook':f'{notebook}', 'cw': f'{cw}'}\n",
    "    )\n",
    "    \n",
    "    # Run Job\n",
    "    forecast = forecast_job.run(\n",
    "        dataset = dataset,\n",
    "        target_column = \"num_trips\",\n",
    "        time_column = \"date\",\n",
    "        time_series_identifier_column = \"start_station_name\",\n",
    "        unavailable_at_forecast_columns = [\"num_trips\",],\n",
    "        available_at_forecast_columns = [\"date\",],\n",
    "        forecast_horizon = fh,\n",
    "        data_granularity_unit = \"day\",\n",
    "        data_granularity_count = 1,\n",
    "        predefined_split_column_name = \"splits\",\n",
    "        context_window = cw,\n",
    "        export_evaluated_data_items = True,\n",
    "        export_evaluated_data_items_bigquery_destination_uri = f\"bq://{bqoutput.replace('.',':')}\",\n",
    "        validation_options = \"fail-pipeline\",\n",
    "        budget_milli_node_hours = 100,\n",
    "        model_display_name = f\"{notebook}_automl_{cw}\",\n",
    "        model_labels = {'notebook':f'{notebook}', 'cw': f'{cw}'}\n",
    "    )\n",
    "        \n",
    "    # Insert Output for Tournament (first remove prior run if present)\n",
    "    query = f\"\"\"\n",
    "        DELETE `{bqmain}`\n",
    "        WHERE platform = '{platform}' and method = '{method}' and scenario = '{scenario}'\n",
    "    \"\"\"\n",
    "    job = bigquery.query(query)\n",
    "    query = f\"\"\"\n",
    "        INSERT INTO `{bqmain}`\n",
    "        WITH\n",
    "            FORECASTS AS (\n",
    "                SELECT\n",
    "                    DATE(date) as date,\n",
    "                    DATE(predicted_on_date) as predicted_on_date,\n",
    "                    CAST(num_trips as INT64) AS num_trips, splits,\n",
    "                    start_station_name,\n",
    "                    predicted_num_trips.value as predicted_num_trips\n",
    "                FROM `{bqoutput}`\n",
    "            ),\n",
    "            LEAD_DAYS AS (\n",
    "                SELECT *, DATE_DIFF(date, predicted_on_date, DAY) as prediction_lead_days\n",
    "                FROM FORECASTS\n",
    "            ),\n",
    "            LATEST AS (\n",
    "                SELECT start_station_name, date, min(prediction_lead_days) as prediction_lead_days\n",
    "                FROM LEAD_DAYS\n",
    "                GROUP BY start_station_name, date\n",
    "            ),\n",
    "            DIFFS AS (\n",
    "                SELECT \n",
    "                    start_station_name, date,\n",
    "                    predicted_num_trips as forecast_value,\n",
    "                    num_trips\n",
    "                FROM LATEST\n",
    "                LEFT OUTER JOIN LEAD_DAYS\n",
    "                USING (start_station_name, date, prediction_lead_days)    \n",
    "            )\n",
    "        SELECT\n",
    "            '{platform}' as platform,\n",
    "            '{method}' as method,\n",
    "            '{scenario}' as scenario,\n",
    "            start_station_name,\n",
    "            date,\n",
    "            num_trips,\n",
    "            forecast_value as yhat,\n",
    "            NULL as yhat_lower,\n",
    "            NULL as yhat_upper\n",
    "        FROM DIFFS\n",
    "        ORDER by start_station_name, date\n",
    "    \"\"\"\n",
    "    job = bigquery.query(query = query)\n",
    "    job.result()\n",
    "\n",
    "    return sources(bqoutput, forecast.resource_name, platform, method, scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex AI Training Custom Jobs for Forecasting with Prophet\n",
    "This component fits a forecasting model using a Prophet script in a custom container (built in `04f`) to fit forecasting.  This follows the logic used in notebook `04f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image = \"python:3.9\",\n",
    "    packages_to_install = ['google-cloud-bigquery','google-cloud-aiplatform']\n",
    ")\n",
    "def forecast_prophet(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    notebook: str,\n",
    "    service_account: str,\n",
    "    bqdataset: str,\n",
    "    bqtable: str,\n",
    "    bqmain: str,\n",
    "    image: str,\n",
    "    yearly: str, # flag for prophet to add seasonality for yearly\n",
    "    fh: int, # forecast horizon\n",
    ") -> NamedTuple('Source', [('Output', str), ('Model', str), ('platform', str), ('method', str), ('scenario', str)])\n",
    "\n",
    "    # Setup\n",
    "    from collections import namedtuple\n",
    "    sources = namedtuple('Source', ['Output', 'Model', 'platform', 'method', 'scenario'])\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    bigquery = bigquery.Client(project = project)\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project, location=region)\n",
    "    \n",
    "    # parameters\n",
    "    platform = 'Vertex AI' \n",
    "    method = 'Prophet Container'\n",
    "    if yearly == 'true': scenario = '--yearly'\n",
    "    else scenario = '--no-yearly'\n",
    "    bqsource = f\"{project}.{bqdataset}.{bqtable}\"\n",
    "    bqoutput = f\"{project}.{bqdataset}.{notebook}_forecast_prophet_{scenario}\"\n",
    "    URI = f\"gs://{project}/{bqdataset}/models/{notebook}\"\n",
    "    \n",
    "    # create job\n",
    "    CMDARGS = [\n",
    "        \"--project=\" + project,\n",
    "        \"--bqsource=\" + bqdataset,\n",
    "        \"--bqoutput=\" + notebook,\n",
    "        \"--horizon=\" + f'{fh}',\n",
    "        f\"{scenario}\"\n",
    "    ]\n",
    "\n",
    "    MACHINE_SPEC = {\n",
    "        \"machine_type\": 'n1-standard-8',\n",
    "        \"accelerator_count\": 0\n",
    "    }\n",
    "\n",
    "    WORKER_POOL_SPEC = [\n",
    "        {\n",
    "            \"replica_count\": 1,\n",
    "            \"machine_spec\": MACHINE_SPEC,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": iamge,\n",
    "                \"command\": [],\n",
    "                \"args\": CMDARGS\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    customJob = aiplatform.CustomJob(\n",
    "        display_name = f'{notebook}_prophet_{scenario}',\n",
    "        worker_pool_specs = WORKER_POOL_SPEC,\n",
    "        base_output_dir = f\"{URI}/{scenario}\",\n",
    "        staging_bucket = f\"{URI}/{scenario}\",\n",
    "        labels = {'notebook':f'{notebook}', 'yearly':f'{scenario}'}\n",
    "    )\n",
    "    \n",
    "    # run job\n",
    "    customJob.run(\n",
    "        service_account = service_account\n",
    "    )\n",
    "                           \n",
    "    # Insert Output for Tournament (first remove prior run if present)\n",
    "    query = f\"\"\"\n",
    "        DELETE `{bqmain}`\n",
    "        WHERE platform = '{platform}' and method = '{method}' and scenario = '{scenario}'\n",
    "    \"\"\"\n",
    "    job = bq.query(query)\n",
    "    query = f\"\"\"\n",
    "        INSERT INTO `{project}.{bqdataset}.{bqmain}`\n",
    "        SELECT\n",
    "            '{platform}' as platform,\n",
    "            '{method}' as method,\n",
    "            '{scenario}' as scenario,\n",
    "            start_station_name,\n",
    "            date,\n",
    "            num_trips,\n",
    "            yhat,\n",
    "            yhat_lower,\n",
    "            yhat_upper\n",
    "        FROM '{bqoutput}'\n",
    "        ORDER by start_station_name, date\n",
    "    \"\"\"\n",
    "    job = bigquery.query(query = query)\n",
    "    job.result()\n",
    "\n",
    "    return sources(f'{bqoutput}', customJob.uri, platform, method, scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Container Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assemble Python File For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p {DIR}/model/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp/04f/model/fit/prophet.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile  {DIR}/model/fit/prophet.py\n",
    "from prophet import Prophet\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "# import parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--project', dest = 'project', type = str)\n",
    "parser.add_argument('--bqsource', dest = 'bqsource', type = str)\n",
    "parser.add_argument('--bqoutput', dest = 'bqoutput', type = str)\n",
    "parser.add_argument('--horizon', type=int)\n",
    "\n",
    "parser.add_argument('--yearly', action='store_true')\n",
    "parser.add_argument('--no-yearly', action='store_false')\n",
    "parser.set_defaults(yearly=False)\n",
    "\n",
    "args = parser.parse_args()\n",
    "project = args.project\n",
    "bqsource = args.bqsource\n",
    "bqoutput = args.bqoutput\n",
    "\n",
    "# client for BQ\n",
    "bigquery = bigquery.Client(project = project)\n",
    "\n",
    "# input data - from BQ\n",
    "query = f\"SELECT * FROM `{bqsource}` ORDER by start_station_name, date\"\n",
    "source = bigquery.query(query = query).to_dataframe()\n",
    "\n",
    "# preprocess data - as a list of dataframes for each series\n",
    "seriesNames = source['start_station_name'].unique().tolist()\n",
    "seriesFrames = []\n",
    "for s in seriesNames:\n",
    "    frame = source[(source['start_station_name']==s) & (source['splits']!='TEST')][['date','num_trips']].rename(columns={'date':'ds','num_trips':'y'})\n",
    "    seriesFrames.append(frame)\n",
    "\n",
    "# function to run a prophet fit & forecast\n",
    "def run_prophet(series):\n",
    "    if args.yearly:\n",
    "        p = Prophet(weekly_seasonality=True, yearly_seasonality=True)\n",
    "    else:\n",
    "        p = Prophet(weekly_seasonality=True)\n",
    "    p.add_country_holidays(country_name='US')\n",
    "    p.fit(series)\n",
    "    f = p.make_future_dataframe(periods = args.horizon, include_history = False)\n",
    "    f = p.predict(f)\n",
    "    return f[['ds','yhat','yhat_lower','yhat_upper']]\n",
    "\n",
    "# run the series in a thread pool for multiprocessing\n",
    "pool = Pool(cpu_count())\n",
    "predictions = list(tqdm(pool.imap(run_prophet, seriesFrames), total = len(seriesFrames)))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# postprocess data - add series name back to dataframe and concatenate all into one dataframe\n",
    "for i, p in enumerate(predictions):\n",
    "    p['start_station_name'] = seriesNames[i]\n",
    "output = pd.concat(predictions)\n",
    "\n",
    "# output data - to BQ\n",
    "output.to_gbq(f\"{bqoutput}\" if_exists = 'replace')\n",
    "\n",
    "# Transform final data in BQ - merge with original input\n",
    "query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{bqoutput}` AS\n",
    "WITH\n",
    "    SOURCE AS (\n",
    "        SELECT *\n",
    "        FROM `{bqsource}`\n",
    "    ),\n",
    "    PROPHET AS (\n",
    "        SELECT start_station_name, DATE(ds) as date, yhat, yhat_lower, yhat_upper\n",
    "        FROM `{bqoutput}`\n",
    "        WHERE \n",
    "    )\n",
    "SELECT *\n",
    "FROM SOURCE\n",
    "LEFT OUTER JOIN PROPHET\n",
    "USING (start_station_name, date)\n",
    "ORDER by start_station_name, date\n",
    "\"\"\"\n",
    "Tjob = bigquery.query(query = query)\n",
    "Tjob.result()\n",
    "(Tjob.ended-Tjob.started).total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choose a Base Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/deeplearning-platform-release/base-cpu'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_IMAGE # Defined above in Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the Dockerfile\n",
    "A basic dockerfile thats take the base image and copies the code in and define an entrypoint - what python script to run first in this case.  Add RUN entries to pip install additional packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile = f\"\"\"\n",
    "FROM {BASE_IMAGE}\n",
    "WORKDIR /\n",
    "RUN pip install pandas pystan==2.19.1.1 prophet pandas-gbq google-cloud-bigquery pyarrow tqdm\n",
    "## Copies the trainer code to the docker image\n",
    "COPY fit /fit\n",
    "## Sets up the entry point to invoke the trainer\n",
    "ENTRYPOINT [\"python\", \"-m\", \"fit.prophet\"]\n",
    "\"\"\"\n",
    "with open(f'{DIR}/model/Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Enable Artifact Registry API:\n",
    "Check to see if the api is enabled, if not then enable it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact Registry is Enabled for This Project: statmike-demo3\n"
     ]
    }
   ],
   "source": [
    "services = !gcloud services list --format=\"json\" --available --filter=name:artifactregistry.googleapis.com\n",
    "services = json.loads(\"\".join(services))\n",
    "\n",
    "if (services[0]['config']['name'] == 'artifactregistry.googleapis.com') & (services[0]['state'] == 'ENABLED'):\n",
    "    print(f\"Artifact Registry is Enabled for This Project: {PROJECT_ID}\")\n",
    "else:\n",
    "    print(f\"Enabeling Artifact Registry for this Project: {PROJECT_ID}\")\n",
    "    !gcloud services enable artifactregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create A Repository\n",
    "Check to see if the registry is already created, if not then create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is already a repository named statmike-demo3\n"
     ]
    }
   ],
   "source": [
    "repositories = !gcloud artifacts repositories list --format=\"json\" --filter=REPOSITORY:{PROJECT_ID}\n",
    "repositories = json.loads(\"\".join(repositories[2:]))\n",
    "\n",
    "if len(repositories) > 0:\n",
    "    print(f'There is already a repository named {PROJECT_ID}')\n",
    "else:\n",
    "    print(f'Creating a repository named {PROJECT_ID}')\n",
    "    !gcloud  artifacts repositories create {PROJECT_ID} --repository-format=docker --location={REGION} --description=\"Vertex AI Training Custom Containers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure Local Docker to Use GCLOUD CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build The Custom Container (local to notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-central1-docker.pkg.dev/statmike-demo3/statmike-demo3/04f_citibikes:latest'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{PROJECT_ID}/{NOTEBOOK}:latest\"\n",
    "IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  10.24kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      " ---> bc8479139130\n",
      "Step 2/5 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> e044b6f855ab\n",
      "Step 3/5 : RUN pip install pandas pystan==2.19.1.1 prophet pandas-gbq google-cloud-bigquery pyarrow tqdm\n",
      " ---> Using cache\n",
      " ---> 0f84b7c3af82\n",
      "Step 4/5 : COPY fit /fit\n",
      " ---> 9e02b7d5bfa9\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"-m\", \"fit.prophet\"]\n",
      " ---> Running in d767c2788b9e\n",
      "Removing intermediate container d767c2788b9e\n",
      " ---> 0ab6b592a66a\n",
      "Successfully built 0ab6b592a66a\n",
      "Successfully tagged us-central1-docker.pkg.dev/statmike-demo3/statmike-demo3/04f_citibikes:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build {DIR}/model/. -t $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metrics\n",
    "This component calculates custom metrics for all time series and all methods to select the best method per series and prepare a champion prediction file in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image = \"python:3.9\",\n",
    "    packages_to_install = ['pandas','pyarrow','google-cloud-bigquery']\n",
    ")\n",
    "def forecast_metrics(\n",
    "    project: str,\n",
    "    notebook: str,\n",
    "    bqmain: str,\n",
    "    platform: str,\n",
    "    method: str,\n",
    "    scenario: str,\n",
    "    metrics: dsl.Output[dsl.Metrics]\n",
    ") -> NamedTuple('metrics', [('MAPE', float), ('MAE', float), ('pMAE', float)]):\n",
    "\n",
    "    # Setup\n",
    "    from collections import namedtuple\n",
    "    sources = namedtuple('metrics', ['MAPE', 'MAE', 'pMAE'])\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    bigquery = bigquery.Client(project = project)\n",
    "        \n",
    "    # metrics\n",
    "    query = f\"\"\"\n",
    "        WITH\n",
    "            FORECASTS AS (\n",
    "                SELECT\n",
    "                    platform, method, scenario\n",
    "                    date, \n",
    "                    num_trips,\n",
    "                    start_station_name, \n",
    "                    yhat,\n",
    "                    (num_trips - yhat) as diff\n",
    "                FROM `{bqmain}`\n",
    "                WHERE platform = '{platform}' and method = '{method}' and scenario = '{scenario}'\n",
    "            )\n",
    "        SELECT\n",
    "            AVG(ABS(diff)/num_trips) as MAPE,\n",
    "            AVG(ABS(diff)) as MAE,\n",
    "            SUM(ABS(diff))/SUM(num_trips) as pMAE\n",
    "        FROM FORECASTS\n",
    "    \"\"\"\n",
    "    customMetrics = bigquery.query(query = query).to_dataframe()\n",
    "    metrics.log_metric('MAPE', customMetrics['MAPE'][0])\n",
    "    metrics.log_metric('MAE', customMetrics['MAE'][0])\n",
    "    metrics.log_metric('pMAE', customMetrics['pMAE'][0])\n",
    "    \n",
    "    return sources(customMetrics['MAPE'][0], customMetrics['MAE'][0], customMetrics['pMAE'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Champion Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline (KFP) Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name = f'kfp-{NOTEBOOK}-tournament',\n",
    "    pipeline_root = URI + '/kfp/'\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    notebook: str,\n",
    "    bqsource: str\n",
    "):\n",
    "\n",
    "    source = forecast_prep(\n",
    "        project = project,\n",
    "        region = region,\n",
    "        notebook = notebook,\n",
    "        bqsource = bqsource\n",
    "    )\n",
    "    source.set_caching_options(False)\n",
    "        \n",
    "    arima = forecast_bqarima(\n",
    "        project = project,\n",
    "        notebook = notebook,\n",
    "        bqtable = source.outputs['bqtable'],\n",
    "        bqmain = source.outputs['bqmain'],\n",
    "        fh = source.outputs['fh']\n",
    "    )\n",
    "    arima.set_caching_options(False)\n",
    "    \n",
    "    arima_metrics = forecast_metrics(\n",
    "        project = project,\n",
    "        notebook = notebook,\n",
    "        bqmain = source.outputs['bqmain'],\n",
    "        platform = arima.outputs['platform'],\n",
    "        method = arima.outputs['method'],\n",
    "        scenario = arima.outputs['scenario']\n",
    "    )\n",
    "    arima_metrics.set_caching_options(False)\n",
    "    \n",
    "    cwValues = [32, 16, 8, 4, 2, 1, 0]\n",
    "    with dsl.ParallelFor(cwValues) as cw:\n",
    "        \n",
    "        automl = forecast_automl(\n",
    "            project = project,\n",
    "            region = region,\n",
    "            notebook = notebook,\n",
    "            bqtable = source.outputs['bqtable'],\n",
    "            bqmain = source.outputs['bqmain'],\n",
    "            ds = source.outputs['dataset_resource_name'], # vertex ai dataset.resource_name\n",
    "            cw = cw, # context window\n",
    "            fh = source.outputs['fh'] # forecast horizon\n",
    "        )\n",
    "        automl.set_caching_options(False)\n",
    "        \n",
    "        automl_metrics = forecast_metrics(\n",
    "            project = project,\n",
    "            notebook = notebook,\n",
    "            bqmain = source.outputs['bqmain'],\n",
    "            platform = automl.outputs['platform'],\n",
    "            method = automl.outputs['method'],\n",
    "            scenario = automl.outputs['scenario']\n",
    "        )\n",
    "        automl_metrics.set_caching_options(False)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.v2.compiler.Compiler().compile(\n",
    "    pipeline_func = pipeline,\n",
    "    package_path = f\"{DIR}/{NOTEBOOK}.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vertex AI Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name = f\"{NOTEBOOK}_tournament\",\n",
    "    template_path = f\"{DIR}/{NOTEBOOK}.json\",\n",
    "    parameter_values = {\n",
    "        \"project\" : f'{PROJECT_ID}',\n",
    "        \"region\" : f'{REGION}',\n",
    "        \"notebook\" : f'{NOTEBOOK}',\n",
    "        \"bqsource\" : f'{BQ_SOURCE}'\n",
    "    },\n",
    "    enable_caching = None,\n",
    "    labels = {'notebook':f'{NOTEBOOK}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/kfp-04g-tournament-20220319192409?project=715288179162\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/715288179162/locations/us-central1/pipelineJobs/kfp-04g-tournament-20220319192409\n"
     ]
    }
   ],
   "source": [
    "response = pipeline_job.run(\n",
    "    service_account = SERVICE_ACCOUNT,\n",
    "    sync = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Pipeline Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

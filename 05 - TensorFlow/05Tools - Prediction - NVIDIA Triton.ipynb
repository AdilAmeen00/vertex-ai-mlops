{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3338167d-26e6-4c56-9aaa-b7657935fd8b",
   "metadata": {},
   "source": [
    "![ga4](https://www.google-analytics.com/collect?v=2&tid=G-6VDTYWLKX6&cid=1&en=page_view&sid=1&dl=statmike%2Fvertex-ai-mlops%2F05+-+TensorFlow&dt=05Tools+-+Prediction+-+NVIDIA+Triton.ipynb)\n",
    "\n",
    "# 05Tools - Prediction - NVIDIA Triton\n",
    "\n",
    "Throughout the `05` Series of notebooks (05, 05a, ..., 05i) each run of the notebooks results in a new model version for the model created by the notebook.  What if you wanted to host all models and version created by this series on a single endpoint to compare predictions throughout the model training lifecycle?  This workflow uses the feature of NVIDIA Triton Inference Server to accomplish this.\n",
    "\n",
    "This workflow uses a Vertex AI Endpoint with NVIDIA Triton Inference Server to serve predictions - [details](https://cloud.google.com/vertex-ai/docs/predictions/using-nvidia-triton).  This is an open-source inference serving solution from NVIDIA that has many benefits:\n",
    "- Many frameworks: TensorFlow, PyTorch, TensorRT, ONNX, OpenVINO, FIL (XGBoost, LightGBM, Scikit-Learn).\n",
    "- Concurrent Models: multiple models, multiple version of same model\n",
    "- CPU and/or GPU\n",
    "- Ensembles that chain multiple models together, including Python backend for pre and post processing\n",
    "- Dynamic batching to combine incoming requests into batches\n",
    "- Optimization setting for batching rules, rate limiting, prioritization and even response caching\n",
    "\n",
    "\n",
    "Workflow:\n",
    "- Create Triton Server Model Repository: \n",
    "    - Source of Models in Vertex AI Model Registry\n",
    "    - Destination is Vertex AI Model Registry Entry for NVIDIA Triton Inference Server Model Repository\n",
    "- Vertex AI Endpoint: create endpoint and deploy NVIDIA Triton Inference Server Model Repository From Vertex AI Model Registry\n",
    "- Predictions\n",
    "    - From default model and version\n",
    "    - From specific models latest version\n",
    "    - From all models latest version\n",
    "    - From all models and all versions\n",
    "    \n",
    "Resources:\n",
    "- Vertex AI Model Registry\n",
    "- GCS\n",
    "- Vetex AI Endpoints\n",
    "- Artifact Registry\n",
    "\n",
    "Prerequisites:\n",
    "- Multiple of [05, 05a-05i] will create multiple models in the SERIES\n",
    "    - including multiple runs will also create multiple versions of the models\n",
    "\n",
    "References:\n",
    "- [Vertex AI Prediction Endpoints With NVIDIA Triton](https://cloud.google.com/vertex-ai/docs/predictions/using-nvidia-triton)\n",
    "- [NVIDIA Triton Server Ensemble Models (with DALI for preprocessing images)](https://developer.nvidia.com/blog/accelerating-inference-with-triton-inference-server-and-dali/)\n",
    "- [Triton Tutorials](https://github.com/triton-inference-server/tutorials/blob/main/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bcaac8-7d1a-4577-96df-cb6cc5b401ae",
   "metadata": {},
   "source": [
    "---\n",
    "## Installs and API Enablement\n",
    "\n",
    "The clients packages may need installing in this environment.  Also, the API for Artifact Registry needs to be enabled (if not already enabled)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88186f6-e672-4d0b-b232-219193a8e312",
   "metadata": {},
   "source": [
    "### Installs (If Needed)\n",
    "The list `packages` contains tuples of package import names and install names.  If the import name is not found then the install name is used to install quitely for the current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc16a5f-b922-48f4-8738-e5a20dfbafac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuples of (import name, install name)\n",
    "packages = [\n",
    "    ('google.cloud.artifactregistry_v1', 'google-cloud-artifact-registry')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46609c95-8816-4ca6-8be8-b7f27fff403a",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334837d7-f67b-45ad-822b-ddd0f542350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209d2b5-03f1-4fbe-8a50-6b8631237783",
   "metadata": {},
   "source": [
    "### API Enablement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a52cc6-0746-4d2b-9226-cf5be17cb251",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable artifactregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abb516-d82b-48e7-bee3-f477c136bc13",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba42dbc4-9a65-4025-b9bb-6c36481c118a",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d87d88d0-67bd-47aa-93e3-1e282c1cc021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statmike-mlops-349915'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704d0ea3-f588-41e9-a58f-49ed56c5fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "EXPERIMENT = 'triton'\n",
    "SERIES = '05'\n",
    "\n",
    "# source data\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = 'fraud'\n",
    "BQ_TABLE = 'fraud_prepped'\n",
    "\n",
    "# Resources\n",
    "DEPLOY_COMPUTE = 'n1-standard-4'\n",
    "\n",
    "# Model Training\n",
    "VAR_TARGET = 'Class'\n",
    "VAR_OMIT = 'transaction_id' # add more variables to the string with space delimiters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e80c26-f8e4-45cd-9de3-4dfbe6dc5de6",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "8954c99e-1e41-4699-b551-ee27c896c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import artifactregistry_v1\n",
    "from google.cloud import storage\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import multiprocessing\n",
    "from google.api import httpbody_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266960d7-e925-4fad-a4d1-fba7e3da4260",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c07af4f-5b60-4bf1-adc5-44af7b87fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project = PROJECT_ID, location = REGION)\n",
    "bq = bigquery.Client(project = PROJECT_ID)\n",
    "gcs = storage.Client(project = PROJECT_ID)\n",
    "ar_client = artifactregistry_v1.ArtifactRegistryClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c110fbe-299b-4018-b9bc-fa5d5e55c2bb",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2586197-6e5e-49f2-bb72-24e7bcae6c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = PROJECT_ID\n",
    "DIR = f\"temp/{EXPERIMENT}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b66c7-5e5d-455f-b1e1-0b69d262eb4a",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905ee649-162c-410a-aef5-c2024cf36bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {DIR}\n",
    "!mkdir -p {DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b3f78-dbdd-4fe4-b6f9-257a82c5da92",
   "metadata": {},
   "source": [
    "---\n",
    "## Retrieve Records For Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f7ef60a7-4962-4236-a245-ad4f33028ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "samples = bq.query(\n",
    "    query = f\"\"\"\n",
    "        SELECT * EXCEPT({VAR_TARGET}, {VAR_OMIT}, splits)\n",
    "        FROM {BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}\n",
    "        WHERE splits='TEST'\n",
    "        AND {VAR_TARGET} = 1\n",
    "        LIMIT {n}\"\"\"\n",
    ").to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9a3096e7-a2a2-40c1-b7f3-b3e8d068acdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85285</td>\n",
       "      <td>-7.030308</td>\n",
       "      <td>3.421991</td>\n",
       "      <td>-9.525072</td>\n",
       "      <td>5.270891</td>\n",
       "      <td>-4.024630</td>\n",
       "      <td>-2.865682</td>\n",
       "      <td>-6.989195</td>\n",
       "      <td>3.791551</td>\n",
       "      <td>-4.622730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545698</td>\n",
       "      <td>1.103398</td>\n",
       "      <td>-0.541855</td>\n",
       "      <td>0.036943</td>\n",
       "      <td>-0.355519</td>\n",
       "      <td>0.353634</td>\n",
       "      <td>1.042458</td>\n",
       "      <td>1.359516</td>\n",
       "      <td>-0.272188</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56887</td>\n",
       "      <td>-0.075483</td>\n",
       "      <td>1.812355</td>\n",
       "      <td>-2.566981</td>\n",
       "      <td>4.127549</td>\n",
       "      <td>-1.628532</td>\n",
       "      <td>-0.805895</td>\n",
       "      <td>-3.390135</td>\n",
       "      <td>1.019353</td>\n",
       "      <td>-2.451251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.338598</td>\n",
       "      <td>0.794372</td>\n",
       "      <td>0.270471</td>\n",
       "      <td>-0.143624</td>\n",
       "      <td>0.013566</td>\n",
       "      <td>0.634203</td>\n",
       "      <td>0.213693</td>\n",
       "      <td>0.773625</td>\n",
       "      <td>0.387434</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43369</td>\n",
       "      <td>-3.365319</td>\n",
       "      <td>2.426503</td>\n",
       "      <td>-3.752227</td>\n",
       "      <td>0.276017</td>\n",
       "      <td>-2.305870</td>\n",
       "      <td>-1.961578</td>\n",
       "      <td>-3.029283</td>\n",
       "      <td>-1.674462</td>\n",
       "      <td>0.183961</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036837</td>\n",
       "      <td>2.070008</td>\n",
       "      <td>-0.512626</td>\n",
       "      <td>-0.248502</td>\n",
       "      <td>0.126550</td>\n",
       "      <td>0.104166</td>\n",
       "      <td>-1.055997</td>\n",
       "      <td>-1.200165</td>\n",
       "      <td>-1.012066</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143354</td>\n",
       "      <td>1.118331</td>\n",
       "      <td>2.074439</td>\n",
       "      <td>-3.837518</td>\n",
       "      <td>5.448060</td>\n",
       "      <td>0.071816</td>\n",
       "      <td>-1.020509</td>\n",
       "      <td>-1.808574</td>\n",
       "      <td>0.521744</td>\n",
       "      <td>-2.032638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163513</td>\n",
       "      <td>0.289861</td>\n",
       "      <td>-0.172718</td>\n",
       "      <td>-0.021910</td>\n",
       "      <td>-0.376560</td>\n",
       "      <td>0.192817</td>\n",
       "      <td>0.114107</td>\n",
       "      <td>0.500996</td>\n",
       "      <td>0.259533</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0   85285 -7.030308  3.421991 -9.525072  5.270891 -4.024630 -2.865682   \n",
       "1   56887 -0.075483  1.812355 -2.566981  4.127549 -1.628532 -0.805895   \n",
       "2   43369 -3.365319  2.426503 -3.752227  0.276017 -2.305870 -1.961578   \n",
       "3  143354  1.118331  2.074439 -3.837518  5.448060  0.071816 -1.020509   \n",
       "\n",
       "         V7        V8        V9  ...       V20       V21       V22       V23  \\\n",
       "0 -6.989195  3.791551 -4.622730  ...  0.545698  1.103398 -0.541855  0.036943   \n",
       "1 -3.390135  1.019353 -2.451251  ...  0.338598  0.794372  0.270471 -0.143624   \n",
       "2 -3.029283 -1.674462  0.183961  ... -0.036837  2.070008 -0.512626 -0.248502   \n",
       "3 -1.808574  0.521744 -2.032638  ...  0.163513  0.289861 -0.172718 -0.021910   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  \n",
       "0 -0.355519  0.353634  1.042458  1.359516 -0.272188     0.0  \n",
       "1  0.013566  0.634203  0.213693  0.773625  0.387434     5.0  \n",
       "2  0.126550  0.104166 -1.055997 -1.200165 -1.012066    88.0  \n",
       "3 -0.376560  0.192817  0.114107  0.500996  0.259533     1.0  \n",
       "\n",
       "[4 rows x 30 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0815aa-74ae-41ca-93be-79565fcf238d",
   "metadata": {},
   "source": [
    "Remove columns not included as features in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a4ef58a8-03ff-4592-b703-15c85d16c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "newobs = samples.to_dict(orient='records')\n",
    "#newobs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "349640c3-53d5-490b-90f1-ae6056aa913f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "774ea125-6f92-4a6f-b7f1-9a729f1028ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Time': 85285,\n",
       " 'V1': -7.03030814445441,\n",
       " 'V2': 3.4219909046755297,\n",
       " 'V3': -9.52507177254752,\n",
       " 'V4': 5.27089100906596,\n",
       " 'V5': -4.02463027558805,\n",
       " 'V6': -2.86568161775739,\n",
       " 'V7': -6.989194734394459,\n",
       " 'V8': 3.7915509375591294,\n",
       " 'V9': -4.62273033596451,\n",
       " 'V10': -8.40966487562735,\n",
       " 'V11': 6.30904400603177,\n",
       " 'V12': -8.57676143258937,\n",
       " 'V13': 0.24674671692986203,\n",
       " 'V14': -11.534046018150802,\n",
       " 'V15': -0.36426513875870004,\n",
       " 'V16': -5.45249465771382,\n",
       " 'V17': -11.8875700201872,\n",
       " 'V18': -3.5635848100701097,\n",
       " 'V19': 0.8760187681566278,\n",
       " 'V20': 0.545698040621445,\n",
       " 'V21': 1.10339774484256,\n",
       " 'V22': -0.541854751589521,\n",
       " 'V23': 0.0369432219896495,\n",
       " 'V24': -0.355519004066217,\n",
       " 'V25': 0.35363438209700004,\n",
       " 'V26': 1.04245799282131,\n",
       " 'V27': 1.35951563156376,\n",
       " 'V28': -0.272188101257294,\n",
       " 'Amount': 0.0}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newobs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce0628-e979-42db-b27b-04a0d5a6951e",
   "metadata": {},
   "source": [
    "Re-format an instance for prediction with Triton Inference Server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a3f7d15a-179b-4f31-ba7a-8bc930d11fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = dict(\n",
    "    inputs = [\n",
    "        dict(\n",
    "            name = key, \n",
    "            data = [newobs[0][key]], \n",
    "            datatype = 'FP32', \n",
    "            shape = [1,1]\n",
    "        ) for key in newobs[0]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869ed08-c716-4d89-8db4-1a6168914fac",
   "metadata": {},
   "source": [
    "---\n",
    "## Copy Container For Serving\n",
    "Actually, with Vertex AI Prediction Endpoints, we mainly need to satisfy the requirement that the serving container be in Artifact Registry (or GCR).  The process below selects an NVIDIA Triton container, pulls it to the local environment, tags it with the desired name, the pushes it to artifact registry.  Note that no dockerfile was created or run to alter the container here.\n",
    "\n",
    "- Containers: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver\n",
    "- Release Notes: https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/overview.html#overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "4b2562c8-441d-4249-9bdc-bb3d64093824",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nvcr.io/nvidia/tritonserver:23.03-py3',\n",
       " 'us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915/05_triton:23.03')"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRITON_IMAGE = \"nvcr.io/nvidia/tritonserver:23.03-py3\"\n",
    "\n",
    "REPOSITORY = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{PROJECT_ID}\"\n",
    "\n",
    "AR_IMAGE = f\"{REPOSITORY}/{SERIES}_{EXPERIMENT}:{TRITON_IMAGE.split(':')[-1].split('-')[0]}\"\n",
    "\n",
    "TRITON_IMAGE, AR_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "6d26300f-1a27-438e-9aa6-9c5ff983f491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.03-py3: Pulling from nvidia/tritonserver\n",
      "Digest: sha256:10579fb31cb7388501649f610f9fc7cf3f78367c626f33d038a33deda3e0961a\n",
      "Status: Image is up to date for nvcr.io/nvidia/tritonserver:23.03-py3\n",
      "nvcr.io/nvidia/tritonserver:23.03-py3\n"
     ]
    }
   ],
   "source": [
    "!docker pull $TRITON_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "1b51b55e-73e3-45c1-bdc7-7fe4e182ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag $TRITON_IMAGE $AR_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfaa618-3086-40f1-8407-c4526f9e1d28",
   "metadata": {},
   "source": [
    "### Create Docker Image Repository\n",
    "\n",
    "Create an Artifact Registry Repository to hold Docker Images created by this notebook. First, check to see if it is already created by a previous run and retrieve it if it has. Otherwise, create!\n",
    "\n",
    "Name the repository the same name as the `PROJECT_ID`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51439a76-97c4-4210-94b5-5b2e75dc0d69",
   "metadata": {},
   "source": [
    "First, configure `gcloud` as the credential helper for Google Cloud Docker registries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "206ead9a-5ba4-46cd-9f43-88ba5311509d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker $REGION-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c03f4997-6cfb-45c5-8407-b317a09172b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved existing repo: projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "docker_repo = None\n",
    "for repo in ar_client.list_repositories(parent = f'projects/{PROJECT_ID}/locations/{REGION}'):\n",
    "    if f'{PROJECT_ID}' == repo.name.split('/')[-1]:\n",
    "        docker_repo = repo\n",
    "        print(f'Retrieved existing repo: {docker_repo.name}')\n",
    "\n",
    "if not docker_repo:\n",
    "    operation = ar_client.create_repository(\n",
    "        request = artifactregistry_v1.CreateRepositoryRequest(\n",
    "            parent = f'projects/{PROJECT_ID}/locations/{REGION}',\n",
    "            repository_id = f'{PROJECT_ID}',\n",
    "            repository = artifactregistry_v1.Repository(\n",
    "                description = f'A repository for the {SERIES} series that holds docker images.',\n",
    "                name = f'{PROJECT_ID}',\n",
    "                format_ = artifactregistry_v1.Repository.Format.DOCKER,\n",
    "                labels = {'series': SERIES}\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print('Creating Repository ...')\n",
    "    docker_repo = operation.result()\n",
    "    print(f'Completed creating repo: {docker_repo.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "b0156334-1558-48c4-8f48-2264bc6f1b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915',\n",
       " 'DOCKER')"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docker_repo.name, docker_repo.format_.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "5161c0ff-231a-420d-bf7d-7e84af4b47e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPOSITORY = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{docker_repo.name.split('/')[-1]}\"\n",
    "REPOSITORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744dcfe5-217e-4ed5-8093-c77a53d02244",
   "metadata": {},
   "source": [
    "### Push Image to Artifact Registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "1283e6e1-e6f7-4733-b0b7-c67fd93483a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915/05_triton]\n",
      "\n",
      "\u001b[1B8d03f49e: Preparing \n",
      "\u001b[1B1c796ff9: Preparing \n",
      "\u001b[1Bf74e4989: Preparing \n",
      "\u001b[1B1545fba7: Preparing \n",
      "\u001b[1Be14e5b31: Preparing \n",
      "\u001b[1B287202c9: Preparing \n",
      "\u001b[1B4ab46b10: Preparing \n",
      "\u001b[1B1a84df07: Preparing \n",
      "\u001b[1Badf23a62: Preparing \n",
      "\u001b[1B2b52dc10: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B5fc56587: Preparing \n",
      "\u001b[1B474188a6: Preparing \n",
      "\u001b[1Bdb6c3896: Preparing \n",
      "\u001b[1Bb7fd341b: Preparing \n",
      "\u001b[1B232d1291: Preparing \n",
      "\u001b[1B3a4224a1: Preparing \n",
      "\u001b[1Bc02687ba: Preparing \n",
      "\u001b[1Be352f364: Preparing \n",
      "\u001b[1B7d3bab63: Preparing \n",
      "\u001b[1Baaf8cc7e: Preparing \n",
      "\u001b[1B6de4f64c: Preparing \n",
      "\u001b[1Bb45bef95: Preparing \n",
      "\u001b[1Bb8cad89e: Layer already exists \u001b[18A\u001b[2K\u001b[15A\u001b[2K\u001b[12A\u001b[2K\u001b[6A\u001b[2K\u001b[3A\u001b[2K23.03: digest: sha256:b784e0da2d9d1f894366d16be90784e95866e29767b1164e661034964470b5b6 size: 5365\n"
     ]
    }
   ],
   "source": [
    "!docker push $AR_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b25e367-6c9b-41a4-9722-1f5a2f5f0253",
   "metadata": {},
   "source": [
    "---\n",
    "## Create A Triton Server Model Repository\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5d44b-5b9b-44a6-88c7-161cb0fec735",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### List Models\n",
    "This series, `05`, has a multiple workflows that create models that each predict the `Class` of transactions from a fraud dataset. This section will list all models in the series as well as all versions of each model.\n",
    "- [aiplatform.Model.list()](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eff52f49-cccd-4dce-b6ba-51110ec68026",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = aiplatform.Model.list(filter = f'labels.series={SERIES}')\n",
    "# filter it further to just the notebooks in the series 05, 05a-05i\n",
    "models = [model for model in models if model.display_name.startswith('05_0')]\n",
    "models.sort(key = lambda x: x.display_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baf9eebc-fff6-46b4-8cfe-7f4585e0625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05_05\n",
      "05_05a\n",
      "05_05b\n",
      "05_05c\n",
      "05_05d\n",
      "05_05e\n",
      "05_05f\n",
      "05_05g\n",
      "05_05h\n",
      "05_05i\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model.display_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cce00c1e-7bbd-4e15-a85b-2a59b5ba6b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Model object at 0x7f9f784fb7f0> \n",
       "resource name: projects/1026793852137/locations/us-central1/models/model_05_05"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf490b-ebf8-4ada-a99b-02e278c400d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### List Model Versions\n",
    "Each model in the series has 1 or more versions.  List each version.\n",
    "- [aiplatform.Model.versioning_registry](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_versioning_registry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f97e953-ebf7-4578-99f3-ad8ff0a59063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05\n",
      "05_05 ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13']\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05a\n",
      "05_05a ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16']\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05b\n",
      "05_05b ['1', '2', '3', '4']\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05c\n",
      "05_05c ['1', '2', '3']\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05d\n",
      "05_05d ['1', '2', '3']\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05e\n",
      "05_05e ['1', '2', '3']\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05f\n",
      "05_05f ['1', '2', '3', '4', '5', '6', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53']\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05g\n",
      "05_05g ['1', '2', '3']\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05h\n",
      "05_05h ['1', '2', '3']\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05i\n",
      "05_05i ['1', '2', '3', '4']\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    versions = [version.version_id for version in model.versioning_registry.list_versions()]\n",
    "    print(model.display_name, versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "833ad525-0165-4a98-a55a-0681c5c18f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VersionInfo(version_id='1', version_create_time=DatetimeWithNanoseconds(2022, 9, 26, 16, 36, 47, 373777, tzinfo=datetime.timezone.utc), version_update_time=DatetimeWithNanoseconds(2022, 9, 27, 12, 2, 12, 192630, tzinfo=datetime.timezone.utc), model_display_name='05_05', model_resource_name='projects/1026793852137/locations/us-central1/models/model_05_05', version_aliases=['run-20220926162349'], version_description='run-20220926162349')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0].versioning_registry.list_versions()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e25d8e6-7dee-4620-a5ae-f2d6128d9bea",
   "metadata": {},
   "source": [
    "---\n",
    "### Links To Model Version Artifacts\n",
    "\n",
    "Each model version has a `uri` parameter that is a gcs bucket path for the models saved files.  Create a list of tuples with \n",
    "```\n",
    "[(model, [(version.version_id, model.uri), ...]), ...]\n",
    "```\n",
    "\n",
    "[aiplatform.Model.uri](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8d06f9a-078c-46d4-becf-a5e3b888abf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05a\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05b\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05c\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05d\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05e\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05f\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05g\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05h\n",
      "Getting versions for projects/1026793852137/locations/us-central1/models/model_05_05i\n"
     ]
    }
   ],
   "source": [
    "models_artifacts = [\n",
    "    (\n",
    "        model,\n",
    "        [\n",
    "            (\n",
    "                version.version_id,\n",
    "                aiplatform.Model(model_name = f'{model.resource_name}@{version.version_id}').uri\n",
    "            ) for version in model.versioning_registry.list_versions()\n",
    "        ]\n",
    "    ) for model in models\n",
    "]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e4eab-c607-4495-8005-00966b3bc0e3",
   "metadata": {},
   "source": [
    "It's possible that artifacts for a model may have been removed.  To prevent trying to copy model versions to the Triton Server model repository that are empty do a check of the artifact URI and remove any that are missing/empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31bb4c26-71de-4de1-9fbc-b5ae5fe8ae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model model_05_05 version 1 is Missing Artifacts\n",
      "Model model_05_05a version 1 is Missing Artifacts\n",
      "Model model_05_05b version 1 is Missing Artifacts\n",
      "Removing Model@Versions With Missing Artifacts:\n",
      "Removing:  ('1', 'gs://statmike-mlops-349915/05/05b/20220926182813/model')\n",
      "Removing:  ('1', 'gs://statmike-mlops-349915/05/05a/20220926133308/model')\n",
      "Removing:  ('1', 'gs://statmike-mlops-349915/05/05/20220926162349/model')\n"
     ]
    }
   ],
   "source": [
    "bucket = gcs.lookup_bucket(BUCKET)\n",
    "\n",
    "# find versions with missing artifacts:\n",
    "missing = []\n",
    "for m, model in enumerate(models_artifacts):\n",
    "    for v, version in enumerate(model[1]):\n",
    "        blob_list = bucket.list_blobs(max_results = 1, prefix = version[1].split(f'gs://{BUCKET}/')[-1])\n",
    "        if sum(1 for _ in blob_list) == 0:\n",
    "            print(f'Model {model[0].name} version {version[0]} is Missing Artifacts')\n",
    "            missing.append((m,v))\n",
    "# remove versions with missing artifacts:\n",
    "if len(missing) > 0:\n",
    "    print('Removing Model@Versions With Missing Artifacts:')\n",
    "    for r in reversed(missing): # remove in reverse order because using indexes\n",
    "        print('Removing: ', models_artifacts[r[0]][1][r[1]])\n",
    "        models_artifacts[r[0]][1].pop(r[1])\n",
    "# find models with no remaining versions:\n",
    "missing = []\n",
    "for m, model in enumerate(models_artifacts):\n",
    "    if len(model[1]) == 0:\n",
    "        print(f'Model {model[0].display_name} has no remaining versions')\n",
    "        missing.append()\n",
    "# remove models with no remaining versions:\n",
    "if len(missing) > 0:\n",
    "    for r in reversed(missing):\n",
    "        print(f'Removing Model {models_artifacts[m][0].display_name}')\n",
    "        models_artifacts.pop(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434cd6e-789a-4b8a-87c0-727a58b8d2d6",
   "metadata": {},
   "source": [
    "---\n",
    "### Create NVIDIA Triton Model Registry\n",
    "\n",
    "NVIDIA Triton Sever uses a specific folder structure for its model registry.  Naming of files and folders, their order, and contents is specific to the type of model being served as well.  Check the guidelines for [Model Registry](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md) here.\n",
    "\n",
    "The model registry also includes configuration files in the form of `config.pbtxt`.  The contents of these files are model type and model specific.  Check the guidlines for [Model Configuration](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md) here.  Some model types can automatically detect configurations.  This workflow is using TensorFlow models which are automatically configured when the `config.pbtxt` files are missing - see [Auto-Generated Model Configuration](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#auto-generated-model-configuration).\n",
    "\n",
    "```\n",
    "    <model-repository-path>/\n",
    "        <model-name>/\n",
    "            [config.pbtxt]\n",
    "            [<output-labels-file> ...]\n",
    "            <version>/\n",
    "                <model-definition-file>\n",
    "            <version>/\n",
    "                <model-definition-file>\n",
    "            ...\n",
    "        <model-name>/\n",
    "            [config.pbtxt]\n",
    "            [<output-labels-file> ...]\n",
    "            <version>/\n",
    "                <model-definition-file>\n",
    "            <version>/\n",
    "                <model-definition-file>\n",
    "            ...\n",
    "        ...\n",
    "```\n",
    "\n",
    "**Model Loading And Versions**\n",
    "\n",
    "When Triton Inference Server starts up it has a control mode. The default is `--model-control-mode=none` which tries to load all models in the registry. [Reference](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_management.html#model-control-mode-none)\n",
    "\n",
    "This sounds like the perfect solution but it has a limit, it loads the model version specified in the config file for the model - see [Model Configuration Version Policy](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#version-policy).  Remember from the second paragraph of this section that we are taking advantage of automatic configuration so what happens by default?  Well, the default version policy is `version_policy: { latest: { num_versions: 1}}` which is just that lastest version of the model.  To override this we need to provide a config file with the desired version policy that loads all versions: `version_policy: { all: {}}`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49b7daed-78d5-470c-98dd-ab95ff07f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = 'version_policy: { all: {}}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df2cd594-d678-4598-97b5-c191834d5a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = gcs.lookup_bucket(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6fabab83-baac-44f1-b2ce-9f3e9e382baf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://statmike-mlops-349915/05/05/models/20220927110007/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20220927110007/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20220927110007/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20220927184222/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20220927184222/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20220927184222/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20221023210622/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20221023210622/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20221023210622/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230209212046/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230209212046/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230209212046/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230210115433/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230210115433/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230210115433/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230308225745/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230308225745/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230308225745/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230324103811/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230324103811/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230324103811/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230324104933/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230324104933/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230324104933/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230325135459/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230325135459/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230325135459/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230325220538/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230325220538/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230325220538/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230327111418/model/fingerprint.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230327111418/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230327111418/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230327111418/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230327111418/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230327115749/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230327115749/model/fingerprint.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230327115749/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230327115749/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05/models/20230327115749/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20220927105742/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20220927105742/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20220927105742/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20220927105742/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20221024120130/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20221024120130/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20221024120130/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20221024120130/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20221109171913/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20221109171913/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20221109171913/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20221109171913/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230210122632/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230210122632/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230210122632/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230210122632/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230210132930/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230210132930/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230210132930/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230210132930/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230214162254/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230214162254/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230214162254/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230214162254/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230925162315/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230925162315/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230925162315/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230925162315/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230929134956/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230929134956/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230929134956/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230929134956/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230930133138/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230930133138/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230930133138/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20230930133138/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231001235637/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231001235637/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231001235637/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231001235637/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231002102623/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231002102623/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231002102623/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231002102623/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231002112215/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231002112215/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231002112215/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231002112215/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231002164113/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231002164113/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231002164113/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231002164113/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231003161407/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231003161407/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231003161407/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231003161407/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231003202509/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231003202509/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231003202509/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05a/models/20231003202509/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05b/models/20220927105812/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05b/models/20220927105812/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05b/models/20220927105812/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05b/models/20220927105812/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05b/models/20221024121343/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05b/models/20221024121343/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05b/models/20221024121343/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05b/models/20221024121343/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05b/models/20230210130602/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05b/models/20230210130602/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05b/models/20230210130602/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05b/models/20230210130602/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05c/models/20220927094738/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05c/models/20220927094738/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05c/models/20220927094738/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05c/models/20221024121349/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05c/models/20221024121349/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05c/models/20221024121349/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05c/models/20230210130701/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05c/models/20230210130701/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05c/models/20230210130701/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05d/models/20220927154304/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05d/models/20220927154304/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05d/models/20220927154304/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05d/models/20220927154304/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05d/models/20221024130031/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05d/models/20221024130031/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05d/models/20221024130031/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05d/models/20221024130031/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05d/models/20230211141913/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05d/models/20230211141913/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05d/models/20230211141913/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05d/models/20230211141913/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05e/models/20220927181116/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05e/models/20220927181116/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05e/models/20220927181116/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05e/models/20220927181116/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05e/models/20221024130058/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05e/models/20221024130058/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05e/models/20221024130058/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05e/models/20221024130058/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05e/models/20230211141838/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05e/models/20230211141838/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05e/models/20230211141838/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05e/models/20230211141838/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20220927190441/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20220927190441/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20220927190441/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221024130131/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221024130131/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221024130131/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221101224649/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221101224649/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221101224649/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221102030007/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221102030007/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221102030007/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221109040010/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221109040010/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221109040010/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221116040015/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221116040015/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221116040015/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221130040015/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221130040015/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221130040015/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221207040014/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221207040014/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221207040014/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221214040012/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221214040012/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221214040012/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221221040013/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221221040013/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221221040013/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221228040012/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221228040012/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20221228040012/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230104040014/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230104040014/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230104040014/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230111040013/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230111040013/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230111040013/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230118040014/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230118040014/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230118040014/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230125040014/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230125040014/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230125040014/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230201040010/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230201040010/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230201040010/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230208040015/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230208040015/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230208040015/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230211141850/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230211141850/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230211141850/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230215040013/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230215040013/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230215040013/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230222040011/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230222040011/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230222040011/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230301040012/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230301040012/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230301040012/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230308040011/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230308040011/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230308040011/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230315030020/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230315030020/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230315030020/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230322030013/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230322030013/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230322030013/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230329030016/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230329030016/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230329030016/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230405030018/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230405030018/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230405030018/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230412030016/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230412030016/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230412030016/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230419030012/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230419030012/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230419030012/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230426030015/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230426030015/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230426030015/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230503030020/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230503030020/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230503030020/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230510030044/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230510030044/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230510030044/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230517030015/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230517030015/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230517030015/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230524030020/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230524030020/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230524030020/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230531030020/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230531030020/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230531030020/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230607030016/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230607030016/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230607030016/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230614030019/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230614030019/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230614030019/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230621030039/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230621030039/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230621030039/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230628030037/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230628030037/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230628030037/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230705030024/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230705030024/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230705030024/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230712030023/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230712030023/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230712030023/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230719030015/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230719030015/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230719030015/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230726030013/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230726030013/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230726030013/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230802030014/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230802030014/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230802030014/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230809030015/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230809030015/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230809030015/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230816030014/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230816030014/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230816030014/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230823030013/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230823030013/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230823030013/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230830030013/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230830030013/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230830030013/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230906030012/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230906030012/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230906030012/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230913030011/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230913030011/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230913030011/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230920030018/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230920030018/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230920030018/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230927030015/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230927030015/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20230927030015/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20231004030017/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20231004030017/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05f/models/20231004030017/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05g/models/20220927230209/10/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05g/models/20220927230209/10/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05g/models/20220927230209/10/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05g/models/20220927230209/10/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05g/models/20221024135352/4/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05g/models/20221024135352/4/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05g/models/20221024135352/4/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05g/models/20221024135352/4/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05g/models/20230211145013/5/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05g/models/20230211145013/5/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05g/models/20230211145013/5/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05g/models/20230211145013/5/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05h/models/20220927230247/6/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05h/models/20220927230247/6/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05h/models/20220927230247/6/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05h/models/20220927230247/6/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05h/models/20221024135432/17/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05h/models/20221024135432/17/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05h/models/20221024135432/17/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05h/models/20221024135432/17/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05h/models/20230211221917/3/model/keras_metadata.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05h/models/20230211221917/3/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05h/models/20230211221917/3/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05h/models/20230211221917/3/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05i/models/20220928000517/13/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05i/models/20220928000517/13/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05i/models/20220928000517/13/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05i/models/20221024135445/11/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05i/models/20221024135445/11/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05i/models/20221024135445/11/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05i/models/20230203134418/3/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05i/models/20230203134418/3/model/variables/variables.data-00000-of-00001...\n",
      "Copying gs://statmike-mlops-349915/05/05i/models/20230203134418/3/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05i/models/20230211221928/11/model/variables/variables.index...\n",
      "Copying gs://statmike-mlops-349915/05/05i/models/20230211221928/11/model/saved_model.pb...\n",
      "Copying gs://statmike-mlops-349915/05/05i/models/20230211221928/11/model/variables/variables.data-00000-of-00001...\n",
      "/ [3/6 files][519.8 KiB/519.8 KiB]  99% Done                                    \r"
     ]
    }
   ],
   "source": [
    "for model in models_artifacts:\n",
    "    blob = bucket.blob(f'{SERIES}/{EXPERIMENT}/model_repo/{model[0].display_name}/config.pbtxt')\n",
    "    blob.upload_from_string(config)\n",
    "    for version in model[1]:\n",
    "        !gsutil -m cp -r {version[1]} gs://{BUCKET}/{SERIES}/{EXPERIMENT}/model_repo/{model[0].display_name}/{version[0]}/model.savedmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd05e8-936a-491d-95ff-18e01d19cb31",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Container Locally (Optional)\n",
    "\n",
    "This section is optional but helpful if troubleshooting a deployment prior to deploying on Vertex AI Endpoints.  There are a few extra steps taken here to make the container run in a separate process so that the notebook does not get tied up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae1c49-440a-432f-a14e-a624ac09a663",
   "metadata": {},
   "source": [
    "### Run the serving image locally\n",
    "\n",
    "The container is going to be run with commands in this notebook.  In order to run the serving while not tying up further exectutions in this notebook, a subprocess will be launched using `multiprocessing`. To learn more about multiprocessing and running tasks from Python in parallel visit the tips notebook [Python Multiprocessing](../Tips/Python%20Multiprocessing.ipynb). Alternatively, the `-d` option could be used to run the container in detached mode but it is not used here because reviewing the logging of the server is very helpful within the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68850649-e342-4e84-9ae0-43f9b1f76199",
   "metadata": {},
   "source": [
    "First, build the syntax of the `docker run` command.  Note that `-e AIP_MODE = True` is use, which allows the model repository to be set directly from a GCS URI rather than using a `-v local/dir:server/dir` mount.  This could be done but the the `gscfuse` above would need the `-o allow_other` option which is not recommended due to security risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "6fea27ef-d16a-4033-a8ba-e2fb84c0af1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker run -t -p 8000:8000 -p 8001:8001 -p 8002:8002 --rm -e AIP_MODE=True --name=local_triton_server us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915/05_triton:23.03 --model-repository gs://statmike-mlops-349915/05/triton/model_repo\n"
     ]
    }
   ],
   "source": [
    "command = f'''docker run -t -p 8000:8000 -p 8001:8001 -p 8002:8002 --rm \\\n",
    "-e AIP_MODE=True \\\n",
    "--name=local_triton_server \\\n",
    "{AR_IMAGE} \\\n",
    "--model-repository gs://{BUCKET}/{SERIES}/{EXPERIMENT}/model_repo'''\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c030d196-599d-4d65-8d8a-a9086deed62e",
   "metadata": {},
   "source": [
    "Run the command in a subprocess at the local folder of this notebook - use multiprocess.Process():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "a6de61d1-2cb0-467d-b89f-9598a06d0f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertex-ai-mlops/05 - TensorFlow\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "64ce4ace-73ec-4473-aac1-62ee9ac3f547",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "== Triton Inference Server ==\n",
      "=============================\n",
      "\n",
      "NVIDIA Release 23.03 (build 56086596)\n",
      "Triton Server Version 2.32.0\n",
      "\n",
      "Copyright (c) 2018-2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "\n",
      "WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n",
      "   Use the NVIDIA Container Toolkit to start this container with GPU support; see\n",
      "   https://docs.nvidia.com/datacenter/cloud-native/ .\n",
      "\n",
      "W1008 23:45:13.550657 1 pinned_memory_manager.cc:236] Unable to allocate pinned system memory, pinned memory pool will not be available: CUDA driver version is insufficient for CUDA runtime version\n",
      "I1008 23:45:13.553756 1 cuda_memory_manager.cc:115] CUDA memory pool disabled\n",
      "I1008 23:46:35.436124 1 model_lifecycle.cc:459] loading: 05_05c:1\n",
      "I1008 23:46:35.436215 1 model_lifecycle.cc:459] loading: 05_05c:2\n",
      "I1008 23:46:35.436281 1 model_lifecycle.cc:459] loading: 05_05c:3\n",
      "I1008 23:46:36.169965 1 model_lifecycle.cc:459] loading: 05_05a:2\n",
      "I1008 23:46:36.170035 1 model_lifecycle.cc:459] loading: 05_05a:3\n",
      "I1008 23:46:36.170084 1 model_lifecycle.cc:459] loading: 05_05a:4\n",
      "I1008 23:46:36.170125 1 model_lifecycle.cc:459] loading: 05_05a:5\n",
      "I1008 23:46:36.170159 1 model_lifecycle.cc:459] loading: 05_05a:6\n",
      "I1008 23:46:36.170200 1 model_lifecycle.cc:459] loading: 05_05a:7\n",
      "I1008 23:46:36.170265 1 model_lifecycle.cc:459] loading: 05_05a:8\n",
      "I1008 23:46:36.170319 1 model_lifecycle.cc:459] loading: 05_05a:9\n",
      "I1008 23:46:36.170466 1 model_lifecycle.cc:459] loading: 05_05a:10\n",
      "I1008 23:46:36.170530 1 model_lifecycle.cc:459] loading: 05_05a:11\n",
      "I1008 23:46:36.170581 1 model_lifecycle.cc:459] loading: 05_05a:12\n",
      "I1008 23:46:36.170691 1 model_lifecycle.cc:459] loading: 05_05a:13\n",
      "I1008 23:46:36.170759 1 model_lifecycle.cc:459] loading: 05_05a:14\n",
      "I1008 23:46:36.170813 1 model_lifecycle.cc:459] loading: 05_05a:15\n",
      "I1008 23:46:36.170867 1 model_lifecycle.cc:459] loading: 05_05a:16\n",
      "I1008 23:46:36.401797 1 model_lifecycle.cc:459] loading: 05_05g:1\n",
      "I1008 23:46:36.401842 1 model_lifecycle.cc:459] loading: 05_05g:2\n",
      "I1008 23:46:36.401854 1 model_lifecycle.cc:459] loading: 05_05g:3\n",
      "I1008 23:46:36.601801 1 model_lifecycle.cc:459] loading: 05_05e:1\n",
      "I1008 23:46:36.601846 1 model_lifecycle.cc:459] loading: 05_05e:2\n",
      "I1008 23:46:36.601866 1 model_lifecycle.cc:459] loading: 05_05e:3\n",
      "I1008 23:46:39.047377 1 model_lifecycle.cc:459] loading: 05_05f:1\n",
      "I1008 23:46:39.047423 1 model_lifecycle.cc:459] loading: 05_05f:2\n",
      "I1008 23:46:39.047432 1 model_lifecycle.cc:459] loading: 05_05f:3\n",
      "I1008 23:46:39.047442 1 model_lifecycle.cc:459] loading: 05_05f:4\n",
      "I1008 23:46:39.047450 1 model_lifecycle.cc:459] loading: 05_05f:5\n",
      "I1008 23:46:39.047458 1 model_lifecycle.cc:459] loading: 05_05f:6\n",
      "I1008 23:46:39.047467 1 model_lifecycle.cc:459] loading: 05_05f:8\n",
      "I1008 23:46:39.047475 1 model_lifecycle.cc:459] loading: 05_05f:9\n",
      "I1008 23:46:39.047491 1 model_lifecycle.cc:459] loading: 05_05f:10\n",
      "I1008 23:46:39.047500 1 model_lifecycle.cc:459] loading: 05_05f:11\n",
      "I1008 23:46:39.047517 1 model_lifecycle.cc:459] loading: 05_05f:12\n",
      "I1008 23:46:39.047535 1 model_lifecycle.cc:459] loading: 05_05f:13\n",
      "I1008 23:46:39.047551 1 model_lifecycle.cc:459] loading: 05_05f:14\n",
      "I1008 23:46:39.047566 1 model_lifecycle.cc:459] loading: 05_05f:15\n",
      "I1008 23:46:39.047584 1 model_lifecycle.cc:459] loading: 05_05f:16\n",
      "I1008 23:46:39.047596 1 model_lifecycle.cc:459] loading: 05_05f:17\n",
      "I1008 23:46:39.047613 1 model_lifecycle.cc:459] loading: 05_05f:18\n",
      "I1008 23:46:39.047630 1 model_lifecycle.cc:459] loading: 05_05f:19\n",
      "I1008 23:46:39.047652 1 model_lifecycle.cc:459] loading: 05_05f:20\n",
      "I1008 23:46:39.047672 1 model_lifecycle.cc:459] loading: 05_05f:21\n",
      "I1008 23:46:39.047688 1 model_lifecycle.cc:459] loading: 05_05f:22\n",
      "I1008 23:46:39.047708 1 model_lifecycle.cc:459] loading: 05_05f:23\n",
      "I1008 23:46:39.047726 1 model_lifecycle.cc:459] loading: 05_05f:24\n",
      "I1008 23:46:39.047749 1 model_lifecycle.cc:459] loading: 05_05f:25\n",
      "I1008 23:46:39.047766 1 model_lifecycle.cc:459] loading: 05_05f:26\n",
      "I1008 23:46:39.047790 1 model_lifecycle.cc:459] loading: 05_05f:27\n",
      "I1008 23:46:39.047813 1 model_lifecycle.cc:459] loading: 05_05f:28\n",
      "I1008 23:46:39.047829 1 model_lifecycle.cc:459] loading: 05_05f:29\n",
      "I1008 23:46:39.047849 1 model_lifecycle.cc:459] loading: 05_05f:30\n",
      "I1008 23:46:39.047869 1 model_lifecycle.cc:459] loading: 05_05f:31\n",
      "I1008 23:46:39.047888 1 model_lifecycle.cc:459] loading: 05_05f:32\n",
      "I1008 23:46:39.047904 1 model_lifecycle.cc:459] loading: 05_05f:33\n",
      "I1008 23:46:39.047923 1 model_lifecycle.cc:459] loading: 05_05f:34\n",
      "I1008 23:46:39.047938 1 model_lifecycle.cc:459] loading: 05_05f:35\n",
      "I1008 23:46:39.047956 1 model_lifecycle.cc:459] loading: 05_05f:36\n",
      "I1008 23:46:39.047977 1 model_lifecycle.cc:459] loading: 05_05f:37\n",
      "I1008 23:46:39.047994 1 model_lifecycle.cc:459] loading: 05_05f:38\n",
      "I1008 23:46:39.048011 1 model_lifecycle.cc:459] loading: 05_05f:39\n",
      "I1008 23:46:39.048030 1 model_lifecycle.cc:459] loading: 05_05f:40\n",
      "I1008 23:46:39.048060 1 model_lifecycle.cc:459] loading: 05_05f:41\n",
      "I1008 23:46:39.048072 1 model_lifecycle.cc:459] loading: 05_05f:42\n",
      "I1008 23:46:39.048084 1 model_lifecycle.cc:459] loading: 05_05f:43\n",
      "I1008 23:46:39.048097 1 model_lifecycle.cc:459] loading: 05_05f:44\n",
      "I1008 23:46:39.048109 1 model_lifecycle.cc:459] loading: 05_05f:45\n",
      "I1008 23:46:39.048128 1 model_lifecycle.cc:459] loading: 05_05f:46\n",
      "I1008 23:46:39.048148 1 model_lifecycle.cc:459] loading: 05_05f:47\n",
      "I1008 23:46:39.048167 1 model_lifecycle.cc:459] loading: 05_05f:48\n",
      "I1008 23:46:39.048178 1 model_lifecycle.cc:459] loading: 05_05f:49\n",
      "I1008 23:46:39.048197 1 model_lifecycle.cc:459] loading: 05_05f:50\n",
      "I1008 23:46:39.048213 1 model_lifecycle.cc:459] loading: 05_05f:51\n",
      "I1008 23:46:39.048228 1 model_lifecycle.cc:459] loading: 05_05f:52\n",
      "I1008 23:46:39.048243 1 model_lifecycle.cc:459] loading: 05_05f:53\n",
      "I1008 23:46:39.233930 1 model_lifecycle.cc:459] loading: 05_05h:1\n",
      "I1008 23:46:39.233977 1 model_lifecycle.cc:459] loading: 05_05h:2\n",
      "I1008 23:46:39.233988 1 model_lifecycle.cc:459] loading: 05_05h:3\n",
      "I1008 23:46:39.482239 1 model_lifecycle.cc:459] loading: 05_05i:1\n",
      "I1008 23:46:39.482284 1 model_lifecycle.cc:459] loading: 05_05i:2\n",
      "I1008 23:46:39.482293 1 model_lifecycle.cc:459] loading: 05_05i:3\n",
      "I1008 23:46:39.482301 1 model_lifecycle.cc:459] loading: 05_05i:4\n",
      "I1008 23:46:39.684842 1 model_lifecycle.cc:459] loading: 05_05d:1\n",
      "I1008 23:46:39.684892 1 model_lifecycle.cc:459] loading: 05_05d:2\n",
      "I1008 23:46:39.684922 1 model_lifecycle.cc:459] loading: 05_05d:3\n",
      "I1008 23:46:40.276283 1 model_lifecycle.cc:459] loading: 05_05:2\n",
      "I1008 23:46:40.276352 1 model_lifecycle.cc:459] loading: 05_05:3\n",
      "I1008 23:46:40.276372 1 model_lifecycle.cc:459] loading: 05_05:4\n",
      "I1008 23:46:40.276391 1 model_lifecycle.cc:459] loading: 05_05:5\n",
      "I1008 23:46:40.276405 1 model_lifecycle.cc:459] loading: 05_05:6\n",
      "I1008 23:46:40.276421 1 model_lifecycle.cc:459] loading: 05_05:7\n",
      "I1008 23:46:40.276438 1 model_lifecycle.cc:459] loading: 05_05:8\n",
      "I1008 23:46:40.276461 1 model_lifecycle.cc:459] loading: 05_05:9\n",
      "I1008 23:46:40.276481 1 model_lifecycle.cc:459] loading: 05_05:10\n",
      "I1008 23:46:40.276500 1 model_lifecycle.cc:459] loading: 05_05:11\n",
      "I1008 23:46:40.276518 1 model_lifecycle.cc:459] loading: 05_05:12\n",
      "I1008 23:46:40.276550 1 model_lifecycle.cc:459] loading: 05_05:13\n",
      "I1008 23:46:40.461468 1 model_lifecycle.cc:459] loading: 05_05b:2\n",
      "I1008 23:46:40.461522 1 model_lifecycle.cc:459] loading: 05_05b:3\n",
      "I1008 23:46:40.461535 1 model_lifecycle.cc:459] loading: 05_05b:4\n",
      "I1008 23:46:40.833068 1 tensorflow.cc:2565] TRITONBACKEND_Initialize: tensorflow\n",
      "I1008 23:46:40.833118 1 tensorflow.cc:2575] Triton TRITONBACKEND API version: 1.12\n",
      "I1008 23:46:40.833130 1 tensorflow.cc:2581] 'tensorflow' TRITONBACKEND API version: 1.12\n",
      "I1008 23:46:40.833140 1 tensorflow.cc:2605] backend configuration:\n",
      "{\"cmdline\":{\"auto-complete-config\":\"true\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}}\n",
      "I1008 23:46:40.834150 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05c (version 2)\n",
      "2023-10-08 23:46:40.955332: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder6qEI8g/2/model.savedmodel\n",
      "2023-10-08 23:46:40.971728: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:40.971813: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder6qEI8g/2/model.savedmodel\n",
      "2023-10-08 23:46:40.972054: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-08 23:46:40.973129: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/tritonserver/backends/onnxruntime:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-11/lib64\n",
      "2023-10-08 23:46:40.973176: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-10-08 23:46:40.973223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9e411609b03c): /proc/driver/nvidia/version does not exist\n",
      "2023-10-08 23:46:41.045460: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-10-08 23:46:41.050968: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:41.180705: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder6qEI8g/2/model.savedmodel\n",
      "2023-10-08 23:46:41.228465: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 273153 microseconds.\n",
      "W1008 23:46:41.228590 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05c' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:41.251256 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05c (version 3)\n",
      "2023-10-08 23:46:41.252010: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldernJs9Zg/3/model.savedmodel\n",
      "2023-10-08 23:46:41.266806: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:41.266881: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldernJs9Zg/3/model.savedmodel\n",
      "2023-10-08 23:46:41.306348: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:41.380860: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldernJs9Zg/3/model.savedmodel\n",
      "2023-10-08 23:46:41.423350: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 171393 microseconds.\n",
      "W1008 23:46:41.423454 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05c' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:41.446679 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05c (version 1)\n",
      "2023-10-08 23:46:41.447560: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderlhx8gj/1/model.savedmodel\n",
      "2023-10-08 23:46:41.463423: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:41.463498: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderlhx8gj/1/model.savedmodel\n",
      "2023-10-08 23:46:41.511019: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:41.597726: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderlhx8gj/1/model.savedmodel\n",
      "2023-10-08 23:46:41.645554: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 198004 microseconds.\n",
      "W1008 23:46:41.645674 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05c' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:41.669598 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05c_0 (CPU device 0)\n",
      "2023-10-08 23:46:41.669754: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder6qEI8g/2/model.savedmodel\n",
      "2023-10-08 23:46:41.679191: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:41.679285: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder6qEI8g/2/model.savedmodel\n",
      "2023-10-08 23:46:41.704176: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:41.783178: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder6qEI8g/2/model.savedmodel\n",
      "2023-10-08 23:46:41.826474: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 156729 microseconds.\n",
      "I1008 23:46:41.826728 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05c_0 (CPU device 0)\n",
      "2023-10-08 23:46:41.826825: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldernJs9Zg/3/model.savedmodel\n",
      "2023-10-08 23:46:41.835427: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:41.835492: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldernJs9Zg/3/model.savedmodel\n",
      "2023-10-08 23:46:41.859039: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:41.931502: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldernJs9Zg/3/model.savedmodel\n",
      "2023-10-08 23:46:41.970398: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 143578 microseconds.\n",
      "I1008 23:46:41.970665 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05c_0 (CPU device 0)\n",
      "2023-10-08 23:46:41.970857: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderlhx8gj/1/model.savedmodel\n",
      "2023-10-08 23:46:41.978628: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:41.978693: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderlhx8gj/1/model.savedmodel\n",
      "2023-10-08 23:46:42.004391: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:42.081036: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderlhx8gj/1/model.savedmodel\n",
      "2023-10-08 23:46:42.127229: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 156379 microseconds.\n",
      "I1008 23:46:42.127502 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05c_1 (CPU device 0)\n",
      "2023-10-08 23:46:42.127617: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder6qEI8g/2/model.savedmodel\n",
      "2023-10-08 23:46:42.135375: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:42.135445: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder6qEI8g/2/model.savedmodel\n",
      "2023-10-08 23:46:42.172937: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:42.249377: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder6qEI8g/2/model.savedmodel\n",
      "2023-10-08 23:46:42.290658: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 163049 microseconds.\n",
      "I1008 23:46:42.290954 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05c_1 (CPU device 0)\n",
      "2023-10-08 23:46:42.291158: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldernJs9Zg/3/model.savedmodel\n",
      "2023-10-08 23:46:42.301090: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:42.301161: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldernJs9Zg/3/model.savedmodel\n",
      "2023-10-08 23:46:42.336302: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:42.417486: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldernJs9Zg/3/model.savedmodel\n",
      "2023-10-08 23:46:42.464312: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 173162 microseconds.\n",
      "I1008 23:46:42.464584 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05c_1 (CPU device 0)\n",
      "2023-10-08 23:46:42.464702: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderlhx8gj/1/model.savedmodel\n",
      "2023-10-08 23:46:42.473387: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:42.473447: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderlhx8gj/1/model.savedmodel\n",
      "2023-10-08 23:46:42.514285: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:42.601434: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderlhx8gj/1/model.savedmodel\n",
      "2023-10-08 23:46:42.648386: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 183687 microseconds.\n",
      "I1008 23:46:42.649014 1 model_lifecycle.cc:694] successfully loaded '05_05c' version 1\n",
      "I1008 23:46:42.649044 1 model_lifecycle.cc:694] successfully loaded '05_05c' version 1\n",
      "I1008 23:46:42.649051 1 model_lifecycle.cc:694] successfully loaded '05_05c' version 1\n",
      "I1008 23:46:44.429489 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05a (version 3)\n",
      "2023-10-08 23:46:44.430250: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder5ZFswj/3/model.savedmodel\n",
      "2023-10-08 23:46:44.447328: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:44.447422: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder5ZFswj/3/model.savedmodel\n",
      "2023-10-08 23:46:44.492499: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:44.566206: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder5ZFswj/3/model.savedmodel\n",
      "2023-10-08 23:46:44.606940: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 176704 microseconds.\n",
      "W1008 23:46:44.607074 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05a' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:44.633759 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05a (version 5)\n",
      "2023-10-08 23:46:44.634550: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldernGkINi/5/model.savedmodel\n",
      "2023-10-08 23:46:44.648680: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:44.648749: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldernGkINi/5/model.savedmodel\n",
      "2023-10-08 23:46:44.688254: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:44.761038: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldernGkINi/5/model.savedmodel\n",
      "2023-10-08 23:46:44.802360: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 167824 microseconds.\n",
      "W1008 23:46:44.802510 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05a' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:44.822228 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05a (version 2)\n",
      "2023-10-08 23:46:44.822909: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderTIQCog/2/model.savedmodel\n",
      "2023-10-08 23:46:44.837033: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:44.837102: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderTIQCog/2/model.savedmodel\n",
      "2023-10-08 23:46:44.876600: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:44.961672: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderTIQCog/2/model.savedmodel\n",
      "2023-10-08 23:46:45.008167: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 185267 microseconds.\n",
      "W1008 23:46:45.008281 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05a' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:45.031426 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05a (version 6)\n",
      "2023-10-08 23:46:45.032337: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderJSLy2e/6/model.savedmodel\n",
      "2023-10-08 23:46:45.045428: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:45.045519: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderJSLy2e/6/model.savedmodel\n",
      "2023-10-08 23:46:45.082928: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:45.157019: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderJSLy2e/6/model.savedmodel\n",
      "2023-10-08 23:46:45.196546: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 164222 microseconds.\n",
      "W1008 23:46:45.196663 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05a' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:45.217735 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05a (version 4)\n",
      "2023-10-08 23:46:45.218351: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderO51yXf/4/model.savedmodel\n",
      "2023-10-08 23:46:45.231802: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:45.231875: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderO51yXf/4/model.savedmodel\n",
      "2023-10-08 23:46:45.268648: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:45.338993: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderO51yXf/4/model.savedmodel\n",
      "2023-10-08 23:46:45.377500: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 159159 microseconds.\n",
      "W1008 23:46:45.377656 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05a' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:45.398629 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_0 (CPU device 0)\n",
      "2023-10-08 23:46:45.398760: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder5ZFswj/3/model.savedmodel\n",
      "2023-10-08 23:46:45.406320: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:45.406381: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder5ZFswj/3/model.savedmodel\n",
      "2023-10-08 23:46:45.427628: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:45.495129: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder5ZFswj/3/model.savedmodel\n",
      "2023-10-08 23:46:45.535492: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 136739 microseconds.\n",
      "I1008 23:46:45.535832 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_0 (CPU device 0)\n",
      "2023-10-08 23:46:45.536191: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldernGkINi/5/model.savedmodel\n",
      "2023-10-08 23:46:45.542968: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:45.543028: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldernGkINi/5/model.savedmodel\n",
      "2023-10-08 23:46:45.563757: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:45.636529: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldernGkINi/5/model.savedmodel\n",
      "2023-10-08 23:46:45.676619: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 140437 microseconds.\n",
      "I1008 23:46:45.676888 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_0 (CPU device 0)\n",
      "2023-10-08 23:46:45.677059: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderTIQCog/2/model.savedmodel\n",
      "2023-10-08 23:46:45.684551: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:45.684615: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderTIQCog/2/model.savedmodel\n",
      "2023-10-08 23:46:45.707992: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:45.783804: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderTIQCog/2/model.savedmodel\n",
      "2023-10-08 23:46:45.827738: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 150685 microseconds.\n",
      "I1008 23:46:45.827997 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_0 (CPU device 0)\n",
      "2023-10-08 23:46:45.828127: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderJSLy2e/6/model.savedmodel\n",
      "2023-10-08 23:46:45.835764: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:45.835834: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderJSLy2e/6/model.savedmodel\n",
      "2023-10-08 23:46:45.860229: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:45.930635: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderJSLy2e/6/model.savedmodel\n",
      "2023-10-08 23:46:45.970024: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 141905 microseconds.\n",
      "I1008 23:46:45.970285 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_0 (CPU device 0)\n",
      "2023-10-08 23:46:45.970703: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderO51yXf/4/model.savedmodel\n",
      "2023-10-08 23:46:45.977478: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:45.977535: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderO51yXf/4/model.savedmodel\n",
      "2023-10-08 23:46:46.000670: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:46.069776: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderO51yXf/4/model.savedmodel\n",
      "2023-10-08 23:46:46.109701: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 139004 microseconds.\n",
      "I1008 23:46:46.109934 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_1 (CPU device 0)\n",
      "2023-10-08 23:46:46.110108: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder5ZFswj/3/model.savedmodel\n",
      "2023-10-08 23:46:46.116994: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:46.117062: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder5ZFswj/3/model.savedmodel\n",
      "2023-10-08 23:46:46.150842: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:46.226293: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder5ZFswj/3/model.savedmodel\n",
      "2023-10-08 23:46:46.267304: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 157208 microseconds.\n",
      "I1008 23:46:46.267613 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_1 (CPU device 0)\n",
      "2023-10-08 23:46:46.267713: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldernGkINi/5/model.savedmodel\n",
      "2023-10-08 23:46:46.275864: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:46.275926: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldernGkINi/5/model.savedmodel\n",
      "2023-10-08 23:46:46.308929: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:46.383016: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldernGkINi/5/model.savedmodel\n",
      "2023-10-08 23:46:46.431014: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 163306 microseconds.\n",
      "I1008 23:46:46.431311 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_1 (CPU device 0)\n",
      "2023-10-08 23:46:46.431681: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderTIQCog/2/model.savedmodel\n",
      "2023-10-08 23:46:46.439878: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:46.439946: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderTIQCog/2/model.savedmodel\n",
      "2023-10-08 23:46:46.477046: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:46.559629: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderTIQCog/2/model.savedmodel\n",
      "2023-10-08 23:46:46.603166: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 171495 microseconds.\n",
      "I1008 23:46:46.603472 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_1 (CPU device 0)\n",
      "2023-10-08 23:46:46.603595: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderJSLy2e/6/model.savedmodel\n",
      "2023-10-08 23:46:46.614611: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:46.614685: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderJSLy2e/6/model.savedmodel\n",
      "2023-10-08 23:46:46.647659: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:46.728784: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderJSLy2e/6/model.savedmodel\n",
      "2023-10-08 23:46:46.773461: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 169871 microseconds.\n",
      "I1008 23:46:46.773787 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_1 (CPU device 0)\n",
      "2023-10-08 23:46:46.773888: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderO51yXf/4/model.savedmodel\n",
      "2023-10-08 23:46:46.782900: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:46.782967: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderO51yXf/4/model.savedmodel\n",
      "2023-10-08 23:46:46.817706: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:46.897931: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderO51yXf/4/model.savedmodel\n",
      "2023-10-08 23:46:46.943355: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 169473 microseconds.\n",
      "I1008 23:46:49.834218 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05a (version 7)\n",
      "2023-10-08 23:46:49.834897: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderaSxS2f/7/model.savedmodel\n",
      "2023-10-08 23:46:49.844307: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:49.844396: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderaSxS2f/7/model.savedmodel\n",
      "2023-10-08 23:46:49.880601: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:49.959561: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderaSxS2f/7/model.savedmodel\n",
      "2023-10-08 23:46:50.001264: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 166377 microseconds.\n",
      "W1008 23:46:50.001419 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05a' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:50.026675 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05a (version 8)\n",
      "2023-10-08 23:46:50.027387: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderGf5lqj/8/model.savedmodel\n",
      "2023-10-08 23:46:50.034709: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:50.034783: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderGf5lqj/8/model.savedmodel\n",
      "2023-10-08 23:46:50.071669: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:50.150247: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderGf5lqj/8/model.savedmodel\n",
      "2023-10-08 23:46:50.192374: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 164997 microseconds.\n",
      "W1008 23:46:50.192677 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05a' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:50.217171 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_0 (CPU device 0)\n",
      "2023-10-08 23:46:50.217337: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderaSxS2f/7/model.savedmodel\n",
      "2023-10-08 23:46:50.226676: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:50.226744: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderaSxS2f/7/model.savedmodel\n",
      "2023-10-08 23:46:50.248988: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:50.319368: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderaSxS2f/7/model.savedmodel\n",
      "2023-10-08 23:46:50.363428: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 146078 microseconds.\n",
      "I1008 23:46:50.363703 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_0 (CPU device 0)\n",
      "2023-10-08 23:46:50.364024: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderGf5lqj/8/model.savedmodel\n",
      "2023-10-08 23:46:50.373302: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:50.373368: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderGf5lqj/8/model.savedmodel\n",
      "2023-10-08 23:46:50.396398: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:50.467819: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderGf5lqj/8/model.savedmodel\n",
      "2023-10-08 23:46:50.510568: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 146554 microseconds.\n",
      "I1008 23:46:50.510841 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05a (version 9)\n",
      "2023-10-08 23:46:50.511644: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder3jwkEf/9/model.savedmodel\n",
      "2023-10-08 23:46:50.519302: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:50.519372: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder3jwkEf/9/model.savedmodel\n",
      "2023-10-08 23:46:50.552933: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:50.630436: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder3jwkEf/9/model.savedmodel\n",
      "2023-10-08 23:46:50.671852: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 160215 microseconds.\n",
      "W1008 23:46:50.671963 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05a' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:50.696533 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_1 (CPU device 0)\n",
      "2023-10-08 23:46:50.696679: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderaSxS2f/7/model.savedmodel\n",
      "2023-10-08 23:46:50.704355: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:50.704423: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderaSxS2f/7/model.savedmodel\n",
      "2023-10-08 23:46:50.740494: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:50.820280: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderaSxS2f/7/model.savedmodel\n",
      "2023-10-08 23:46:50.865235: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 168569 microseconds.\n",
      "I1008 23:46:50.865529 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_1 (CPU device 0)\n",
      "2023-10-08 23:46:50.865845: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderGf5lqj/8/model.savedmodel\n",
      "2023-10-08 23:46:50.872468: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:50.872528: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderGf5lqj/8/model.savedmodel\n",
      "2023-10-08 23:46:50.905774: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:50.983855: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderGf5lqj/8/model.savedmodel\n",
      "2023-10-08 23:46:51.031866: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 166029 microseconds.\n",
      "I1008 23:46:51.032157 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_0 (CPU device 0)\n",
      "2023-10-08 23:46:51.032486: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder3jwkEf/9/model.savedmodel\n",
      "2023-10-08 23:46:51.041521: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:51.041593: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder3jwkEf/9/model.savedmodel\n",
      "2023-10-08 23:46:51.067173: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:51.139978: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder3jwkEf/9/model.savedmodel\n",
      "2023-10-08 23:46:51.181924: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 149641 microseconds.\n",
      "I1008 23:46:51.182501 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_1 (CPU device 0)\n",
      "2023-10-08 23:46:51.182614: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder3jwkEf/9/model.savedmodel\n",
      "2023-10-08 23:46:51.189350: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:51.189407: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder3jwkEf/9/model.savedmodel\n",
      "2023-10-08 23:46:51.235961: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:51.317721: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder3jwkEf/9/model.savedmodel\n",
      "2023-10-08 23:46:51.363099: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 180490 microseconds.\n",
      "I1008 23:46:53.338655 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05g (version 1)\n",
      "2023-10-08 23:46:53.339316: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderkKe4ug/1/model.savedmodel\n",
      "2023-10-08 23:46:53.347240: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:53.347315: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderkKe4ug/1/model.savedmodel\n",
      "2023-10-08 23:46:53.385767: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:53.482878: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderkKe4ug/1/model.savedmodel\n",
      "2023-10-08 23:46:53.532115: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 192810 microseconds.\n",
      "W1008 23:46:53.532245 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05g' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:53.556860 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05g_0 (CPU device 0)\n",
      "2023-10-08 23:46:53.556961: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderkKe4ug/1/model.savedmodel\n",
      "2023-10-08 23:46:53.564374: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:53.564447: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderkKe4ug/1/model.savedmodel\n",
      "2023-10-08 23:46:53.590907: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:53.682732: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderkKe4ug/1/model.savedmodel\n",
      "2023-10-08 23:46:53.729824: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 172870 microseconds.\n",
      "I1008 23:46:53.730674 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05a (version 10)\n",
      "2023-10-08 23:46:53.731506: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderGd5Tlj/10/model.savedmodel\n",
      "2023-10-08 23:46:53.739519: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:53.739588: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderGd5Tlj/10/model.savedmodel\n",
      "2023-10-08 23:46:53.774752: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:53.853364: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderGd5Tlj/10/model.savedmodel\n",
      "2023-10-08 23:46:53.893752: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 162253 microseconds.\n",
      "W1008 23:46:53.893869 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05a' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:53.918032 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05g_1 (CPU device 0)\n",
      "2023-10-08 23:46:53.918474: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderkKe4ug/1/model.savedmodel\n",
      "2023-10-08 23:46:53.926335: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:53.926402: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderkKe4ug/1/model.savedmodel\n",
      "2023-10-08 23:46:53.964909: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:54.065670: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderkKe4ug/1/model.savedmodel\n",
      "2023-10-08 23:46:54.111796: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 193337 microseconds.\n",
      "I1008 23:46:54.112064 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05a (version 12)\n",
      "2023-10-08 23:46:54.112980: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderJsHUzg/12/model.savedmodel\n",
      "2023-10-08 23:46:54.120838: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:54.120914: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderJsHUzg/12/model.savedmodel\n",
      "2023-10-08 23:46:54.163661: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:54.248477: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderJsHUzg/12/model.savedmodel\n",
      "2023-10-08 23:46:54.291185: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 178215 microseconds.\n",
      "W1008 23:46:54.291412 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05a' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:54.315578 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_0 (CPU device 0)\n",
      "2023-10-08 23:46:54.315746: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderGd5Tlj/10/model.savedmodel\n",
      "2023-10-08 23:46:54.323272: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:54.323340: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderGd5Tlj/10/model.savedmodel\n",
      "2023-10-08 23:46:54.344604: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:54.414308: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderGd5Tlj/10/model.savedmodel\n",
      "2023-10-08 23:46:54.455522: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 139786 microseconds.\n",
      "I1008 23:46:54.455801 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05a (version 13)\n",
      "2023-10-08 23:46:54.456500: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderMBKfXg/13/model.savedmodel\n",
      "2023-10-08 23:46:54.463632: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:54.463696: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderMBKfXg/13/model.savedmodel\n",
      "2023-10-08 23:46:54.497804: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:54.572744: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderMBKfXg/13/model.savedmodel\n",
      "2023-10-08 23:46:54.613227: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 156735 microseconds.\n",
      "W1008 23:46:54.613360 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05a' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:54.636800 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05a (version 11)\n",
      "2023-10-08 23:46:54.637538: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderqMORti/11/model.savedmodel\n",
      "2023-10-08 23:46:54.644546: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:54.644614: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderqMORti/11/model.savedmodel\n",
      "2023-10-08 23:46:54.677901: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:54.750873: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderqMORti/11/model.savedmodel\n",
      "2023-10-08 23:46:54.792897: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 155370 microseconds.\n",
      "W1008 23:46:54.793026 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05a' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:54.814823 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_0 (CPU device 0)\n",
      "2023-10-08 23:46:54.814965: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderJsHUzg/12/model.savedmodel\n",
      "2023-10-08 23:46:54.821864: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:54.821938: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderJsHUzg/12/model.savedmodel\n",
      "2023-10-08 23:46:54.845043: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:54.913540: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderJsHUzg/12/model.savedmodel\n",
      "2023-10-08 23:46:54.955171: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 140215 microseconds.\n",
      "I1008 23:46:54.955439 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05a (version 14)\n",
      "2023-10-08 23:46:54.956253: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder4gkujh/14/model.savedmodel\n",
      "2023-10-08 23:46:54.963957: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:54.964024: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder4gkujh/14/model.savedmodel\n",
      "2023-10-08 23:46:54.998131: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:55.076270: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder4gkujh/14/model.savedmodel\n",
      "2023-10-08 23:46:55.118871: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 162624 microseconds.\n",
      "W1008 23:46:55.119018 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05a' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:55.141940 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_1 (CPU device 0)\n",
      "2023-10-08 23:46:55.142098: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderGd5Tlj/10/model.savedmodel\n",
      "2023-10-08 23:46:55.152281: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:55.152362: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderGd5Tlj/10/model.savedmodel\n",
      "2023-10-08 23:46:55.185734: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:55.260845: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderGd5Tlj/10/model.savedmodel\n",
      "2023-10-08 23:46:55.301216: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 159129 microseconds.\n",
      "I1008 23:46:55.301683 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_0 (CPU device 0)\n",
      "2023-10-08 23:46:55.301794: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderMBKfXg/13/model.savedmodel\n",
      "2023-10-08 23:46:55.310341: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:55.310405: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderMBKfXg/13/model.savedmodel\n",
      "2023-10-08 23:46:55.332315: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:55.399323: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderMBKfXg/13/model.savedmodel\n",
      "2023-10-08 23:46:55.440182: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 138392 microseconds.\n",
      "I1008 23:46:55.440470 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_0 (CPU device 0)\n",
      "2023-10-08 23:46:55.440730: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderqMORti/11/model.savedmodel\n",
      "2023-10-08 23:46:55.447553: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:55.447607: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderqMORti/11/model.savedmodel\n",
      "2023-10-08 23:46:55.471251: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:55.541077: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderqMORti/11/model.savedmodel\n",
      "2023-10-08 23:46:55.582604: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 141883 microseconds.\n",
      "I1008 23:46:55.582861 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_1 (CPU device 0)\n",
      "2023-10-08 23:46:55.583052: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderJsHUzg/12/model.savedmodel\n",
      "2023-10-08 23:46:55.590921: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:55.590981: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderJsHUzg/12/model.savedmodel\n",
      "2023-10-08 23:46:55.625179: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:55.700513: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderJsHUzg/12/model.savedmodel\n",
      "2023-10-08 23:46:55.743952: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 160909 microseconds.\n",
      "I1008 23:46:55.744229 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_0 (CPU device 0)\n",
      "2023-10-08 23:46:55.744666: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder4gkujh/14/model.savedmodel\n",
      "2023-10-08 23:46:55.752490: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:55.752562: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder4gkujh/14/model.savedmodel\n",
      "2023-10-08 23:46:55.774193: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:55.848460: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder4gkujh/14/model.savedmodel\n",
      "2023-10-08 23:46:55.889687: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 145032 microseconds.\n",
      "I1008 23:46:55.889975 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_1 (CPU device 0)\n",
      "2023-10-08 23:46:55.890357: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderMBKfXg/13/model.savedmodel\n",
      "2023-10-08 23:46:55.899884: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:55.899949: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderMBKfXg/13/model.savedmodel\n",
      "2023-10-08 23:46:55.933956: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:56.008726: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderMBKfXg/13/model.savedmodel\n",
      "2023-10-08 23:46:56.052022: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 161676 microseconds.\n",
      "I1008 23:46:56.052349 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_1 (CPU device 0)\n",
      "2023-10-08 23:46:56.052634: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderqMORti/11/model.savedmodel\n",
      "2023-10-08 23:46:56.061913: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:56.061985: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderqMORti/11/model.savedmodel\n",
      "2023-10-08 23:46:56.096780: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:56.172909: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderqMORti/11/model.savedmodel\n",
      "2023-10-08 23:46:56.216537: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 163912 microseconds.\n",
      "I1008 23:46:56.216830 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05g (version 2)\n",
      "2023-10-08 23:46:56.217738: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderogSjwf/2/model.savedmodel\n",
      "2023-10-08 23:46:56.226042: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:56.226111: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderogSjwf/2/model.savedmodel\n",
      "2023-10-08 23:46:56.262684: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:56.345517: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderogSjwf/2/model.savedmodel\n",
      "2023-10-08 23:46:56.390765: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 173033 microseconds.\n",
      "W1008 23:46:56.390889 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05g' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:56.415198 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_1 (CPU device 0)\n",
      "2023-10-08 23:46:56.415373: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder4gkujh/14/model.savedmodel\n",
      "2023-10-08 23:46:56.423662: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:56.423743: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder4gkujh/14/model.savedmodel\n",
      "2023-10-08 23:46:56.458681: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:56.539023: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder4gkujh/14/model.savedmodel\n",
      "2023-10-08 23:46:56.580576: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 165215 microseconds.\n",
      "I1008 23:46:56.580852 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05g_0 (CPU device 0)\n",
      "2023-10-08 23:46:56.581148: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderogSjwf/2/model.savedmodel\n",
      "2023-10-08 23:46:56.590099: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:56.590169: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderogSjwf/2/model.savedmodel\n",
      "2023-10-08 23:46:56.612134: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:56.694411: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderogSjwf/2/model.savedmodel\n",
      "2023-10-08 23:46:56.739409: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 158270 microseconds.\n",
      "I1008 23:46:56.739900 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05g_1 (CPU device 0)\n",
      "2023-10-08 23:46:56.739999: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderogSjwf/2/model.savedmodel\n",
      "2023-10-08 23:46:56.747634: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:56.747707: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderogSjwf/2/model.savedmodel\n",
      "2023-10-08 23:46:56.781735: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:56.873511: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderogSjwf/2/model.savedmodel\n",
      "2023-10-08 23:46:56.915864: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 175871 microseconds.\n",
      "I1008 23:46:56.945653 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05g (version 3)\n",
      "2023-10-08 23:46:56.946393: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderlXihqi/3/model.savedmodel\n",
      "2023-10-08 23:46:56.954348: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:56.954412: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderlXihqi/3/model.savedmodel\n",
      "2023-10-08 23:46:56.988525: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:57.075930: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderlXihqi/3/model.savedmodel\n",
      "2023-10-08 23:46:57.132197: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 185812 microseconds.\n",
      "W1008 23:46:57.132330 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05g' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:57.156533 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05g_0 (CPU device 0)\n",
      "2023-10-08 23:46:57.156636: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderlXihqi/3/model.savedmodel\n",
      "2023-10-08 23:46:57.164634: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:57.164702: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderlXihqi/3/model.savedmodel\n",
      "2023-10-08 23:46:57.186011: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:57.260465: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderlXihqi/3/model.savedmodel\n",
      "2023-10-08 23:46:57.303062: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 146433 microseconds.\n",
      "I1008 23:46:57.303542 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05g_1 (CPU device 0)\n",
      "2023-10-08 23:46:57.303808: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderlXihqi/3/model.savedmodel\n",
      "2023-10-08 23:46:57.311910: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:57.311970: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderlXihqi/3/model.savedmodel\n",
      "2023-10-08 23:46:57.345148: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:57.425221: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderlXihqi/3/model.savedmodel\n",
      "2023-10-08 23:46:57.466386: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 162598 microseconds.\n",
      "I1008 23:46:57.467090 1 model_lifecycle.cc:694] successfully loaded '05_05g' version 3\n",
      "I1008 23:46:57.467157 1 model_lifecycle.cc:694] successfully loaded '05_05g' version 3\n",
      "I1008 23:46:57.467173 1 model_lifecycle.cc:694] successfully loaded '05_05g' version 3\n",
      "I1008 23:46:57.694029 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05e (version 1)\n",
      "2023-10-08 23:46:57.694684: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderIfUyNf/1/model.savedmodel\n",
      "2023-10-08 23:46:57.701999: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:57.702065: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderIfUyNf/1/model.savedmodel\n",
      "2023-10-08 23:46:57.745662: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:57.829445: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderIfUyNf/1/model.savedmodel\n",
      "2023-10-08 23:46:57.875262: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 180587 microseconds.\n",
      "W1008 23:46:57.875385 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05e' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:57.897357 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05e (version 2)\n",
      "2023-10-08 23:46:57.898167: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldervxnflg/2/model.savedmodel\n",
      "2023-10-08 23:46:57.905983: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:57.906054: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldervxnflg/2/model.savedmodel\n",
      "2023-10-08 23:46:57.941284: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:58.016800: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldervxnflg/2/model.savedmodel\n",
      "2023-10-08 23:46:58.059268: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 161111 microseconds.\n",
      "W1008 23:46:58.059397 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05e' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:58.080314 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05e (version 3)\n",
      "2023-10-08 23:46:58.081211: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder2EjC9i/3/model.savedmodel\n",
      "2023-10-08 23:46:58.089018: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:58.089087: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder2EjC9i/3/model.savedmodel\n",
      "2023-10-08 23:46:58.122539: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:58.198489: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder2EjC9i/3/model.savedmodel\n",
      "2023-10-08 23:46:58.240454: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 159256 microseconds.\n",
      "W1008 23:46:58.240584 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05e' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:58.260818 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05e_0 (CPU device 0)\n",
      "2023-10-08 23:46:58.261015: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderIfUyNf/1/model.savedmodel\n",
      "2023-10-08 23:46:58.269089: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:58.269163: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderIfUyNf/1/model.savedmodel\n",
      "2023-10-08 23:46:58.293759: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:58.371639: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderIfUyNf/1/model.savedmodel\n",
      "2023-10-08 23:46:58.413189: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 152232 microseconds.\n",
      "I1008 23:46:58.413457 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05e_0 (CPU device 0)\n",
      "2023-10-08 23:46:58.413706: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldervxnflg/2/model.savedmodel\n",
      "2023-10-08 23:46:58.420607: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:58.420666: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldervxnflg/2/model.savedmodel\n",
      "2023-10-08 23:46:58.440693: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:58.504591: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldervxnflg/2/model.savedmodel\n",
      "2023-10-08 23:46:58.541482: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 127785 microseconds.\n",
      "I1008 23:46:58.541764 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05a (version 15)\n",
      "2023-10-08 23:46:58.542490: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderx7VRtg/15/model.savedmodel\n",
      "2023-10-08 23:46:58.551148: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:58.551218: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderx7VRtg/15/model.savedmodel\n",
      "2023-10-08 23:46:58.583964: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:58.657634: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderx7VRtg/15/model.savedmodel\n",
      "2023-10-08 23:46:58.697298: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 154785 microseconds.\n",
      "W1008 23:46:58.697436 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05a' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:58.719174 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05e_0 (CPU device 0)\n",
      "2023-10-08 23:46:58.719366: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder2EjC9i/3/model.savedmodel\n",
      "2023-10-08 23:46:58.727683: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:58.727765: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder2EjC9i/3/model.savedmodel\n",
      "2023-10-08 23:46:58.748445: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:58.816394: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder2EjC9i/3/model.savedmodel\n",
      "2023-10-08 23:46:58.858026: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 138671 microseconds.\n",
      "I1008 23:46:58.858309 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05a (version 16)\n",
      "2023-10-08 23:46:58.859264: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderwQyK1g/16/model.savedmodel\n",
      "2023-10-08 23:46:58.866273: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:58.866337: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderwQyK1g/16/model.savedmodel\n",
      "2023-10-08 23:46:58.899699: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:58.975481: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderwQyK1g/16/model.savedmodel\n",
      "2023-10-08 23:46:59.020333: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 161079 microseconds.\n",
      "W1008 23:46:59.020465 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05a' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:46:59.042269 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05e_1 (CPU device 0)\n",
      "2023-10-08 23:46:59.042420: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderIfUyNf/1/model.savedmodel\n",
      "2023-10-08 23:46:59.050478: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:59.050551: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderIfUyNf/1/model.savedmodel\n",
      "2023-10-08 23:46:59.086656: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:59.168073: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderIfUyNf/1/model.savedmodel\n",
      "2023-10-08 23:46:59.206229: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 163818 microseconds.\n",
      "I1008 23:46:59.206520 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05e_1 (CPU device 0)\n",
      "2023-10-08 23:46:59.206643: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldervxnflg/2/model.savedmodel\n",
      "2023-10-08 23:46:59.215071: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:59.215132: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldervxnflg/2/model.savedmodel\n",
      "2023-10-08 23:46:59.250225: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:59.334955: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldervxnflg/2/model.savedmodel\n",
      "2023-10-08 23:46:59.372609: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 165971 microseconds.\n",
      "I1008 23:46:59.372896 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_0 (CPU device 0)\n",
      "2023-10-08 23:46:59.373493: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderx7VRtg/15/model.savedmodel\n",
      "2023-10-08 23:46:59.384723: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:59.384799: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderx7VRtg/15/model.savedmodel\n",
      "2023-10-08 23:46:59.408066: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:59.482969: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderx7VRtg/15/model.savedmodel\n",
      "2023-10-08 23:46:59.522422: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 148955 microseconds.\n",
      "I1008 23:46:59.522776 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05e_1 (CPU device 0)\n",
      "2023-10-08 23:46:59.522924: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder2EjC9i/3/model.savedmodel\n",
      "2023-10-08 23:46:59.530010: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:59.530158: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder2EjC9i/3/model.savedmodel\n",
      "2023-10-08 23:46:59.564530: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:59.637083: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder2EjC9i/3/model.savedmodel\n",
      "2023-10-08 23:46:59.676300: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 153383 microseconds.\n",
      "I1008 23:46:59.676635 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_0 (CPU device 0)\n",
      "2023-10-08 23:46:59.676817: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderwQyK1g/16/model.savedmodel\n",
      "I1008 23:46:59.677098 1 model_lifecycle.cc:694] successfully loaded '05_05e' version 3\n",
      "I1008 23:46:59.677147 1 model_lifecycle.cc:694] successfully loaded '05_05e' version 3\n",
      "I1008 23:46:59.677169 1 model_lifecycle.cc:694] successfully loaded '05_05e' version 3\n",
      "2023-10-08 23:46:59.685939: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:59.686010: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderwQyK1g/16/model.savedmodel\n",
      "2023-10-08 23:46:59.708249: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:59.784478: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderwQyK1g/16/model.savedmodel\n",
      "2023-10-08 23:46:59.828214: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 151405 microseconds.\n",
      "I1008 23:46:59.828645 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_1 (CPU device 0)\n",
      "2023-10-08 23:46:59.828736: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderx7VRtg/15/model.savedmodel\n",
      "2023-10-08 23:46:59.841159: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:46:59.841236: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderx7VRtg/15/model.savedmodel\n",
      "2023-10-08 23:46:59.880810: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:46:59.965973: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderx7VRtg/15/model.savedmodel\n",
      "2023-10-08 23:47:00.006990: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 178260 microseconds.\n",
      "I1008 23:47:00.007265 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05a_1 (CPU device 0)\n",
      "2023-10-08 23:47:00.007412: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderwQyK1g/16/model.savedmodel\n",
      "2023-10-08 23:47:00.015027: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:00.015187: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderwQyK1g/16/model.savedmodel\n",
      "2023-10-08 23:47:00.049492: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:00.132022: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderwQyK1g/16/model.savedmodel\n",
      "2023-10-08 23:47:00.177475: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 170073 microseconds.\n",
      "I1008 23:47:00.178151 1 model_lifecycle.cc:694] successfully loaded '05_05a' version 16\n",
      "I1008 23:47:00.178179 1 model_lifecycle.cc:694] successfully loaded '05_05a' version 16\n",
      "I1008 23:47:00.178196 1 model_lifecycle.cc:694] successfully loaded '05_05a' version 16\n",
      "I1008 23:47:00.178215 1 model_lifecycle.cc:694] successfully loaded '05_05a' version 16\n",
      "I1008 23:47:00.178240 1 model_lifecycle.cc:694] successfully loaded '05_05a' version 16\n",
      "I1008 23:47:00.178249 1 model_lifecycle.cc:694] successfully loaded '05_05a' version 16\n",
      "I1008 23:47:00.178264 1 model_lifecycle.cc:694] successfully loaded '05_05a' version 16\n",
      "I1008 23:47:00.178285 1 model_lifecycle.cc:694] successfully loaded '05_05a' version 16\n",
      "I1008 23:47:00.178305 1 model_lifecycle.cc:694] successfully loaded '05_05a' version 16\n",
      "I1008 23:47:00.178320 1 model_lifecycle.cc:694] successfully loaded '05_05a' version 16\n",
      "I1008 23:47:00.178334 1 model_lifecycle.cc:694] successfully loaded '05_05a' version 16\n",
      "I1008 23:47:00.178355 1 model_lifecycle.cc:694] successfully loaded '05_05a' version 16\n",
      "I1008 23:47:00.178368 1 model_lifecycle.cc:694] successfully loaded '05_05a' version 16\n",
      "I1008 23:47:00.178382 1 model_lifecycle.cc:694] successfully loaded '05_05a' version 16\n",
      "I1008 23:47:00.178401 1 model_lifecycle.cc:694] successfully loaded '05_05a' version 16\n",
      "I1008 23:47:17.132008 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 1)\n",
      "2023-10-08 23:47:17.132722: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderhYJGVh/1/model.savedmodel\n",
      "2023-10-08 23:47:17.142768: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:17.142840: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderhYJGVh/1/model.savedmodel\n",
      "2023-10-08 23:47:17.183754: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:17.270178: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderhYJGVh/1/model.savedmodel\n",
      "2023-10-08 23:47:17.316191: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 183481 microseconds.\n",
      "W1008 23:47:17.316308 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:17.340389 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:17.340500: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderhYJGVh/1/model.savedmodel\n",
      "2023-10-08 23:47:17.348138: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:17.348208: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderhYJGVh/1/model.savedmodel\n",
      "2023-10-08 23:47:17.372469: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:17.452400: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderhYJGVh/1/model.savedmodel\n",
      "2023-10-08 23:47:17.497964: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 157474 microseconds.\n",
      "I1008 23:47:17.498449 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:17.498547: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderhYJGVh/1/model.savedmodel\n",
      "2023-10-08 23:47:17.508282: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:17.508348: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderhYJGVh/1/model.savedmodel\n",
      "2023-10-08 23:47:17.547179: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:17.636029: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderhYJGVh/1/model.savedmodel\n",
      "2023-10-08 23:47:17.684567: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 186025 microseconds.\n",
      "I1008 23:47:17.684786 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 2)\n",
      "2023-10-08 23:47:17.685509: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderGIdMni/2/model.savedmodel\n",
      "2023-10-08 23:47:17.695056: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:17.695117: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderGIdMni/2/model.savedmodel\n",
      "2023-10-08 23:47:17.732937: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:17.825805: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderGIdMni/2/model.savedmodel\n",
      "2023-10-08 23:47:17.880019: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 194518 microseconds.\n",
      "W1008 23:47:17.880152 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:17.910684 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 3)\n",
      "2023-10-08 23:47:17.911659: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderNYs8xf/3/model.savedmodel\n",
      "2023-10-08 23:47:17.922461: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:17.922535: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderNYs8xf/3/model.savedmodel\n",
      "2023-10-08 23:47:17.961690: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:18.037983: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderNYs8xf/3/model.savedmodel\n",
      "2023-10-08 23:47:18.082809: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 171159 microseconds.\n",
      "W1008 23:47:18.082952 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:18.107009 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:18.107143: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderGIdMni/2/model.savedmodel\n",
      "2023-10-08 23:47:18.117322: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:18.117403: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderGIdMni/2/model.savedmodel\n",
      "2023-10-08 23:47:18.143269: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:18.223948: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderGIdMni/2/model.savedmodel\n",
      "2023-10-08 23:47:18.266245: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 159111 microseconds.\n",
      "I1008 23:47:18.266522 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:18.266798: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderNYs8xf/3/model.savedmodel\n",
      "2023-10-08 23:47:18.276625: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:18.276693: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderNYs8xf/3/model.savedmodel\n",
      "2023-10-08 23:47:18.300196: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:18.372574: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderNYs8xf/3/model.savedmodel\n",
      "2023-10-08 23:47:18.415086: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 148294 microseconds.\n",
      "I1008 23:47:18.415381 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:18.415695: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderGIdMni/2/model.savedmodel\n",
      "2023-10-08 23:47:18.424719: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:18.424797: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderGIdMni/2/model.savedmodel\n",
      "2023-10-08 23:47:18.461082: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:18.541953: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderGIdMni/2/model.savedmodel\n",
      "2023-10-08 23:47:18.586867: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 171330 microseconds.\n",
      "I1008 23:47:18.587170 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:18.587372: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderNYs8xf/3/model.savedmodel\n",
      "2023-10-08 23:47:18.596805: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:18.596897: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderNYs8xf/3/model.savedmodel\n",
      "2023-10-08 23:47:18.638311: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:18.719239: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderNYs8xf/3/model.savedmodel\n",
      "2023-10-08 23:47:18.767568: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 180201 microseconds.\n",
      "I1008 23:47:19.294599 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 5)\n",
      "2023-10-08 23:47:19.295270: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderKhTmch/5/model.savedmodel\n",
      "2023-10-08 23:47:19.305207: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:19.305298: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderKhTmch/5/model.savedmodel\n",
      "2023-10-08 23:47:19.341161: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:19.421671: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderKhTmch/5/model.savedmodel\n",
      "2023-10-08 23:47:19.464327: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 169108 microseconds.\n",
      "W1008 23:47:19.464441 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:19.488118 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:19.488222: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderKhTmch/5/model.savedmodel\n",
      "2023-10-08 23:47:19.496015: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:19.496088: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderKhTmch/5/model.savedmodel\n",
      "2023-10-08 23:47:19.520329: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:19.593941: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderKhTmch/5/model.savedmodel\n",
      "2023-10-08 23:47:19.665416: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 177199 microseconds.\n",
      "I1008 23:47:19.666541 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:19.666704: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderKhTmch/5/model.savedmodel\n",
      "2023-10-08 23:47:19.682161: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:19.682245: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderKhTmch/5/model.savedmodel\n",
      "2023-10-08 23:47:19.737181: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:19.851847: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderKhTmch/5/model.savedmodel\n",
      "2023-10-08 23:47:19.902431: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 235734 microseconds.\n",
      "I1008 23:47:19.902684 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 9)\n",
      "2023-10-08 23:47:19.903821: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderv7nI9e/9/model.savedmodel\n",
      "2023-10-08 23:47:19.917842: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:19.917926: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderv7nI9e/9/model.savedmodel\n",
      "2023-10-08 23:47:19.970880: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:20.085624: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderv7nI9e/9/model.savedmodel\n",
      "2023-10-08 23:47:20.151049: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 247236 microseconds.\n",
      "W1008 23:47:20.151197 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:20.180710 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 8)\n",
      "2023-10-08 23:47:20.181578: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder9PMkLh/8/model.savedmodel\n",
      "2023-10-08 23:47:20.191297: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:20.192250: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder9PMkLh/8/model.savedmodel\n",
      "2023-10-08 23:47:20.245332: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:20.332525: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder9PMkLh/8/model.savedmodel\n",
      "2023-10-08 23:47:20.389400: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 207831 microseconds.\n",
      "W1008 23:47:20.389538 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:20.419479 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 4)\n",
      "2023-10-08 23:47:20.420324: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderYT71Bh/4/model.savedmodel\n",
      "2023-10-08 23:47:20.428518: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:20.428600: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderYT71Bh/4/model.savedmodel\n",
      "2023-10-08 23:47:20.466365: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:20.546085: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderYT71Bh/4/model.savedmodel\n",
      "2023-10-08 23:47:20.589795: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 169481 microseconds.\n",
      "W1008 23:47:20.589947 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:20.613798 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 6)\n",
      "2023-10-08 23:47:20.615174: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderYnetTf/6/model.savedmodel\n",
      "2023-10-08 23:47:20.623543: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:20.623623: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderYnetTf/6/model.savedmodel\n",
      "2023-10-08 23:47:20.662939: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:20.782058: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderYnetTf/6/model.savedmodel\n",
      "2023-10-08 23:47:20.847392: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 232229 microseconds.\n",
      "W1008 23:47:20.847552 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:20.871821 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:20.871946: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderYnetTf/6/model.savedmodel\n",
      "2023-10-08 23:47:20.883615: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:20.883690: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderYnetTf/6/model.savedmodel\n",
      "2023-10-08 23:47:20.909309: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:20.980350: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderYnetTf/6/model.savedmodel\n",
      "2023-10-08 23:47:21.023099: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 151162 microseconds.\n",
      "I1008 23:47:21.023345 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:21.023607: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder9PMkLh/8/model.savedmodel\n",
      "2023-10-08 23:47:21.034941: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:21.035018: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder9PMkLh/8/model.savedmodel\n",
      "2023-10-08 23:47:21.058769: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:21.135909: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder9PMkLh/8/model.savedmodel\n",
      "2023-10-08 23:47:21.177779: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 154179 microseconds.\n",
      "I1008 23:47:21.178091 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:21.178429: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderYT71Bh/4/model.savedmodel\n",
      "2023-10-08 23:47:21.187756: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:21.187875: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderYT71Bh/4/model.savedmodel\n",
      "2023-10-08 23:47:21.212689: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:21.287528: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderYT71Bh/4/model.savedmodel\n",
      "2023-10-08 23:47:21.335236: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 156815 microseconds.\n",
      "I1008 23:47:21.335485 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:21.335643: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderv7nI9e/9/model.savedmodel\n",
      "2023-10-08 23:47:21.345337: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:21.345401: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderv7nI9e/9/model.savedmodel\n",
      "2023-10-08 23:47:21.372018: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:21.443442: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderv7nI9e/9/model.savedmodel\n",
      "2023-10-08 23:47:21.489638: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 154002 microseconds.\n",
      "I1008 23:47:21.489903 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:21.490133: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderYnetTf/6/model.savedmodel\n",
      "2023-10-08 23:47:21.499497: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:21.499578: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderYnetTf/6/model.savedmodel\n",
      "2023-10-08 23:47:21.534279: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:21.608278: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderYnetTf/6/model.savedmodel\n",
      "2023-10-08 23:47:21.652145: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 162022 microseconds.\n",
      "I1008 23:47:21.652400 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:21.652496: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder9PMkLh/8/model.savedmodel\n",
      "2023-10-08 23:47:21.662165: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:21.662227: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder9PMkLh/8/model.savedmodel\n",
      "2023-10-08 23:47:21.697548: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:21.774159: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder9PMkLh/8/model.savedmodel\n",
      "2023-10-08 23:47:21.814101: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 161611 microseconds.\n",
      "I1008 23:47:21.814384 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:21.814737: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderYT71Bh/4/model.savedmodel\n",
      "2023-10-08 23:47:21.823919: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:21.823986: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderYT71Bh/4/model.savedmodel\n",
      "2023-10-08 23:47:21.859867: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:21.956388: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderYT71Bh/4/model.savedmodel\n",
      "2023-10-08 23:47:21.999702: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 185105 microseconds.\n",
      "I1008 23:47:21.999999 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:22.000091: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderv7nI9e/9/model.savedmodel\n",
      "2023-10-08 23:47:22.014162: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:22.014256: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderv7nI9e/9/model.savedmodel\n",
      "2023-10-08 23:47:22.055862: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:22.144147: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderv7nI9e/9/model.savedmodel\n",
      "2023-10-08 23:47:22.197925: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 197837 microseconds.\n",
      "I1008 23:47:37.719057 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 10)\n",
      "2023-10-08 23:47:37.719668: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderIj74Ti/10/model.savedmodel\n",
      "2023-10-08 23:47:37.727368: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:37.727433: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderIj74Ti/10/model.savedmodel\n",
      "2023-10-08 23:47:37.765064: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:37.842828: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderIj74Ti/10/model.savedmodel\n",
      "2023-10-08 23:47:37.887674: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 168016 microseconds.\n",
      "W1008 23:47:37.887822 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:37.910600 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:37.910700: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderIj74Ti/10/model.savedmodel\n",
      "2023-10-08 23:47:37.918309: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:37.918394: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderIj74Ti/10/model.savedmodel\n",
      "2023-10-08 23:47:37.943274: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:38.015575: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderIj74Ti/10/model.savedmodel\n",
      "2023-10-08 23:47:38.059978: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 149285 microseconds.\n",
      "I1008 23:47:38.060563 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:38.060688: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderIj74Ti/10/model.savedmodel\n",
      "2023-10-08 23:47:38.067905: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:38.067963: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderIj74Ti/10/model.savedmodel\n",
      "2023-10-08 23:47:38.105527: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:38.186846: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderIj74Ti/10/model.savedmodel\n",
      "2023-10-08 23:47:38.232210: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 171529 microseconds.\n",
      "I1008 23:47:38.232474 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 12)\n",
      "2023-10-08 23:47:38.233554: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderjE4yyg/12/model.savedmodel\n",
      "2023-10-08 23:47:38.243545: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:38.243608: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderjE4yyg/12/model.savedmodel\n",
      "2023-10-08 23:47:38.280355: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:38.359031: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderjE4yyg/12/model.savedmodel\n",
      "2023-10-08 23:47:38.403055: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 169509 microseconds.\n",
      "W1008 23:47:38.403181 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:38.425884 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 11)\n",
      "2023-10-08 23:47:38.426611: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderdVv6Zf/11/model.savedmodel\n",
      "2023-10-08 23:47:38.434423: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:38.434523: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderdVv6Zf/11/model.savedmodel\n",
      "2023-10-08 23:47:38.469515: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:38.545599: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderdVv6Zf/11/model.savedmodel\n",
      "2023-10-08 23:47:38.589303: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 162701 microseconds.\n",
      "W1008 23:47:38.589443 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:38.612947 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:38.613094: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderjE4yyg/12/model.savedmodel\n",
      "2023-10-08 23:47:38.621636: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:38.621706: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderjE4yyg/12/model.savedmodel\n",
      "2023-10-08 23:47:38.645606: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:38.720013: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderjE4yyg/12/model.savedmodel\n",
      "2023-10-08 23:47:38.766934: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 153849 microseconds.\n",
      "I1008 23:47:38.767243 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:38.767368: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderdVv6Zf/11/model.savedmodel\n",
      "2023-10-08 23:47:38.776691: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:38.776758: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderdVv6Zf/11/model.savedmodel\n",
      "2023-10-08 23:47:38.800895: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:38.872294: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderdVv6Zf/11/model.savedmodel\n",
      "2023-10-08 23:47:38.919518: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 152156 microseconds.\n",
      "I1008 23:47:38.919933 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:38.920154: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderjE4yyg/12/model.savedmodel\n",
      "2023-10-08 23:47:38.929231: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:38.929310: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderjE4yyg/12/model.savedmodel\n",
      "2023-10-08 23:47:38.965872: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:39.045111: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderjE4yyg/12/model.savedmodel\n",
      "2023-10-08 23:47:39.088550: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 168406 microseconds.\n",
      "I1008 23:47:39.088827 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:39.089407: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderdVv6Zf/11/model.savedmodel\n",
      "2023-10-08 23:47:39.098343: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:39.098406: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderdVv6Zf/11/model.savedmodel\n",
      "2023-10-08 23:47:39.136343: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:39.221425: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderdVv6Zf/11/model.savedmodel\n",
      "2023-10-08 23:47:39.264785: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 175386 microseconds.\n",
      "I1008 23:47:39.266591 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 13)\n",
      "2023-10-08 23:47:39.267183: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderGkuScf/13/model.savedmodel\n",
      "2023-10-08 23:47:39.275381: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:39.275464: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderGkuScf/13/model.savedmodel\n",
      "2023-10-08 23:47:39.321043: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:39.398779: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderGkuScf/13/model.savedmodel\n",
      "2023-10-08 23:47:39.445054: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 177877 microseconds.\n",
      "W1008 23:47:39.445198 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:39.471162 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:39.471260: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderGkuScf/13/model.savedmodel\n",
      "2023-10-08 23:47:39.481004: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:39.481071: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderGkuScf/13/model.savedmodel\n",
      "2023-10-08 23:47:39.504378: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:39.573354: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderGkuScf/13/model.savedmodel\n",
      "2023-10-08 23:47:39.616065: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 144811 microseconds.\n",
      "I1008 23:47:39.616584 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:39.616684: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderGkuScf/13/model.savedmodel\n",
      "2023-10-08 23:47:39.625198: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:39.625289: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderGkuScf/13/model.savedmodel\n",
      "2023-10-08 23:47:39.660909: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:39.737546: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderGkuScf/13/model.savedmodel\n",
      "2023-10-08 23:47:39.780755: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 164078 microseconds.\n",
      "I1008 23:47:40.830266 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 14)\n",
      "2023-10-08 23:47:40.831074: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderkArbjj/14/model.savedmodel\n",
      "2023-10-08 23:47:40.841423: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:40.841503: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderkArbjj/14/model.savedmodel\n",
      "2023-10-08 23:47:40.879747: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:40.955858: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderkArbjj/14/model.savedmodel\n",
      "2023-10-08 23:47:41.001461: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 170401 microseconds.\n",
      "W1008 23:47:41.001639 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:41.025720 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:41.025829: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderkArbjj/14/model.savedmodel\n",
      "2023-10-08 23:47:41.033937: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:41.034007: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderkArbjj/14/model.savedmodel\n",
      "2023-10-08 23:47:41.059529: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:41.134037: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderkArbjj/14/model.savedmodel\n",
      "2023-10-08 23:47:41.179316: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 153496 microseconds.\n",
      "I1008 23:47:41.179621 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 15)\n",
      "2023-10-08 23:47:41.180601: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderFckoij/15/model.savedmodel\n",
      "2023-10-08 23:47:41.190036: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:41.190117: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderFckoij/15/model.savedmodel\n",
      "2023-10-08 23:47:41.228107: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:41.309141: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderFckoij/15/model.savedmodel\n",
      "2023-10-08 23:47:41.349626: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 169032 microseconds.\n",
      "W1008 23:47:41.349733 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:41.374021 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:41.374183: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderkArbjj/14/model.savedmodel\n",
      "2023-10-08 23:47:41.382809: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:41.382886: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderkArbjj/14/model.savedmodel\n",
      "2023-10-08 23:47:41.419539: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:41.504766: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderkArbjj/14/model.savedmodel\n",
      "2023-10-08 23:47:41.550250: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 176078 microseconds.\n",
      "I1008 23:47:41.550500 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:41.550766: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderFckoij/15/model.savedmodel\n",
      "2023-10-08 23:47:41.558815: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:41.558876: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderFckoij/15/model.savedmodel\n",
      "2023-10-08 23:47:41.582255: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:41.655715: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderFckoij/15/model.savedmodel\n",
      "2023-10-08 23:47:41.698046: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 147287 microseconds.\n",
      "I1008 23:47:41.698327 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 16)\n",
      "2023-10-08 23:47:41.699028: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderSUQYng/16/model.savedmodel\n",
      "2023-10-08 23:47:41.706944: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:41.707025: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderSUQYng/16/model.savedmodel\n",
      "2023-10-08 23:47:41.744102: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:41.823293: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderSUQYng/16/model.savedmodel\n",
      "2023-10-08 23:47:41.867209: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 168186 microseconds.\n",
      "W1008 23:47:41.867346 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:41.890770 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:41.890979: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderFckoij/15/model.savedmodel\n",
      "2023-10-08 23:47:41.901375: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:41.901463: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderFckoij/15/model.savedmodel\n",
      "2023-10-08 23:47:41.937035: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:42.015930: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderFckoij/15/model.savedmodel\n",
      "2023-10-08 23:47:42.059518: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 168589 microseconds.\n",
      "I1008 23:47:42.060037 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 17)\n",
      "2023-10-08 23:47:42.061418: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldern4p00g/17/model.savedmodel\n",
      "2023-10-08 23:47:42.069746: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:42.069818: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldern4p00g/17/model.savedmodel\n",
      "2023-10-08 23:47:42.106064: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:42.189880: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldern4p00g/17/model.savedmodel\n",
      "2023-10-08 23:47:42.235361: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 173952 microseconds.\n",
      "W1008 23:47:42.235482 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:42.259704 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:42.259852: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderSUQYng/16/model.savedmodel\n",
      "2023-10-08 23:47:42.269742: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:42.269812: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderSUQYng/16/model.savedmodel\n",
      "2023-10-08 23:47:42.293392: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:42.368040: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderSUQYng/16/model.savedmodel\n",
      "2023-10-08 23:47:42.410264: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 150420 microseconds.\n",
      "I1008 23:47:42.410594 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:42.410854: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldern4p00g/17/model.savedmodel\n",
      "2023-10-08 23:47:42.418309: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:42.418370: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldern4p00g/17/model.savedmodel\n",
      "2023-10-08 23:47:42.441205: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:42.512318: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldern4p00g/17/model.savedmodel\n",
      "2023-10-08 23:47:42.554783: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 143937 microseconds.\n",
      "I1008 23:47:42.555076 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:42.555376: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderSUQYng/16/model.savedmodel\n",
      "2023-10-08 23:47:42.565684: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:42.565764: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderSUQYng/16/model.savedmodel\n",
      "2023-10-08 23:47:42.606243: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:42.687274: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderSUQYng/16/model.savedmodel\n",
      "2023-10-08 23:47:42.730700: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 175332 microseconds.\n",
      "I1008 23:47:42.731104 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:42.731226: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldern4p00g/17/model.savedmodel\n",
      "2023-10-08 23:47:42.739046: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:42.739114: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldern4p00g/17/model.savedmodel\n",
      "2023-10-08 23:47:42.775086: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:42.869986: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldern4p00g/17/model.savedmodel\n",
      "2023-10-08 23:47:42.910547: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 179327 microseconds.\n",
      "I1008 23:47:57.925844 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 18)\n",
      "2023-10-08 23:47:57.926636: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderb4Qb1f/18/model.savedmodel\n",
      "2023-10-08 23:47:57.934874: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:57.934948: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderb4Qb1f/18/model.savedmodel\n",
      "2023-10-08 23:47:57.970387: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:58.048707: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderb4Qb1f/18/model.savedmodel\n",
      "2023-10-08 23:47:58.091607: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 164983 microseconds.\n",
      "W1008 23:47:58.091729 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:58.115757 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:58.115848: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderb4Qb1f/18/model.savedmodel\n",
      "2023-10-08 23:47:58.123396: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:58.123476: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderb4Qb1f/18/model.savedmodel\n",
      "2023-10-08 23:47:58.147969: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:58.222590: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderb4Qb1f/18/model.savedmodel\n",
      "2023-10-08 23:47:58.268269: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 152427 microseconds.\n",
      "I1008 23:47:58.268783 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:58.268956: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderb4Qb1f/18/model.savedmodel\n",
      "2023-10-08 23:47:58.278758: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:58.278823: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderb4Qb1f/18/model.savedmodel\n",
      "2023-10-08 23:47:58.315741: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:58.393255: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderb4Qb1f/18/model.savedmodel\n",
      "2023-10-08 23:47:58.435639: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 166691 microseconds.\n",
      "I1008 23:47:58.887766 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 19)\n",
      "2023-10-08 23:47:58.888461: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderTisgug/19/model.savedmodel\n",
      "2023-10-08 23:47:58.895794: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:58.895877: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderTisgug/19/model.savedmodel\n",
      "2023-10-08 23:47:58.932827: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:59.014482: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderTisgug/19/model.savedmodel\n",
      "2023-10-08 23:47:59.058819: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 170364 microseconds.\n",
      "W1008 23:47:59.058963 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:59.081685 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:59.081783: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderTisgug/19/model.savedmodel\n",
      "2023-10-08 23:47:59.089206: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:59.089293: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderTisgug/19/model.savedmodel\n",
      "2023-10-08 23:47:59.114044: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:59.185818: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderTisgug/19/model.savedmodel\n",
      "2023-10-08 23:47:59.234280: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 152506 microseconds.\n",
      "I1008 23:47:59.234688 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 20)\n",
      "2023-10-08 23:47:59.235782: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderyPS9Bg/20/model.savedmodel\n",
      "2023-10-08 23:47:59.243881: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:59.243946: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderyPS9Bg/20/model.savedmodel\n",
      "2023-10-08 23:47:59.280599: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:59.356895: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderyPS9Bg/20/model.savedmodel\n",
      "2023-10-08 23:47:59.399616: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 163842 microseconds.\n",
      "W1008 23:47:59.399731 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:59.422753 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:59.423232: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderTisgug/19/model.savedmodel\n",
      "2023-10-08 23:47:59.434358: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:59.434427: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderTisgug/19/model.savedmodel\n",
      "2023-10-08 23:47:59.471167: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:59.549290: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderTisgug/19/model.savedmodel\n",
      "2023-10-08 23:47:59.595596: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 172376 microseconds.\n",
      "I1008 23:47:59.595864 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:47:59.596506: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderyPS9Bg/20/model.savedmodel\n",
      "2023-10-08 23:47:59.604326: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:59.604386: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderyPS9Bg/20/model.savedmodel\n",
      "2023-10-08 23:47:59.627399: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:59.708629: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderyPS9Bg/20/model.savedmodel\n",
      "2023-10-08 23:47:59.751085: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 154587 microseconds.\n",
      "I1008 23:47:59.751486 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 21)\n",
      "2023-10-08 23:47:59.752309: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderpYFNah/21/model.savedmodel\n",
      "2023-10-08 23:47:59.761774: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:59.761859: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderpYFNah/21/model.savedmodel\n",
      "2023-10-08 23:47:59.797740: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:47:59.877416: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderpYFNah/21/model.savedmodel\n",
      "2023-10-08 23:47:59.919601: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 167298 microseconds.\n",
      "W1008 23:47:59.919730 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:47:59.941590 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:47:59.941714: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderyPS9Bg/20/model.savedmodel\n",
      "2023-10-08 23:47:59.951268: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:47:59.951344: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderyPS9Bg/20/model.savedmodel\n",
      "2023-10-08 23:47:59.986800: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:00.062497: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderyPS9Bg/20/model.savedmodel\n",
      "2023-10-08 23:48:00.105528: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 163823 microseconds.\n",
      "I1008 23:48:00.105778 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:00.106373: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderpYFNah/21/model.savedmodel\n",
      "2023-10-08 23:48:00.114713: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:00.114780: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderpYFNah/21/model.savedmodel\n",
      "2023-10-08 23:48:00.137571: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:00.211167: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderpYFNah/21/model.savedmodel\n",
      "2023-10-08 23:48:00.252719: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 146355 microseconds.\n",
      "I1008 23:48:00.253297 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:00.253395: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderpYFNah/21/model.savedmodel\n",
      "2023-10-08 23:48:00.263917: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:00.263995: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderpYFNah/21/model.savedmodel\n",
      "2023-10-08 23:48:00.299312: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:00.377672: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderpYFNah/21/model.savedmodel\n",
      "2023-10-08 23:48:00.419432: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 166043 microseconds.\n",
      "I1008 23:48:01.028193 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 23)\n",
      "2023-10-08 23:48:01.028908: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderK2bdef/23/model.savedmodel\n",
      "2023-10-08 23:48:01.036221: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:01.036295: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderK2bdef/23/model.savedmodel\n",
      "2023-10-08 23:48:01.071795: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:01.153764: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderK2bdef/23/model.savedmodel\n",
      "2023-10-08 23:48:01.223115: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 194216 microseconds.\n",
      "W1008 23:48:01.223251 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:01.249233 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 22)\n",
      "2023-10-08 23:48:01.250348: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderg9Vuzf/22/model.savedmodel\n",
      "2023-10-08 23:48:01.258425: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:01.258499: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderg9Vuzf/22/model.savedmodel\n",
      "2023-10-08 23:48:01.296981: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:01.387562: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderg9Vuzf/22/model.savedmodel\n",
      "2023-10-08 23:48:01.433625: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 183287 microseconds.\n",
      "W1008 23:48:01.433770 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:01.465728 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:01.465928: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderK2bdef/23/model.savedmodel\n",
      "2023-10-08 23:48:01.475116: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:01.475209: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderK2bdef/23/model.savedmodel\n",
      "2023-10-08 23:48:01.505011: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:01.586514: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderK2bdef/23/model.savedmodel\n",
      "2023-10-08 23:48:01.647505: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 181593 microseconds.\n",
      "I1008 23:48:01.647949 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:01.648095: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderg9Vuzf/22/model.savedmodel\n",
      "2023-10-08 23:48:01.657257: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:01.657349: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderg9Vuzf/22/model.savedmodel\n",
      "2023-10-08 23:48:01.683564: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:01.759957: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderg9Vuzf/22/model.savedmodel\n",
      "2023-10-08 23:48:01.825481: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 177392 microseconds.\n",
      "I1008 23:48:01.825772 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:01.825889: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderK2bdef/23/model.savedmodel\n",
      "2023-10-08 23:48:01.838507: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:01.838577: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderK2bdef/23/model.savedmodel\n",
      "2023-10-08 23:48:01.874253: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:01.956965: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderK2bdef/23/model.savedmodel\n",
      "2023-10-08 23:48:02.023853: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 197971 microseconds.\n",
      "I1008 23:48:02.024172 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:02.024636: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderg9Vuzf/22/model.savedmodel\n",
      "2023-10-08 23:48:02.037068: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:02.037140: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderg9Vuzf/22/model.savedmodel\n",
      "2023-10-08 23:48:02.077668: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:02.187590: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderg9Vuzf/22/model.savedmodel\n",
      "2023-10-08 23:48:02.248533: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 223905 microseconds.\n",
      "I1008 23:48:02.248812 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 25)\n",
      "2023-10-08 23:48:02.249740: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder9JrGvh/25/model.savedmodel\n",
      "2023-10-08 23:48:02.263082: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:02.263164: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder9JrGvh/25/model.savedmodel\n",
      "2023-10-08 23:48:02.316732: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:02.407156: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder9JrGvh/25/model.savedmodel\n",
      "2023-10-08 23:48:02.464831: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 215101 microseconds.\n",
      "W1008 23:48:02.464941 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:02.490169 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 24)\n",
      "2023-10-08 23:48:02.492092: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderTrWsYi/24/model.savedmodel\n",
      "2023-10-08 23:48:02.505618: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:02.505708: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderTrWsYi/24/model.savedmodel\n",
      "2023-10-08 23:48:02.562482: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:02.689999: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderTrWsYi/24/model.savedmodel\n",
      "2023-10-08 23:48:02.760908: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 268836 microseconds.\n",
      "W1008 23:48:02.761060 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:02.791934 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:02.792083: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder9JrGvh/25/model.savedmodel\n",
      "2023-10-08 23:48:02.800304: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:02.800372: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder9JrGvh/25/model.savedmodel\n",
      "2023-10-08 23:48:02.825825: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:02.906042: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder9JrGvh/25/model.savedmodel\n",
      "2023-10-08 23:48:02.957865: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 165790 microseconds.\n",
      "I1008 23:48:02.958128 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:02.958249: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderTrWsYi/24/model.savedmodel\n",
      "2023-10-08 23:48:02.969560: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:02.969637: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderTrWsYi/24/model.savedmodel\n",
      "2023-10-08 23:48:02.998298: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:03.077039: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderTrWsYi/24/model.savedmodel\n",
      "2023-10-08 23:48:03.123722: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 165478 microseconds.\n",
      "I1008 23:48:03.124110 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:03.124228: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder9JrGvh/25/model.savedmodel\n",
      "2023-10-08 23:48:03.133802: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:03.133877: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder9JrGvh/25/model.savedmodel\n",
      "2023-10-08 23:48:03.169889: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:03.260103: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder9JrGvh/25/model.savedmodel\n",
      "2023-10-08 23:48:03.311974: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 187751 microseconds.\n",
      "I1008 23:48:03.312414 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:03.312688: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderTrWsYi/24/model.savedmodel\n",
      "2023-10-08 23:48:03.326508: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:03.326589: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderTrWsYi/24/model.savedmodel\n",
      "2023-10-08 23:48:03.376670: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:03.464480: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderTrWsYi/24/model.savedmodel\n",
      "2023-10-08 23:48:03.529859: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 217178 microseconds.\n",
      "I1008 23:48:18.062768 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 26)\n",
      "2023-10-08 23:48:18.063428: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderOndJEi/26/model.savedmodel\n",
      "2023-10-08 23:48:18.071855: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:18.071922: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderOndJEi/26/model.savedmodel\n",
      "2023-10-08 23:48:18.109487: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:18.189542: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderOndJEi/26/model.savedmodel\n",
      "2023-10-08 23:48:18.233609: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 170193 microseconds.\n",
      "W1008 23:48:18.233750 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:18.258287 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:18.258399: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderOndJEi/26/model.savedmodel\n",
      "2023-10-08 23:48:18.266781: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:18.266874: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderOndJEi/26/model.savedmodel\n",
      "2023-10-08 23:48:18.290017: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:18.362845: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderOndJEi/26/model.savedmodel\n",
      "2023-10-08 23:48:18.406192: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 147801 microseconds.\n",
      "I1008 23:48:18.406755 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:18.406853: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderOndJEi/26/model.savedmodel\n",
      "2023-10-08 23:48:18.415961: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:18.416024: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderOndJEi/26/model.savedmodel\n",
      "2023-10-08 23:48:18.450553: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:18.527873: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderOndJEi/26/model.savedmodel\n",
      "2023-10-08 23:48:18.572164: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 165317 microseconds.\n",
      "I1008 23:48:19.028745 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 27)\n",
      "2023-10-08 23:48:19.029384: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderIcJI7h/27/model.savedmodel\n",
      "2023-10-08 23:48:19.036736: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:19.036803: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderIcJI7h/27/model.savedmodel\n",
      "2023-10-08 23:48:19.072453: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:19.152078: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderIcJI7h/27/model.savedmodel\n",
      "2023-10-08 23:48:19.194640: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 165265 microseconds.\n",
      "W1008 23:48:19.194765 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:19.217952 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:19.218048: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderIcJI7h/27/model.savedmodel\n",
      "2023-10-08 23:48:19.228586: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:19.228659: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderIcJI7h/27/model.savedmodel\n",
      "2023-10-08 23:48:19.251829: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:19.324959: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderIcJI7h/27/model.savedmodel\n",
      "2023-10-08 23:48:19.366779: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 148731 microseconds.\n",
      "I1008 23:48:19.367500 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:19.367597: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderIcJI7h/27/model.savedmodel\n",
      "2023-10-08 23:48:19.376770: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:19.376847: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderIcJI7h/27/model.savedmodel\n",
      "2023-10-08 23:48:19.415238: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:19.493932: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderIcJI7h/27/model.savedmodel\n",
      "2023-10-08 23:48:19.537288: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 169679 microseconds.\n",
      "I1008 23:48:19.537562 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 28)\n",
      "2023-10-08 23:48:19.538463: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderOBY6Cf/28/model.savedmodel\n",
      "2023-10-08 23:48:19.546394: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:19.546450: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderOBY6Cf/28/model.savedmodel\n",
      "2023-10-08 23:48:19.581915: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:19.661814: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderOBY6Cf/28/model.savedmodel\n",
      "2023-10-08 23:48:19.705842: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 167386 microseconds.\n",
      "W1008 23:48:19.705992 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:19.730213 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:19.730332: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderOBY6Cf/28/model.savedmodel\n",
      "2023-10-08 23:48:19.738827: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:19.738911: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderOBY6Cf/28/model.savedmodel\n",
      "2023-10-08 23:48:19.763848: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:19.836420: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderOBY6Cf/28/model.savedmodel\n",
      "2023-10-08 23:48:19.884705: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 154383 microseconds.\n",
      "I1008 23:48:19.885258 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:19.885387: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderOBY6Cf/28/model.savedmodel\n",
      "2023-10-08 23:48:19.895642: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:19.895725: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderOBY6Cf/28/model.savedmodel\n",
      "2023-10-08 23:48:19.934116: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:20.013959: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderOBY6Cf/28/model.savedmodel\n",
      "2023-10-08 23:48:20.064267: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 178887 microseconds.\n",
      "I1008 23:48:20.064580 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 29)\n",
      "2023-10-08 23:48:20.065256: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderoaWjNi/29/model.savedmodel\n",
      "2023-10-08 23:48:20.074988: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:20.075071: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderoaWjNi/29/model.savedmodel\n",
      "2023-10-08 23:48:20.111540: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:20.194113: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderoaWjNi/29/model.savedmodel\n",
      "2023-10-08 23:48:20.238834: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 173584 microseconds.\n",
      "W1008 23:48:20.238984 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:20.262999 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:20.263118: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderoaWjNi/29/model.savedmodel\n",
      "2023-10-08 23:48:20.271383: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:20.271454: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderoaWjNi/29/model.savedmodel\n",
      "2023-10-08 23:48:20.295338: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:20.371186: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderoaWjNi/29/model.savedmodel\n",
      "2023-10-08 23:48:20.434675: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 171566 microseconds.\n",
      "I1008 23:48:20.435497 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:20.436214: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderoaWjNi/29/model.savedmodel\n",
      "2023-10-08 23:48:20.445397: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:20.445702: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderoaWjNi/29/model.savedmodel\n",
      "2023-10-08 23:48:20.483920: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:20.567040: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderoaWjNi/29/model.savedmodel\n",
      "2023-10-08 23:48:20.615980: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 179917 microseconds.\n",
      "I1008 23:48:21.061099 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 30)\n",
      "2023-10-08 23:48:21.061752: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder0EYCPg/30/model.savedmodel\n",
      "2023-10-08 23:48:21.070486: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:21.070548: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder0EYCPg/30/model.savedmodel\n",
      "2023-10-08 23:48:21.105799: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:21.182565: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder0EYCPg/30/model.savedmodel\n",
      "2023-10-08 23:48:21.227519: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 165775 microseconds.\n",
      "W1008 23:48:21.227662 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:21.251073 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 31)\n",
      "2023-10-08 23:48:21.251756: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder453Fof/31/model.savedmodel\n",
      "2023-10-08 23:48:21.261247: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:21.261335: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder453Fof/31/model.savedmodel\n",
      "2023-10-08 23:48:21.298277: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:21.376757: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder453Fof/31/model.savedmodel\n",
      "2023-10-08 23:48:21.422928: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 171180 microseconds.\n",
      "W1008 23:48:21.423077 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:21.446543 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:21.446711: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder0EYCPg/30/model.savedmodel\n",
      "2023-10-08 23:48:21.456097: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:21.456168: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder0EYCPg/30/model.savedmodel\n",
      "2023-10-08 23:48:21.481342: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:21.554570: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder0EYCPg/30/model.savedmodel\n",
      "2023-10-08 23:48:21.599577: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 152874 microseconds.\n",
      "I1008 23:48:21.599865 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:21.600068: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder453Fof/31/model.savedmodel\n",
      "2023-10-08 23:48:21.608696: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:21.608761: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder453Fof/31/model.savedmodel\n",
      "2023-10-08 23:48:21.633731: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:21.711862: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder453Fof/31/model.savedmodel\n",
      "2023-10-08 23:48:21.759317: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 159256 microseconds.\n",
      "I1008 23:48:21.759598 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:21.759716: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder0EYCPg/30/model.savedmodel\n",
      "2023-10-08 23:48:21.768762: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:21.768821: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder0EYCPg/30/model.savedmodel\n",
      "2023-10-08 23:48:21.805636: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:21.881090: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder0EYCPg/30/model.savedmodel\n",
      "2023-10-08 23:48:21.924966: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 165258 microseconds.\n",
      "I1008 23:48:21.925217 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:21.925357: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder453Fof/31/model.savedmodel\n",
      "2023-10-08 23:48:21.934623: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:21.934682: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder453Fof/31/model.savedmodel\n",
      "2023-10-08 23:48:21.969785: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:22.058739: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder453Fof/31/model.savedmodel\n",
      "2023-10-08 23:48:22.114051: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 188698 microseconds.\n",
      "I1008 23:48:22.442946 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 33)\n",
      "2023-10-08 23:48:22.443594: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderesCLKg/33/model.savedmodel\n",
      "2023-10-08 23:48:22.452867: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:22.452941: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderesCLKg/33/model.savedmodel\n",
      "2023-10-08 23:48:22.487678: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:22.567753: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderesCLKg/33/model.savedmodel\n",
      "2023-10-08 23:48:22.612026: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 168444 microseconds.\n",
      "W1008 23:48:22.612162 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:22.635556 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 32)\n",
      "2023-10-08 23:48:22.636315: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderTNAwkh/32/model.savedmodel\n",
      "2023-10-08 23:48:22.645572: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:22.645683: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderTNAwkh/32/model.savedmodel\n",
      "2023-10-08 23:48:22.682390: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:22.757653: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderTNAwkh/32/model.savedmodel\n",
      "2023-10-08 23:48:22.797068: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 160763 microseconds.\n",
      "W1008 23:48:22.797189 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:22.819647 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:22.819790: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderesCLKg/33/model.savedmodel\n",
      "2023-10-08 23:48:22.830327: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:22.830395: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderesCLKg/33/model.savedmodel\n",
      "2023-10-08 23:48:22.852876: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:22.925570: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderesCLKg/33/model.savedmodel\n",
      "2023-10-08 23:48:22.970006: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 150225 microseconds.\n",
      "I1008 23:48:22.970257 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:22.970388: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderTNAwkh/32/model.savedmodel\n",
      "2023-10-08 23:48:22.979312: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:22.979387: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderTNAwkh/32/model.savedmodel\n",
      "2023-10-08 23:48:23.003826: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:23.085722: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderTNAwkh/32/model.savedmodel\n",
      "2023-10-08 23:48:23.129820: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 159439 microseconds.\n",
      "I1008 23:48:23.130047 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:23.130141: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderesCLKg/33/model.savedmodel\n",
      "2023-10-08 23:48:23.139371: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:23.139445: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderesCLKg/33/model.savedmodel\n",
      "2023-10-08 23:48:23.173804: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:23.248868: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderesCLKg/33/model.savedmodel\n",
      "2023-10-08 23:48:23.291147: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 161012 microseconds.\n",
      "I1008 23:48:23.291507 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:23.291712: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderTNAwkh/32/model.savedmodel\n",
      "2023-10-08 23:48:23.301259: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:23.301404: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderTNAwkh/32/model.savedmodel\n",
      "2023-10-08 23:48:23.346580: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:23.433079: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderTNAwkh/32/model.savedmodel\n",
      "2023-10-08 23:48:23.474176: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 182474 microseconds.\n",
      "I1008 23:48:37.837094 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 34)\n",
      "2023-10-08 23:48:37.837919: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderNRBiTe/34/model.savedmodel\n",
      "2023-10-08 23:48:37.850106: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:37.850181: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderNRBiTe/34/model.savedmodel\n",
      "2023-10-08 23:48:37.893803: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:37.971738: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderNRBiTe/34/model.savedmodel\n",
      "2023-10-08 23:48:38.018011: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 180102 microseconds.\n",
      "W1008 23:48:38.018124 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:38.042496 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:38.042601: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderNRBiTe/34/model.savedmodel\n",
      "2023-10-08 23:48:38.050641: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:38.050709: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderNRBiTe/34/model.savedmodel\n",
      "2023-10-08 23:48:38.074746: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:38.152142: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderNRBiTe/34/model.savedmodel\n",
      "2023-10-08 23:48:38.199573: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 156980 microseconds.\n",
      "I1008 23:48:38.200119 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:38.200204: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderNRBiTe/34/model.savedmodel\n",
      "2023-10-08 23:48:38.209689: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:38.209754: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderNRBiTe/34/model.savedmodel\n",
      "2023-10-08 23:48:38.250200: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:38.335338: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderNRBiTe/34/model.savedmodel\n",
      "2023-10-08 23:48:38.378974: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 178774 microseconds.\n",
      "I1008 23:48:38.633881 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 35)\n",
      "2023-10-08 23:48:38.634535: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderTMRlNf/35/model.savedmodel\n",
      "2023-10-08 23:48:38.643378: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:38.643474: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderTMRlNf/35/model.savedmodel\n",
      "2023-10-08 23:48:38.688950: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:38.772695: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderTMRlNf/35/model.savedmodel\n",
      "2023-10-08 23:48:38.816567: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 182043 microseconds.\n",
      "W1008 23:48:38.816697 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:38.840328 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:38.840424: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderTMRlNf/35/model.savedmodel\n",
      "2023-10-08 23:48:38.848121: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:38.848187: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderTMRlNf/35/model.savedmodel\n",
      "2023-10-08 23:48:38.873359: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:38.946670: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderTMRlNf/35/model.savedmodel\n",
      "2023-10-08 23:48:38.990038: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 149621 microseconds.\n",
      "I1008 23:48:38.990301 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 36)\n",
      "2023-10-08 23:48:38.991269: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderH5Mc6e/36/model.savedmodel\n",
      "2023-10-08 23:48:38.999667: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:38.999732: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderH5Mc6e/36/model.savedmodel\n",
      "2023-10-08 23:48:39.046113: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:39.127401: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderH5Mc6e/36/model.savedmodel\n",
      "2023-10-08 23:48:39.168838: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 177578 microseconds.\n",
      "W1008 23:48:39.168978 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:39.194057 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:39.194193: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderTMRlNf/35/model.savedmodel\n",
      "2023-10-08 23:48:39.202399: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:39.202471: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderTMRlNf/35/model.savedmodel\n",
      "2023-10-08 23:48:39.244042: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:39.327720: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderTMRlNf/35/model.savedmodel\n",
      "2023-10-08 23:48:39.375432: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 181250 microseconds.\n",
      "I1008 23:48:39.375686 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:39.375810: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderH5Mc6e/36/model.savedmodel\n",
      "2023-10-08 23:48:39.384466: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:39.384546: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderH5Mc6e/36/model.savedmodel\n",
      "2023-10-08 23:48:39.409156: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:39.483144: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderH5Mc6e/36/model.savedmodel\n",
      "2023-10-08 23:48:39.527802: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 151997 microseconds.\n",
      "I1008 23:48:39.528350 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:39.528438: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderH5Mc6e/36/model.savedmodel\n",
      "2023-10-08 23:48:39.538097: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:39.538168: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderH5Mc6e/36/model.savedmodel\n",
      "2023-10-08 23:48:39.576253: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:39.659362: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderH5Mc6e/36/model.savedmodel\n",
      "2023-10-08 23:48:39.704909: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 176476 microseconds.\n",
      "I1008 23:48:39.705332 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 37)\n",
      "2023-10-08 23:48:39.706086: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderQTc0oi/37/model.savedmodel\n",
      "2023-10-08 23:48:39.715833: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:39.715900: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderQTc0oi/37/model.savedmodel\n",
      "2023-10-08 23:48:39.755709: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:39.837645: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderQTc0oi/37/model.savedmodel\n",
      "2023-10-08 23:48:39.897486: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 191406 microseconds.\n",
      "W1008 23:48:39.897619 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:39.924131 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:39.924236: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderQTc0oi/37/model.savedmodel\n",
      "2023-10-08 23:48:39.934720: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:39.934799: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderQTc0oi/37/model.savedmodel\n",
      "2023-10-08 23:48:39.961802: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:40.035648: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderQTc0oi/37/model.savedmodel\n",
      "2023-10-08 23:48:40.082387: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 158160 microseconds.\n",
      "I1008 23:48:40.082935 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:40.083038: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderQTc0oi/37/model.savedmodel\n",
      "2023-10-08 23:48:40.091877: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:40.091937: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderQTc0oi/37/model.savedmodel\n",
      "2023-10-08 23:48:40.133326: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:40.217170: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderQTc0oi/37/model.savedmodel\n",
      "2023-10-08 23:48:40.260863: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 177830 microseconds.\n",
      "I1008 23:48:40.975185 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 39)\n",
      "2023-10-08 23:48:40.975892: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderNdKQ1g/39/model.savedmodel\n",
      "2023-10-08 23:48:40.987097: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:40.987168: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderNdKQ1g/39/model.savedmodel\n",
      "2023-10-08 23:48:41.027688: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:41.114893: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderNdKQ1g/39/model.savedmodel\n",
      "2023-10-08 23:48:41.157603: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 181719 microseconds.\n",
      "W1008 23:48:41.157757 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:41.180438 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:41.180541: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderNdKQ1g/39/model.savedmodel\n",
      "2023-10-08 23:48:41.187818: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:41.187887: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderNdKQ1g/39/model.savedmodel\n",
      "2023-10-08 23:48:41.213215: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:41.285995: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderNdKQ1g/39/model.savedmodel\n",
      "2023-10-08 23:48:41.328871: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 148336 microseconds.\n",
      "I1008 23:48:41.329154 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 38)\n",
      "2023-10-08 23:48:41.330109: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderYCqH7i/38/model.savedmodel\n",
      "2023-10-08 23:48:41.338471: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:41.338546: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderYCqH7i/38/model.savedmodel\n",
      "2023-10-08 23:48:41.377776: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:41.458494: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderYCqH7i/38/model.savedmodel\n",
      "2023-10-08 23:48:41.502789: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 172687 microseconds.\n",
      "W1008 23:48:41.502930 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:41.527767 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:41.527914: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderNdKQ1g/39/model.savedmodel\n",
      "2023-10-08 23:48:41.536749: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:41.536826: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderNdKQ1g/39/model.savedmodel\n",
      "2023-10-08 23:48:41.577089: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:41.656084: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderNdKQ1g/39/model.savedmodel\n",
      "2023-10-08 23:48:41.703196: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 175292 microseconds.\n",
      "I1008 23:48:41.703458 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:41.703678: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderYCqH7i/38/model.savedmodel\n",
      "2023-10-08 23:48:41.714011: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:41.714113: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderYCqH7i/38/model.savedmodel\n",
      "2023-10-08 23:48:41.736711: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:41.817798: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderYCqH7i/38/model.savedmodel\n",
      "2023-10-08 23:48:41.869119: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 165448 microseconds.\n",
      "I1008 23:48:41.869822 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:41.870105: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderYCqH7i/38/model.savedmodel\n",
      "2023-10-08 23:48:41.880691: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:41.880764: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderYCqH7i/38/model.savedmodel\n",
      "2023-10-08 23:48:41.928337: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:42.015114: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderYCqH7i/38/model.savedmodel\n",
      "2023-10-08 23:48:42.060539: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 190442 microseconds.\n",
      "I1008 23:48:42.226825 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 41)\n",
      "2023-10-08 23:48:42.227436: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderpAhXch/41/model.savedmodel\n",
      "2023-10-08 23:48:42.237686: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:42.237757: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderpAhXch/41/model.savedmodel\n",
      "2023-10-08 23:48:42.280798: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:42.365076: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderpAhXch/41/model.savedmodel\n",
      "2023-10-08 23:48:42.410363: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 182934 microseconds.\n",
      "W1008 23:48:42.410498 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:42.432879 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 40)\n",
      "2023-10-08 23:48:42.433575: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderv9lyVf/40/model.savedmodel\n",
      "2023-10-08 23:48:42.443975: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:42.444062: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderv9lyVf/40/model.savedmodel\n",
      "2023-10-08 23:48:42.490554: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:42.572280: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderv9lyVf/40/model.savedmodel\n",
      "2023-10-08 23:48:42.618987: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 185419 microseconds.\n",
      "W1008 23:48:42.619111 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:42.641846 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:42.642021: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderpAhXch/41/model.savedmodel\n",
      "2023-10-08 23:48:42.650998: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:42.651082: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderpAhXch/41/model.savedmodel\n",
      "2023-10-08 23:48:42.676320: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:42.746942: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderpAhXch/41/model.savedmodel\n",
      "2023-10-08 23:48:42.796014: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 154003 microseconds.\n",
      "I1008 23:48:42.796264 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:42.796444: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderv9lyVf/40/model.savedmodel\n",
      "2023-10-08 23:48:42.805724: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:42.805804: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderv9lyVf/40/model.savedmodel\n",
      "2023-10-08 23:48:42.830768: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:42.907334: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderv9lyVf/40/model.savedmodel\n",
      "2023-10-08 23:48:42.952748: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 156310 microseconds.\n",
      "I1008 23:48:42.953048 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:42.953237: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderpAhXch/41/model.savedmodel\n",
      "2023-10-08 23:48:42.963738: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:42.963810: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderpAhXch/41/model.savedmodel\n",
      "2023-10-08 23:48:43.001602: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:43.085032: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderpAhXch/41/model.savedmodel\n",
      "2023-10-08 23:48:43.135244: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 182017 microseconds.\n",
      "I1008 23:48:43.135506 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:43.135895: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderv9lyVf/40/model.savedmodel\n",
      "2023-10-08 23:48:43.145100: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:43.145178: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderv9lyVf/40/model.savedmodel\n",
      "2023-10-08 23:48:43.185203: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:43.268579: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderv9lyVf/40/model.savedmodel\n",
      "2023-10-08 23:48:43.313643: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 177758 microseconds.\n",
      "I1008 23:48:57.390111 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 42)\n",
      "2023-10-08 23:48:57.390718: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder9VhX4i/42/model.savedmodel\n",
      "2023-10-08 23:48:57.398178: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:57.398240: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder9VhX4i/42/model.savedmodel\n",
      "2023-10-08 23:48:57.436660: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:57.516111: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder9VhX4i/42/model.savedmodel\n",
      "2023-10-08 23:48:57.561199: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 170493 microseconds.\n",
      "W1008 23:48:57.561340 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:57.584209 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:57.584343: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder9VhX4i/42/model.savedmodel\n",
      "2023-10-08 23:48:57.592279: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:57.592359: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder9VhX4i/42/model.savedmodel\n",
      "2023-10-08 23:48:57.616093: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:57.685769: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder9VhX4i/42/model.savedmodel\n",
      "2023-10-08 23:48:57.727660: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 143325 microseconds.\n",
      "I1008 23:48:57.728195 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:57.728331: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder9VhX4i/42/model.savedmodel\n",
      "2023-10-08 23:48:57.735644: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:57.735704: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder9VhX4i/42/model.savedmodel\n",
      "2023-10-08 23:48:57.778173: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:57.860348: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder9VhX4i/42/model.savedmodel\n",
      "2023-10-08 23:48:57.906151: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 177827 microseconds.\n",
      "I1008 23:48:58.448248 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 43)\n",
      "2023-10-08 23:48:58.448867: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldereyuzVg/43/model.savedmodel\n",
      "2023-10-08 23:48:58.458568: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:58.458637: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldereyuzVg/43/model.savedmodel\n",
      "2023-10-08 23:48:58.495814: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:58.578354: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldereyuzVg/43/model.savedmodel\n",
      "2023-10-08 23:48:58.621021: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 172165 microseconds.\n",
      "W1008 23:48:58.621140 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:58.643665 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:58.643765: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldereyuzVg/43/model.savedmodel\n",
      "2023-10-08 23:48:58.651511: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:58.651593: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldereyuzVg/43/model.savedmodel\n",
      "2023-10-08 23:48:58.675372: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:58.748512: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldereyuzVg/43/model.savedmodel\n",
      "2023-10-08 23:48:58.793735: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 149975 microseconds.\n",
      "I1008 23:48:58.794040 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 44)\n",
      "2023-10-08 23:48:58.794821: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder6B4bZh/44/model.savedmodel\n",
      "2023-10-08 23:48:58.805041: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:58.805103: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder6B4bZh/44/model.savedmodel\n",
      "2023-10-08 23:48:58.848987: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:58.927969: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder6B4bZh/44/model.savedmodel\n",
      "2023-10-08 23:48:58.973374: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 178563 microseconds.\n",
      "W1008 23:48:58.973498 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:58.999961 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:59.000118: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldereyuzVg/43/model.savedmodel\n",
      "2023-10-08 23:48:59.008267: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:59.008349: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldereyuzVg/43/model.savedmodel\n",
      "2023-10-08 23:48:59.053050: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:59.131312: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldereyuzVg/43/model.savedmodel\n",
      "2023-10-08 23:48:59.177039: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 176931 microseconds.\n",
      "I1008 23:48:59.178195 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:59.178297: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder6B4bZh/44/model.savedmodel\n",
      "2023-10-08 23:48:59.188114: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:59.188187: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder6B4bZh/44/model.savedmodel\n",
      "2023-10-08 23:48:59.212821: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:59.300032: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder6B4bZh/44/model.savedmodel\n",
      "2023-10-08 23:48:59.346365: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 168072 microseconds.\n",
      "I1008 23:48:59.346646 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 45)\n",
      "2023-10-08 23:48:59.347650: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderhPxjQg/45/model.savedmodel\n",
      "2023-10-08 23:48:59.358953: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:59.359025: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderhPxjQg/45/model.savedmodel\n",
      "2023-10-08 23:48:59.407896: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:59.487498: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderhPxjQg/45/model.savedmodel\n",
      "2023-10-08 23:48:59.532482: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 184841 microseconds.\n",
      "W1008 23:48:59.532611 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:48:59.554682 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:59.554806: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder6B4bZh/44/model.savedmodel\n",
      "2023-10-08 23:48:59.563374: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:59.563438: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder6B4bZh/44/model.savedmodel\n",
      "2023-10-08 23:48:59.601833: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:59.682194: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder6B4bZh/44/model.savedmodel\n",
      "2023-10-08 23:48:59.726481: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 171684 microseconds.\n",
      "I1008 23:48:59.726804 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:48:59.727003: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderhPxjQg/45/model.savedmodel\n",
      "2023-10-08 23:48:59.735886: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:59.735968: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderhPxjQg/45/model.savedmodel\n",
      "2023-10-08 23:48:59.759417: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:48:59.842952: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderhPxjQg/45/model.savedmodel\n",
      "2023-10-08 23:48:59.887960: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 160965 microseconds.\n",
      "I1008 23:48:59.888626 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:48:59.888719: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderhPxjQg/45/model.savedmodel\n",
      "2023-10-08 23:48:59.899751: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:48:59.899807: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderhPxjQg/45/model.savedmodel\n",
      "2023-10-08 23:48:59.937379: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:00.019710: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderhPxjQg/45/model.savedmodel\n",
      "2023-10-08 23:49:00.067223: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 178508 microseconds.\n",
      "I1008 23:49:00.683461 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 46)\n",
      "2023-10-08 23:49:00.684211: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderpy1fCi/46/model.savedmodel\n",
      "2023-10-08 23:49:00.691859: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:00.691922: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderpy1fCi/46/model.savedmodel\n",
      "2023-10-08 23:49:00.731447: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:00.811872: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderpy1fCi/46/model.savedmodel\n",
      "2023-10-08 23:49:00.858122: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 173925 microseconds.\n",
      "W1008 23:49:00.858241 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:00.884177 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 47)\n",
      "2023-10-08 23:49:00.884867: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderQuIsbf/47/model.savedmodel\n",
      "2023-10-08 23:49:00.895240: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:00.895304: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderQuIsbf/47/model.savedmodel\n",
      "2023-10-08 23:49:00.941696: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:01.019811: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderQuIsbf/47/model.savedmodel\n",
      "2023-10-08 23:49:01.063688: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 178831 microseconds.\n",
      "W1008 23:49:01.063802 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:01.088498 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:49:01.088620: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderpy1fCi/46/model.savedmodel\n",
      "2023-10-08 23:49:01.098375: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:01.098456: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderpy1fCi/46/model.savedmodel\n",
      "2023-10-08 23:49:01.123087: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:01.194666: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderpy1fCi/46/model.savedmodel\n",
      "2023-10-08 23:49:01.240541: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 151929 microseconds.\n",
      "I1008 23:49:01.241010 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:49:01.241094: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderpy1fCi/46/model.savedmodel\n",
      "2023-10-08 23:49:01.248341: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:01.248418: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderpy1fCi/46/model.savedmodel\n",
      "2023-10-08 23:49:01.283573: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:01.358714: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderpy1fCi/46/model.savedmodel\n",
      "2023-10-08 23:49:01.399181: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 158094 microseconds.\n",
      "I1008 23:49:01.399467 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:49:01.399859: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderQuIsbf/47/model.savedmodel\n",
      "2023-10-08 23:49:01.407387: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:01.407450: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderQuIsbf/47/model.savedmodel\n",
      "2023-10-08 23:49:01.432219: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:01.519811: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderQuIsbf/47/model.savedmodel\n",
      "2023-10-08 23:49:01.565059: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 165328 microseconds.\n",
      "I1008 23:49:01.565568 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:49:01.565676: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderQuIsbf/47/model.savedmodel\n",
      "2023-10-08 23:49:01.577684: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:01.577756: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderQuIsbf/47/model.savedmodel\n",
      "2023-10-08 23:49:01.615363: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:01.705874: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderQuIsbf/47/model.savedmodel\n",
      "2023-10-08 23:49:01.754475: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 188803 microseconds.\n",
      "I1008 23:49:02.172450 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 48)\n",
      "2023-10-08 23:49:02.174169: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderLbTMrh/48/model.savedmodel\n",
      "2023-10-08 23:49:02.183510: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:02.183589: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderLbTMrh/48/model.savedmodel\n",
      "2023-10-08 23:49:02.221040: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:02.303815: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderLbTMrh/48/model.savedmodel\n",
      "2023-10-08 23:49:02.351794: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 177635 microseconds.\n",
      "W1008 23:49:02.351924 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:02.374305 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 49)\n",
      "2023-10-08 23:49:02.375034: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderY2ONkh/49/model.savedmodel\n",
      "2023-10-08 23:49:02.384579: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:02.384647: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderY2ONkh/49/model.savedmodel\n",
      "2023-10-08 23:49:02.421582: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:02.511778: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderY2ONkh/49/model.savedmodel\n",
      "2023-10-08 23:49:02.561792: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 186771 microseconds.\n",
      "W1008 23:49:02.561936 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:02.591843 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:49:02.592080: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderLbTMrh/48/model.savedmodel\n",
      "2023-10-08 23:49:02.601716: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:02.601785: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderLbTMrh/48/model.savedmodel\n",
      "2023-10-08 23:49:02.628994: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:02.708582: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderLbTMrh/48/model.savedmodel\n",
      "2023-10-08 23:49:02.753956: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 161888 microseconds.\n",
      "I1008 23:49:02.754211 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:49:02.754678: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderY2ONkh/49/model.savedmodel\n",
      "2023-10-08 23:49:02.768274: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:02.768357: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderY2ONkh/49/model.savedmodel\n",
      "2023-10-08 23:49:02.804772: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:02.886468: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderY2ONkh/49/model.savedmodel\n",
      "2023-10-08 23:49:02.941485: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 186815 microseconds.\n",
      "I1008 23:49:02.941725 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:49:02.941837: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderLbTMrh/48/model.savedmodel\n",
      "2023-10-08 23:49:02.950852: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:02.950926: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderLbTMrh/48/model.savedmodel\n",
      "2023-10-08 23:49:02.987602: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:03.082089: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderLbTMrh/48/model.savedmodel\n",
      "2023-10-08 23:49:03.143245: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 201414 microseconds.\n",
      "I1008 23:49:03.143607 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:49:03.143739: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderY2ONkh/49/model.savedmodel\n",
      "2023-10-08 23:49:03.153461: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:03.153521: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderY2ONkh/49/model.savedmodel\n",
      "2023-10-08 23:49:03.188653: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:03.279578: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderY2ONkh/49/model.savedmodel\n",
      "2023-10-08 23:49:03.342066: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 198332 microseconds.\n",
      "I1008 23:49:03.419196 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05h (version 2)\n",
      "2023-10-08 23:49:03.419854: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderEHI9kh/2/model.savedmodel\n",
      "2023-10-08 23:49:03.431267: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:03.431352: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderEHI9kh/2/model.savedmodel\n",
      "2023-10-08 23:49:03.476855: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:03.573673: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderEHI9kh/2/model.savedmodel\n",
      "2023-10-08 23:49:03.623046: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 203202 microseconds.\n",
      "W1008 23:49:03.623199 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05h' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:03.652553 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05h (version 1)\n",
      "2023-10-08 23:49:03.653486: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder010Ztg/1/model.savedmodel\n",
      "2023-10-08 23:49:03.663067: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:03.663139: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder010Ztg/1/model.savedmodel\n",
      "2023-10-08 23:49:03.703161: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:03.821113: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder010Ztg/1/model.savedmodel\n",
      "2023-10-08 23:49:03.866901: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 213425 microseconds.\n",
      "W1008 23:49:03.867036 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05h' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:03.893712 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05h_0 (CPU device 0)\n",
      "2023-10-08 23:49:03.893895: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderEHI9kh/2/model.savedmodel\n",
      "2023-10-08 23:49:03.907839: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:03.907928: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderEHI9kh/2/model.savedmodel\n",
      "2023-10-08 23:49:03.934951: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:04.018672: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderEHI9kh/2/model.savedmodel\n",
      "2023-10-08 23:49:04.065940: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 172084 microseconds.\n",
      "I1008 23:49:04.066210 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05h_0 (CPU device 0)\n",
      "2023-10-08 23:49:04.066317: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder010Ztg/1/model.savedmodel\n",
      "2023-10-08 23:49:04.076347: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:04.076639: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder010Ztg/1/model.savedmodel\n",
      "2023-10-08 23:49:04.109407: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:04.245686: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder010Ztg/1/model.savedmodel\n",
      "2023-10-08 23:49:04.292464: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 226150 microseconds.\n",
      "I1008 23:49:04.293296 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05h_1 (CPU device 0)\n",
      "2023-10-08 23:49:04.294029: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderEHI9kh/2/model.savedmodel\n",
      "2023-10-08 23:49:04.306620: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:04.306680: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderEHI9kh/2/model.savedmodel\n",
      "2023-10-08 23:49:04.340845: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:04.425084: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderEHI9kh/2/model.savedmodel\n",
      "2023-10-08 23:49:04.471231: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 177232 microseconds.\n",
      "I1008 23:49:04.471816 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05h_1 (CPU device 0)\n",
      "2023-10-08 23:49:04.472265: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder010Ztg/1/model.savedmodel\n",
      "2023-10-08 23:49:04.481133: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:04.481209: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder010Ztg/1/model.savedmodel\n",
      "2023-10-08 23:49:04.534210: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:04.640132: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder010Ztg/1/model.savedmodel\n",
      "2023-10-08 23:49:04.707533: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 235278 microseconds.\n",
      "I1008 23:49:04.825752 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05h (version 3)\n",
      "2023-10-08 23:49:04.826454: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderzyJpPg/3/model.savedmodel\n",
      "2023-10-08 23:49:04.838501: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:04.838582: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderzyJpPg/3/model.savedmodel\n",
      "2023-10-08 23:49:04.881411: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:04.965245: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderzyJpPg/3/model.savedmodel\n",
      "2023-10-08 23:49:05.012166: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 185723 microseconds.\n",
      "W1008 23:49:05.012302 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05h' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:05.037198 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05h_0 (CPU device 0)\n",
      "2023-10-08 23:49:05.037320: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderzyJpPg/3/model.savedmodel\n",
      "2023-10-08 23:49:05.044918: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:05.044993: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderzyJpPg/3/model.savedmodel\n",
      "2023-10-08 23:49:05.071062: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:05.151949: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderzyJpPg/3/model.savedmodel\n",
      "2023-10-08 23:49:05.199550: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 162238 microseconds.\n",
      "I1008 23:49:05.200148 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05h_1 (CPU device 0)\n",
      "2023-10-08 23:49:05.200237: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderzyJpPg/3/model.savedmodel\n",
      "2023-10-08 23:49:05.207714: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:05.207799: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderzyJpPg/3/model.savedmodel\n",
      "2023-10-08 23:49:05.242144: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:05.322238: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderzyJpPg/3/model.savedmodel\n",
      "2023-10-08 23:49:05.368040: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 167808 microseconds.\n",
      "I1008 23:49:05.368782 1 model_lifecycle.cc:694] successfully loaded '05_05h' version 3\n",
      "I1008 23:49:05.368825 1 model_lifecycle.cc:694] successfully loaded '05_05h' version 3\n",
      "I1008 23:49:05.368848 1 model_lifecycle.cc:694] successfully loaded '05_05h' version 3\n",
      "I1008 23:49:05.383213 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05i (version 1)\n",
      "2023-10-08 23:49:05.383922: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder3DPAzi/1/model.savedmodel\n",
      "2023-10-08 23:49:05.392447: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:05.392513: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder3DPAzi/1/model.savedmodel\n",
      "2023-10-08 23:49:05.443156: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:05.548615: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder3DPAzi/1/model.savedmodel\n",
      "2023-10-08 23:49:05.598520: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 214610 microseconds.\n",
      "W1008 23:49:05.598645 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05i' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:05.626697 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05i_0 (CPU device 0)\n",
      "2023-10-08 23:49:05.626799: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder3DPAzi/1/model.savedmodel\n",
      "2023-10-08 23:49:05.635263: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:05.635331: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder3DPAzi/1/model.savedmodel\n",
      "2023-10-08 23:49:05.660584: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:05.756451: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder3DPAzi/1/model.savedmodel\n",
      "2023-10-08 23:49:05.805345: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 178555 microseconds.\n",
      "I1008 23:49:05.805795 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05i_1 (CPU device 0)\n",
      "2023-10-08 23:49:05.805888: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder3DPAzi/1/model.savedmodel\n",
      "2023-10-08 23:49:05.814684: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:05.814755: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder3DPAzi/1/model.savedmodel\n",
      "2023-10-08 23:49:05.852638: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:05.953266: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder3DPAzi/1/model.savedmodel\n",
      "2023-10-08 23:49:06.001516: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 195633 microseconds.\n",
      "I1008 23:49:06.283815 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05i (version 2)\n",
      "2023-10-08 23:49:06.284692: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder6iBjWh/2/model.savedmodel\n",
      "2023-10-08 23:49:06.293943: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:06.294022: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder6iBjWh/2/model.savedmodel\n",
      "2023-10-08 23:49:06.334952: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:06.427082: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder6iBjWh/2/model.savedmodel\n",
      "2023-10-08 23:49:06.476026: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 191346 microseconds.\n",
      "W1008 23:49:06.476151 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05i' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:06.503284 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05i (version 3)\n",
      "2023-10-08 23:49:06.504317: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder8rOOri/3/model.savedmodel\n",
      "2023-10-08 23:49:06.512539: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:06.512620: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder8rOOri/3/model.savedmodel\n",
      "2023-10-08 23:49:06.561689: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:06.646697: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder8rOOri/3/model.savedmodel\n",
      "2023-10-08 23:49:06.688104: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 183799 microseconds.\n",
      "W1008 23:49:06.688212 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05i' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:06.712562 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05i_0 (CPU device 0)\n",
      "2023-10-08 23:49:06.712698: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder6iBjWh/2/model.savedmodel\n",
      "2023-10-08 23:49:06.721027: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:06.721101: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder6iBjWh/2/model.savedmodel\n",
      "2023-10-08 23:49:06.744041: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:06.824458: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder6iBjWh/2/model.savedmodel\n",
      "2023-10-08 23:49:06.871798: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 159108 microseconds.\n",
      "I1008 23:49:06.872065 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05i_0 (CPU device 0)\n",
      "2023-10-08 23:49:06.872229: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder8rOOri/3/model.savedmodel\n",
      "2023-10-08 23:49:06.881934: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:06.882007: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder8rOOri/3/model.savedmodel\n",
      "2023-10-08 23:49:06.909367: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:06.986049: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder8rOOri/3/model.savedmodel\n",
      "2023-10-08 23:49:07.036691: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 164469 microseconds.\n",
      "I1008 23:49:07.036956 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05i_1 (CPU device 0)\n",
      "2023-10-08 23:49:07.037139: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder6iBjWh/2/model.savedmodel\n",
      "2023-10-08 23:49:07.044712: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:07.044783: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder6iBjWh/2/model.savedmodel\n",
      "2023-10-08 23:49:07.089665: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:07.179461: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder6iBjWh/2/model.savedmodel\n",
      "2023-10-08 23:49:07.225857: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 188727 microseconds.\n",
      "I1008 23:49:07.226180 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05i (version 4)\n",
      "2023-10-08 23:49:07.227107: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderhMliai/4/model.savedmodel\n",
      "2023-10-08 23:49:07.235009: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:07.235080: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderhMliai/4/model.savedmodel\n",
      "2023-10-08 23:49:07.271670: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:07.355619: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderhMliai/4/model.savedmodel\n",
      "2023-10-08 23:49:07.401536: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 174466 microseconds.\n",
      "W1008 23:49:07.401672 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05i' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:07.424545 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05i_1 (CPU device 0)\n",
      "2023-10-08 23:49:07.424689: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder8rOOri/3/model.savedmodel\n",
      "2023-10-08 23:49:07.434957: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:07.435032: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder8rOOri/3/model.savedmodel\n",
      "2023-10-08 23:49:07.480613: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:07.569691: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder8rOOri/3/model.savedmodel\n",
      "2023-10-08 23:49:07.616711: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 192029 microseconds.\n",
      "I1008 23:49:07.616981 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05i_0 (CPU device 0)\n",
      "2023-10-08 23:49:07.617132: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderhMliai/4/model.savedmodel\n",
      "2023-10-08 23:49:07.626564: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:07.626643: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderhMliai/4/model.savedmodel\n",
      "2023-10-08 23:49:07.652782: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:07.731083: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderhMliai/4/model.savedmodel\n",
      "2023-10-08 23:49:07.777039: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 159913 microseconds.\n",
      "I1008 23:49:07.777320 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05d (version 1)\n",
      "2023-10-08 23:49:07.778066: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderxLMCSi/1/model.savedmodel\n",
      "2023-10-08 23:49:07.785793: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:07.785855: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderxLMCSi/1/model.savedmodel\n",
      "2023-10-08 23:49:07.821561: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:07.910549: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderxLMCSi/1/model.savedmodel\n",
      "2023-10-08 23:49:07.951826: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 173766 microseconds.\n",
      "W1008 23:49:07.951969 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05d' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:07.972660 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05i_1 (CPU device 0)\n",
      "2023-10-08 23:49:07.972884: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderhMliai/4/model.savedmodel\n",
      "2023-10-08 23:49:07.983600: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:07.983662: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderhMliai/4/model.savedmodel\n",
      "2023-10-08 23:49:08.024991: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:08.115389: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderhMliai/4/model.savedmodel\n",
      "2023-10-08 23:49:08.161908: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 189034 microseconds.\n",
      "I1008 23:49:08.162195 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05d_0 (CPU device 0)\n",
      "2023-10-08 23:49:08.162610: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderxLMCSi/1/model.savedmodel\n",
      "I1008 23:49:08.162994 1 model_lifecycle.cc:694] successfully loaded '05_05i' version 4\n",
      "I1008 23:49:08.163027 1 model_lifecycle.cc:694] successfully loaded '05_05i' version 4\n",
      "I1008 23:49:08.163041 1 model_lifecycle.cc:694] successfully loaded '05_05i' version 4\n",
      "I1008 23:49:08.163083 1 model_lifecycle.cc:694] successfully loaded '05_05i' version 4\n",
      "2023-10-08 23:49:08.173130: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:08.173201: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderxLMCSi/1/model.savedmodel\n",
      "2023-10-08 23:49:08.197882: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:08.281359: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderxLMCSi/1/model.savedmodel\n",
      "2023-10-08 23:49:08.327653: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 165052 microseconds.\n",
      "I1008 23:49:08.328310 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05d_1 (CPU device 0)\n",
      "2023-10-08 23:49:08.328434: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderxLMCSi/1/model.savedmodel\n",
      "2023-10-08 23:49:08.339255: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:08.339386: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderxLMCSi/1/model.savedmodel\n",
      "2023-10-08 23:49:08.381349: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:08.470450: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderxLMCSi/1/model.savedmodel\n",
      "2023-10-08 23:49:08.516353: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 187952 microseconds.\n",
      "I1008 23:49:08.921788 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05d (version 2)\n",
      "2023-10-08 23:49:08.922428: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldereFacXe/2/model.savedmodel\n",
      "2023-10-08 23:49:08.930877: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:08.930939: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldereFacXe/2/model.savedmodel\n",
      "2023-10-08 23:49:08.965787: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:09.045449: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldereFacXe/2/model.savedmodel\n",
      "2023-10-08 23:49:09.090171: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 167754 microseconds.\n",
      "W1008 23:49:09.090315 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05d' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:09.113871 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05d (version 3)\n",
      "2023-10-08 23:49:09.114863: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder9dC5wg/3/model.savedmodel\n",
      "2023-10-08 23:49:09.121845: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:09.121914: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder9dC5wg/3/model.savedmodel\n",
      "2023-10-08 23:49:09.157194: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:09.234754: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder9dC5wg/3/model.savedmodel\n",
      "2023-10-08 23:49:09.277125: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 162272 microseconds.\n",
      "W1008 23:49:09.277254 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05d' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:09.297846 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05d_0 (CPU device 0)\n",
      "2023-10-08 23:49:09.298485: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldereFacXe/2/model.savedmodel\n",
      "2023-10-08 23:49:09.307571: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:09.307649: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldereFacXe/2/model.savedmodel\n",
      "2023-10-08 23:49:09.330449: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:09.400227: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldereFacXe/2/model.savedmodel\n",
      "2023-10-08 23:49:09.445533: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 147060 microseconds.\n",
      "I1008 23:49:09.445780 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05d_0 (CPU device 0)\n",
      "2023-10-08 23:49:09.445902: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder9dC5wg/3/model.savedmodel\n",
      "2023-10-08 23:49:09.452803: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:09.452875: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder9dC5wg/3/model.savedmodel\n",
      "2023-10-08 23:49:09.476623: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:09.549719: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder9dC5wg/3/model.savedmodel\n",
      "2023-10-08 23:49:09.592141: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 146246 microseconds.\n",
      "I1008 23:49:09.592641 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05d_1 (CPU device 0)\n",
      "2023-10-08 23:49:09.592751: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldereFacXe/2/model.savedmodel\n",
      "2023-10-08 23:49:09.599366: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:09.599428: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldereFacXe/2/model.savedmodel\n",
      "2023-10-08 23:49:09.634439: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:09.713508: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldereFacXe/2/model.savedmodel\n",
      "2023-10-08 23:49:09.756243: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 163499 microseconds.\n",
      "I1008 23:49:09.756614 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05d_1 (CPU device 0)\n",
      "2023-10-08 23:49:09.756753: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder9dC5wg/3/model.savedmodel\n",
      "2023-10-08 23:49:09.765539: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:09.765599: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder9dC5wg/3/model.savedmodel\n",
      "2023-10-08 23:49:09.801262: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:09.879556: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder9dC5wg/3/model.savedmodel\n",
      "2023-10-08 23:49:09.921031: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 164285 microseconds.\n",
      "I1008 23:49:09.921784 1 model_lifecycle.cc:694] successfully loaded '05_05d' version 3\n",
      "I1008 23:49:09.921824 1 model_lifecycle.cc:694] successfully loaded '05_05d' version 3\n",
      "I1008 23:49:09.921839 1 model_lifecycle.cc:694] successfully loaded '05_05d' version 3\n",
      "I1008 23:49:13.947979 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05 (version 3)\n",
      "2023-10-08 23:49:13.948673: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder352jki/3/model.savedmodel\n",
      "2023-10-08 23:49:13.956587: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:13.956658: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder352jki/3/model.savedmodel\n",
      "2023-10-08 23:49:13.996915: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:14.083086: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder352jki/3/model.savedmodel\n",
      "2023-10-08 23:49:14.129244: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 180583 microseconds.\n",
      "W1008 23:49:14.129398 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:14.152804 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05 (version 2)\n",
      "2023-10-08 23:49:14.153582: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder3NPLlg/2/model.savedmodel\n",
      "2023-10-08 23:49:14.162506: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:14.162584: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder3NPLlg/2/model.savedmodel\n",
      "2023-10-08 23:49:14.202793: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:14.286499: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder3NPLlg/2/model.savedmodel\n",
      "2023-10-08 23:49:14.332668: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 179098 microseconds.\n",
      "W1008 23:49:14.332774 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:14.358642 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_0 (CPU device 0)\n",
      "2023-10-08 23:49:14.358830: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder352jki/3/model.savedmodel\n",
      "2023-10-08 23:49:14.367812: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:14.367879: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder352jki/3/model.savedmodel\n",
      "2023-10-08 23:49:14.393834: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:14.470806: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder352jki/3/model.savedmodel\n",
      "2023-10-08 23:49:14.516862: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 158069 microseconds.\n",
      "I1008 23:49:14.517103 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_0 (CPU device 0)\n",
      "2023-10-08 23:49:14.517346: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder3NPLlg/2/model.savedmodel\n",
      "2023-10-08 23:49:14.527101: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:14.527205: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder3NPLlg/2/model.savedmodel\n",
      "2023-10-08 23:49:14.553353: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:14.632389: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder3NPLlg/2/model.savedmodel\n",
      "2023-10-08 23:49:14.679083: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 161746 microseconds.\n",
      "I1008 23:49:14.679342 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_1 (CPU device 0)\n",
      "2023-10-08 23:49:14.680057: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder352jki/3/model.savedmodel\n",
      "2023-10-08 23:49:14.689251: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:14.689329: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder352jki/3/model.savedmodel\n",
      "2023-10-08 23:49:14.728776: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:14.814186: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder352jki/3/model.savedmodel\n",
      "2023-10-08 23:49:14.860528: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 180480 microseconds.\n",
      "I1008 23:49:14.860844 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_1 (CPU device 0)\n",
      "2023-10-08 23:49:14.861046: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder3NPLlg/2/model.savedmodel\n",
      "2023-10-08 23:49:14.871201: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:14.871262: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder3NPLlg/2/model.savedmodel\n",
      "2023-10-08 23:49:14.907685: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:15.000107: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder3NPLlg/2/model.savedmodel\n",
      "2023-10-08 23:49:15.046770: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 185738 microseconds.\n",
      "I1008 23:49:15.047179 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05 (version 4)\n",
      "2023-10-08 23:49:15.047902: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder1VXV6f/4/model.savedmodel\n",
      "2023-10-08 23:49:15.056158: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:15.056227: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder1VXV6f/4/model.savedmodel\n",
      "2023-10-08 23:49:15.097827: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:15.179613: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder1VXV6f/4/model.savedmodel\n",
      "2023-10-08 23:49:15.224571: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 176676 microseconds.\n",
      "W1008 23:49:15.224700 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:15.248111 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05 (version 5)\n",
      "2023-10-08 23:49:15.248910: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderCt7bfi/5/model.savedmodel\n",
      "2023-10-08 23:49:15.257377: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:15.257445: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderCt7bfi/5/model.savedmodel\n",
      "2023-10-08 23:49:15.296423: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:15.379133: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderCt7bfi/5/model.savedmodel\n",
      "2023-10-08 23:49:15.420714: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 171816 microseconds.\n",
      "W1008 23:49:15.420858 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:15.447082 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_0 (CPU device 0)\n",
      "2023-10-08 23:49:15.447237: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder1VXV6f/4/model.savedmodel\n",
      "2023-10-08 23:49:15.454713: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:15.454781: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder1VXV6f/4/model.savedmodel\n",
      "2023-10-08 23:49:15.477027: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:15.547217: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder1VXV6f/4/model.savedmodel\n",
      "2023-10-08 23:49:15.591028: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 143801 microseconds.\n",
      "I1008 23:49:15.591313 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_0 (CPU device 0)\n",
      "2023-10-08 23:49:15.591616: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderCt7bfi/5/model.savedmodel\n",
      "2023-10-08 23:49:15.600237: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:15.600307: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderCt7bfi/5/model.savedmodel\n",
      "2023-10-08 23:49:15.624714: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:15.695959: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderCt7bfi/5/model.savedmodel\n",
      "2023-10-08 23:49:15.739633: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 148025 microseconds.\n",
      "I1008 23:49:15.739899 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_1 (CPU device 0)\n",
      "2023-10-08 23:49:15.740255: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder1VXV6f/4/model.savedmodel\n",
      "2023-10-08 23:49:15.747659: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:15.747727: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder1VXV6f/4/model.savedmodel\n",
      "2023-10-08 23:49:15.786021: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:15.866429: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder1VXV6f/4/model.savedmodel\n",
      "2023-10-08 23:49:15.911964: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 171729 microseconds.\n",
      "I1008 23:49:15.912236 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_1 (CPU device 0)\n",
      "2023-10-08 23:49:15.912347: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderCt7bfi/5/model.savedmodel\n",
      "2023-10-08 23:49:15.921739: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:15.921797: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderCt7bfi/5/model.savedmodel\n",
      "2023-10-08 23:49:15.959041: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:16.040085: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderCt7bfi/5/model.savedmodel\n",
      "2023-10-08 23:49:16.084168: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 171826 microseconds.\n",
      "I1008 23:49:16.907001 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 50)\n",
      "2023-10-08 23:49:16.907680: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder09WD1g/50/model.savedmodel\n",
      "2023-10-08 23:49:16.919451: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:16.919525: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder09WD1g/50/model.savedmodel\n",
      "2023-10-08 23:49:16.959805: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:17.043877: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder09WD1g/50/model.savedmodel\n",
      "2023-10-08 23:49:17.088813: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 181144 microseconds.\n",
      "W1008 23:49:17.088974 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:17.111449 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:49:17.111564: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder09WD1g/50/model.savedmodel\n",
      "2023-10-08 23:49:17.119693: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:17.119773: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder09WD1g/50/model.savedmodel\n",
      "2023-10-08 23:49:17.144636: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:17.217969: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder09WD1g/50/model.savedmodel\n",
      "2023-10-08 23:49:17.261614: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 150058 microseconds.\n",
      "I1008 23:49:17.262269 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:49:17.262378: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder09WD1g/50/model.savedmodel\n",
      "2023-10-08 23:49:17.271261: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:17.271338: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder09WD1g/50/model.savedmodel\n",
      "2023-10-08 23:49:17.307547: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:17.385374: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder09WD1g/50/model.savedmodel\n",
      "2023-10-08 23:49:17.428975: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 166601 microseconds.\n",
      "I1008 23:49:18.315642 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 51)\n",
      "2023-10-08 23:49:18.316459: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderXZaxTh/51/model.savedmodel\n",
      "2023-10-08 23:49:18.324571: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:18.324634: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderXZaxTh/51/model.savedmodel\n",
      "2023-10-08 23:49:18.362959: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:18.445743: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderXZaxTh/51/model.savedmodel\n",
      "2023-10-08 23:49:18.491449: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 174999 microseconds.\n",
      "W1008 23:49:18.491587 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:18.525508 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:49:18.525608: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderXZaxTh/51/model.savedmodel\n",
      "2023-10-08 23:49:18.538507: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:18.538597: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderXZaxTh/51/model.savedmodel\n",
      "2023-10-08 23:49:18.568331: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:18.640618: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderXZaxTh/51/model.savedmodel\n",
      "2023-10-08 23:49:18.689133: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 163532 microseconds.\n",
      "I1008 23:49:18.689758 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:49:18.689842: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderXZaxTh/51/model.savedmodel\n",
      "2023-10-08 23:49:18.697991: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:18.698049: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderXZaxTh/51/model.savedmodel\n",
      "2023-10-08 23:49:18.735196: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:18.816072: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderXZaxTh/51/model.savedmodel\n",
      "2023-10-08 23:49:18.863026: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 173189 microseconds.\n",
      "I1008 23:49:18.863309 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 52)\n",
      "2023-10-08 23:49:18.864398: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldervHBS4f/52/model.savedmodel\n",
      "2023-10-08 23:49:18.873704: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:18.873784: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldervHBS4f/52/model.savedmodel\n",
      "2023-10-08 23:49:18.909975: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:18.993319: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldervHBS4f/52/model.savedmodel\n",
      "2023-10-08 23:49:19.041295: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 176880 microseconds.\n",
      "W1008 23:49:19.041422 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:19.066129 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:49:19.066247: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldervHBS4f/52/model.savedmodel\n",
      "2023-10-08 23:49:19.074248: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:19.074319: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldervHBS4f/52/model.savedmodel\n",
      "2023-10-08 23:49:19.104032: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:19.182289: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldervHBS4f/52/model.savedmodel\n",
      "2023-10-08 23:49:19.231506: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 165268 microseconds.\n",
      "I1008 23:49:19.232049 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:49:19.232164: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/foldervHBS4f/52/model.savedmodel\n",
      "2023-10-08 23:49:19.242191: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:19.242271: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/foldervHBS4f/52/model.savedmodel\n",
      "2023-10-08 23:49:19.278321: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:19.354872: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/foldervHBS4f/52/model.savedmodel\n",
      "2023-10-08 23:49:19.403157: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 170997 microseconds.\n",
      "I1008 23:49:19.403647 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05f (version 53)\n",
      "2023-10-08 23:49:19.404386: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderhT45aj/53/model.savedmodel\n",
      "2023-10-08 23:49:19.414633: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:19.414699: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderhT45aj/53/model.savedmodel\n",
      "2023-10-08 23:49:19.453144: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:19.539929: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderhT45aj/53/model.savedmodel\n",
      "2023-10-08 23:49:19.585557: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 181179 microseconds.\n",
      "W1008 23:49:19.585690 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05f' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:19.608598 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_0 (CPU device 0)\n",
      "2023-10-08 23:49:19.608699: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderhT45aj/53/model.savedmodel\n",
      "2023-10-08 23:49:19.616774: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:19.616858: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderhT45aj/53/model.savedmodel\n",
      "2023-10-08 23:49:19.641399: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:19.713171: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderhT45aj/53/model.savedmodel\n",
      "2023-10-08 23:49:19.758163: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 149472 microseconds.\n",
      "I1008 23:49:19.758763 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05f_1 (CPU device 0)\n",
      "2023-10-08 23:49:19.758853: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderhT45aj/53/model.savedmodel\n",
      "2023-10-08 23:49:19.768207: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:19.768293: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderhT45aj/53/model.savedmodel\n",
      "2023-10-08 23:49:19.802695: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:19.877708: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderhT45aj/53/model.savedmodel\n",
      "2023-10-08 23:49:19.922004: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 163156 microseconds.\n",
      "I1008 23:49:19.922737 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.922774 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.922811 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.922838 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.922860 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.922879 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.922899 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.922925 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.922953 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.922974 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923010 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923029 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923050 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923076 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923094 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923112 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923123 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923150 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923174 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923190 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923218 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923259 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923284 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923304 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923324 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923343 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923365 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923398 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923461 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923486 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923511 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923531 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923549 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923562 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923586 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923612 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923634 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923662 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923684 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923702 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923726 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923749 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923771 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923790 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923807 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923819 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923839 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923866 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923886 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923900 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923912 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.923934 1 model_lifecycle.cc:694] successfully loaded '05_05f' version 53\n",
      "I1008 23:49:19.975117 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05 (version 6)\n",
      "2023-10-08 23:49:19.975807: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder4mdERi/6/model.savedmodel\n",
      "2023-10-08 23:49:19.987490: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:19.987564: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder4mdERi/6/model.savedmodel\n",
      "2023-10-08 23:49:20.027690: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:20.111629: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder4mdERi/6/model.savedmodel\n",
      "2023-10-08 23:49:20.155987: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 180190 microseconds.\n",
      "W1008 23:49:20.156105 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:20.177629 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05 (version 7)\n",
      "2023-10-08 23:49:20.178272: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderHnxL3e/7/model.savedmodel\n",
      "2023-10-08 23:49:20.185990: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:20.186057: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderHnxL3e/7/model.savedmodel\n",
      "2023-10-08 23:49:20.224645: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:20.308073: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderHnxL3e/7/model.savedmodel\n",
      "2023-10-08 23:49:20.354116: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 175854 microseconds.\n",
      "W1008 23:49:20.354251 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:20.378253 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_0 (CPU device 0)\n",
      "2023-10-08 23:49:20.378385: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder4mdERi/6/model.savedmodel\n",
      "2023-10-08 23:49:20.389976: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:20.390051: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder4mdERi/6/model.savedmodel\n",
      "2023-10-08 23:49:20.413167: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:20.485905: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder4mdERi/6/model.savedmodel\n",
      "2023-10-08 23:49:20.531537: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 153160 microseconds.\n",
      "I1008 23:49:20.531803 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_0 (CPU device 0)\n",
      "2023-10-08 23:49:20.532079: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderHnxL3e/7/model.savedmodel\n",
      "2023-10-08 23:49:20.539815: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:20.539872: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderHnxL3e/7/model.savedmodel\n",
      "2023-10-08 23:49:20.564927: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:20.640419: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderHnxL3e/7/model.savedmodel\n",
      "2023-10-08 23:49:20.686648: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 154575 microseconds.\n",
      "I1008 23:49:20.686873 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_1 (CPU device 0)\n",
      "2023-10-08 23:49:20.687178: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder4mdERi/6/model.savedmodel\n",
      "2023-10-08 23:49:20.694692: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:20.694753: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder4mdERi/6/model.savedmodel\n",
      "2023-10-08 23:49:20.731766: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:20.815990: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder4mdERi/6/model.savedmodel\n",
      "2023-10-08 23:49:20.858638: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 171467 microseconds.\n",
      "I1008 23:49:20.858951 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_1 (CPU device 0)\n",
      "2023-10-08 23:49:20.859188: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderHnxL3e/7/model.savedmodel\n",
      "2023-10-08 23:49:20.868891: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:20.868953: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderHnxL3e/7/model.savedmodel\n",
      "2023-10-08 23:49:20.909505: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:20.990679: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderHnxL3e/7/model.savedmodel\n",
      "2023-10-08 23:49:21.035467: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 176286 microseconds.\n",
      "I1008 23:49:21.035740 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05 (version 8)\n",
      "2023-10-08 23:49:21.036858: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder1GPQSf/8/model.savedmodel\n",
      "2023-10-08 23:49:21.046269: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:21.046327: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder1GPQSf/8/model.savedmodel\n",
      "2023-10-08 23:49:21.085764: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:21.183157: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder1GPQSf/8/model.savedmodel\n",
      "2023-10-08 23:49:21.230433: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 193581 microseconds.\n",
      "W1008 23:49:21.230564 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:21.252401 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05 (version 9)\n",
      "2023-10-08 23:49:21.253105: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderkG8ygf/9/model.savedmodel\n",
      "2023-10-08 23:49:21.262054: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:21.262191: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderkG8ygf/9/model.savedmodel\n",
      "2023-10-08 23:49:21.302333: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:21.388746: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderkG8ygf/9/model.savedmodel\n",
      "2023-10-08 23:49:21.431879: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 178784 microseconds.\n",
      "W1008 23:49:21.432057 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:21.456196 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_0 (CPU device 0)\n",
      "2023-10-08 23:49:21.456386: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder1GPQSf/8/model.savedmodel\n",
      "2023-10-08 23:49:21.465718: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:21.465795: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder1GPQSf/8/model.savedmodel\n",
      "2023-10-08 23:49:21.489399: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:21.564703: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder1GPQSf/8/model.savedmodel\n",
      "2023-10-08 23:49:21.612453: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 156078 microseconds.\n",
      "I1008 23:49:21.612703 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_0 (CPU device 0)\n",
      "2023-10-08 23:49:21.612871: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderkG8ygf/9/model.savedmodel\n",
      "2023-10-08 23:49:21.621794: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:21.621867: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderkG8ygf/9/model.savedmodel\n",
      "2023-10-08 23:49:21.647703: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:21.720923: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderkG8ygf/9/model.savedmodel\n",
      "2023-10-08 23:49:21.764128: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 151263 microseconds.\n",
      "I1008 23:49:21.764411 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_1 (CPU device 0)\n",
      "2023-10-08 23:49:21.764642: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folder1GPQSf/8/model.savedmodel\n",
      "2023-10-08 23:49:21.772638: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:21.772703: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folder1GPQSf/8/model.savedmodel\n",
      "2023-10-08 23:49:21.811565: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:21.890051: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folder1GPQSf/8/model.savedmodel\n",
      "2023-10-08 23:49:21.934543: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 169910 microseconds.\n",
      "I1008 23:49:21.934811 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_1 (CPU device 0)\n",
      "2023-10-08 23:49:21.935168: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderkG8ygf/9/model.savedmodel\n",
      "2023-10-08 23:49:21.944783: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:21.944845: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderkG8ygf/9/model.savedmodel\n",
      "2023-10-08 23:49:21.984047: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:22.068094: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderkG8ygf/9/model.savedmodel\n",
      "2023-10-08 23:49:22.113580: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 178418 microseconds.\n",
      "I1008 23:49:22.468711 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05 (version 10)\n",
      "2023-10-08 23:49:22.469342: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderzMHLyi/10/model.savedmodel\n",
      "2023-10-08 23:49:22.479084: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:22.479154: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderzMHLyi/10/model.savedmodel\n",
      "2023-10-08 23:49:22.517122: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:22.595563: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderzMHLyi/10/model.savedmodel\n",
      "2023-10-08 23:49:22.641664: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 172333 microseconds.\n",
      "W1008 23:49:22.641797 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:22.666103 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_0 (CPU device 0)\n",
      "2023-10-08 23:49:22.666226: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderzMHLyi/10/model.savedmodel\n",
      "2023-10-08 23:49:22.674036: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:22.674102: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderzMHLyi/10/model.savedmodel\n",
      "2023-10-08 23:49:22.697879: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:22.771108: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderzMHLyi/10/model.savedmodel\n",
      "2023-10-08 23:49:22.814811: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 148591 microseconds.\n",
      "I1008 23:49:22.815077 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05b (version 2)\n",
      "2023-10-08 23:49:22.815721: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderaoz38i/2/model.savedmodel\n",
      "2023-10-08 23:49:22.825656: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:22.825726: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderaoz38i/2/model.savedmodel\n",
      "2023-10-08 23:49:22.861184: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:22.943768: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderaoz38i/2/model.savedmodel\n",
      "2023-10-08 23:49:22.986103: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 170390 microseconds.\n",
      "W1008 23:49:22.986250 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05b' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:23.011720 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_1 (CPU device 0)\n",
      "2023-10-08 23:49:23.011857: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderzMHLyi/10/model.savedmodel\n",
      "2023-10-08 23:49:23.020949: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:23.021020: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderzMHLyi/10/model.savedmodel\n",
      "2023-10-08 23:49:23.055553: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:23.138714: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderzMHLyi/10/model.savedmodel\n",
      "2023-10-08 23:49:23.184479: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 172634 microseconds.\n",
      "I1008 23:49:23.184773 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05b (version 3)\n",
      "2023-10-08 23:49:23.185507: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderB414zi/3/model.savedmodel\n",
      "2023-10-08 23:49:23.192590: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:23.192665: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderB414zi/3/model.savedmodel\n",
      "2023-10-08 23:49:23.231798: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:23.313205: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderB414zi/3/model.savedmodel\n",
      "2023-10-08 23:49:23.356776: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 171274 microseconds.\n",
      "W1008 23:49:23.356956 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05b' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:23.378584 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05b_0 (CPU device 0)\n",
      "2023-10-08 23:49:23.378752: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderaoz38i/2/model.savedmodel\n",
      "2023-10-08 23:49:23.386309: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:23.386388: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderaoz38i/2/model.savedmodel\n",
      "2023-10-08 23:49:23.411427: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:23.491574: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderaoz38i/2/model.savedmodel\n",
      "2023-10-08 23:49:23.538317: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 159576 microseconds.\n",
      "I1008 23:49:23.538728 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05b_0 (CPU device 0)\n",
      "2023-10-08 23:49:23.538941: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderB414zi/3/model.savedmodel\n",
      "2023-10-08 23:49:23.547338: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:23.547401: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderB414zi/3/model.savedmodel\n",
      "2023-10-08 23:49:23.569196: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:23.638340: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderB414zi/3/model.savedmodel\n",
      "2023-10-08 23:49:23.680659: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 141726 microseconds.\n",
      "I1008 23:49:23.680936 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05b_1 (CPU device 0)\n",
      "2023-10-08 23:49:23.681222: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderaoz38i/2/model.savedmodel\n",
      "2023-10-08 23:49:23.689484: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:23.689550: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderaoz38i/2/model.savedmodel\n",
      "2023-10-08 23:49:23.727024: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:23.811309: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderaoz38i/2/model.savedmodel\n",
      "2023-10-08 23:49:23.852902: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 171690 microseconds.\n",
      "I1008 23:49:23.853214 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05b (version 4)\n",
      "2023-10-08 23:49:23.854154: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderphxExi/4/model.savedmodel\n",
      "2023-10-08 23:49:23.862080: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:23.862148: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderphxExi/4/model.savedmodel\n",
      "2023-10-08 23:49:23.893797: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:23.965837: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderphxExi/4/model.savedmodel\n",
      "2023-10-08 23:49:24.006197: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 152048 microseconds.\n",
      "W1008 23:49:24.006398 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05b' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:24.029056 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05b_1 (CPU device 0)\n",
      "2023-10-08 23:49:24.029202: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderB414zi/3/model.savedmodel\n",
      "2023-10-08 23:49:24.036272: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:24.036350: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderB414zi/3/model.savedmodel\n",
      "2023-10-08 23:49:24.070380: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:24.143183: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderB414zi/3/model.savedmodel\n",
      "2023-10-08 23:49:24.182576: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 153381 microseconds.\n",
      "I1008 23:49:24.182839 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05 (version 11)\n",
      "2023-10-08 23:49:24.183651: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderDzXkYf/11/model.savedmodel\n",
      "2023-10-08 23:49:24.190936: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:24.190998: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderDzXkYf/11/model.savedmodel\n",
      "2023-10-08 23:49:24.227170: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:24.303453: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderDzXkYf/11/model.savedmodel\n",
      "2023-10-08 23:49:24.343924: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 160279 microseconds.\n",
      "W1008 23:49:24.344076 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:24.367627 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05b_0 (CPU device 0)\n",
      "2023-10-08 23:49:24.367768: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderphxExi/4/model.savedmodel\n",
      "2023-10-08 23:49:24.375636: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:24.375717: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderphxExi/4/model.savedmodel\n",
      "2023-10-08 23:49:24.398434: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:24.463629: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderphxExi/4/model.savedmodel\n",
      "2023-10-08 23:49:24.502791: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 135031 microseconds.\n",
      "I1008 23:49:24.503076 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_0 (CPU device 0)\n",
      "2023-10-08 23:49:24.503253: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderDzXkYf/11/model.savedmodel\n",
      "2023-10-08 23:49:24.513071: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:24.513139: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderDzXkYf/11/model.savedmodel\n",
      "2023-10-08 23:49:24.534618: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:24.599888: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderDzXkYf/11/model.savedmodel\n",
      "2023-10-08 23:49:24.642534: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 139289 microseconds.\n",
      "I1008 23:49:24.642836 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05 (version 12)\n",
      "2023-10-08 23:49:24.643638: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderrmR8Qi/12/model.savedmodel\n",
      "2023-10-08 23:49:24.651433: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:24.651500: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderrmR8Qi/12/model.savedmodel\n",
      "2023-10-08 23:49:24.692170: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:24.769909: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderrmR8Qi/12/model.savedmodel\n",
      "2023-10-08 23:49:24.808006: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 164377 microseconds.\n",
      "W1008 23:49:24.808127 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:24.829774 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05b_1 (CPU device 0)\n",
      "2023-10-08 23:49:24.829909: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderphxExi/4/model.savedmodel\n",
      "2023-10-08 23:49:24.836893: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:24.836963: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderphxExi/4/model.savedmodel\n",
      "2023-10-08 23:49:24.871068: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:24.944892: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderphxExi/4/model.savedmodel\n",
      "2023-10-08 23:49:24.984228: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 154330 microseconds.\n",
      "I1008 23:49:24.984541 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_1 (CPU device 0)\n",
      "I1008 23:49:24.984839 1 model_lifecycle.cc:694] successfully loaded '05_05b' version 4\n",
      "I1008 23:49:24.984947 1 model_lifecycle.cc:694] successfully loaded '05_05b' version 4\n",
      "2023-10-08 23:49:24.984981: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderDzXkYf/11/model.savedmodel\n",
      "I1008 23:49:24.984993 1 model_lifecycle.cc:694] successfully loaded '05_05b' version 4\n",
      "2023-10-08 23:49:24.994331: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:24.994388: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderDzXkYf/11/model.savedmodel\n",
      "2023-10-08 23:49:25.036586: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:25.113839: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderDzXkYf/11/model.savedmodel\n",
      "2023-10-08 23:49:25.153968: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 168990 microseconds.\n",
      "I1008 23:49:25.154281 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_0 (CPU device 0)\n",
      "2023-10-08 23:49:25.154419: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderrmR8Qi/12/model.savedmodel\n",
      "2023-10-08 23:49:25.163704: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:25.163776: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderrmR8Qi/12/model.savedmodel\n",
      "2023-10-08 23:49:25.184431: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:25.253867: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderrmR8Qi/12/model.savedmodel\n",
      "2023-10-08 23:49:25.292589: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 138176 microseconds.\n",
      "I1008 23:49:25.292833 1 tensorflow.cc:2671] TRITONBACKEND_ModelInitialize: 05_05 (version 13)\n",
      "2023-10-08 23:49:25.293500: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderG2jBPi/13/model.savedmodel\n",
      "2023-10-08 23:49:25.301800: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:25.301857: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderG2jBPi/13/model.savedmodel\n",
      "2023-10-08 23:49:25.342425: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:25.419747: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderG2jBPi/13/model.savedmodel\n",
      "2023-10-08 23:49:25.460512: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 167021 microseconds.\n",
      "W1008 23:49:25.460633 1 tensorflow.cc:1431] autofilled max_batch_size to 4 for model '05_05' since batching is supporrted but no max_batch_size is specified in model configuration. Must specify max_batch_size to utilize autofill with a larger max batch size\n",
      "I1008 23:49:25.482092 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_1 (CPU device 0)\n",
      "2023-10-08 23:49:25.482229: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderrmR8Qi/12/model.savedmodel\n",
      "2023-10-08 23:49:25.489259: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:25.489350: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderrmR8Qi/12/model.savedmodel\n",
      "2023-10-08 23:49:25.532312: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:25.616582: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderrmR8Qi/12/model.savedmodel\n",
      "2023-10-08 23:49:25.658176: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 175956 microseconds.\n",
      "I1008 23:49:25.658456 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_0 (CPU device 0)\n",
      "2023-10-08 23:49:25.658573: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderG2jBPi/13/model.savedmodel\n",
      "2023-10-08 23:49:25.665407: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:25.665475: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderG2jBPi/13/model.savedmodel\n",
      "2023-10-08 23:49:25.686519: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:25.755101: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderG2jBPi/13/model.savedmodel\n",
      "2023-10-08 23:49:25.794457: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 135891 microseconds.\n",
      "I1008 23:49:25.795204 1 tensorflow.cc:2720] TRITONBACKEND_ModelInstanceInitialize: 05_05_1 (CPU device 0)\n",
      "2023-10-08 23:49:25.795309: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/folderG2jBPi/13/model.savedmodel\n",
      "2023-10-08 23:49:25.802240: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-10-08 23:49:25.802312: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/folderG2jBPi/13/model.savedmodel\n",
      "2023-10-08 23:49:25.845961: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-10-08 23:49:25.924951: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/folderG2jBPi/13/model.savedmodel\n",
      "2023-10-08 23:49:25.962185: I tensorflow/cc/saved_model/loader.cc:325] SavedModel load for tags { serve }; Status: success: OK. Took 166880 microseconds.\n",
      "I1008 23:49:25.963116 1 model_lifecycle.cc:694] successfully loaded '05_05' version 13\n",
      "I1008 23:49:25.963151 1 model_lifecycle.cc:694] successfully loaded '05_05' version 13\n",
      "I1008 23:49:25.963160 1 model_lifecycle.cc:694] successfully loaded '05_05' version 13\n",
      "I1008 23:49:25.963175 1 model_lifecycle.cc:694] successfully loaded '05_05' version 13\n",
      "I1008 23:49:25.963193 1 model_lifecycle.cc:694] successfully loaded '05_05' version 13\n",
      "I1008 23:49:25.963208 1 model_lifecycle.cc:694] successfully loaded '05_05' version 13\n",
      "I1008 23:49:25.963223 1 model_lifecycle.cc:694] successfully loaded '05_05' version 13\n",
      "I1008 23:49:25.963238 1 model_lifecycle.cc:694] successfully loaded '05_05' version 13\n",
      "I1008 23:49:25.963251 1 model_lifecycle.cc:694] successfully loaded '05_05' version 13\n",
      "I1008 23:49:25.963261 1 model_lifecycle.cc:694] successfully loaded '05_05' version 13\n",
      "I1008 23:49:25.963275 1 model_lifecycle.cc:694] successfully loaded '05_05' version 13\n",
      "I1008 23:49:25.963292 1 model_lifecycle.cc:694] successfully loaded '05_05' version 13\n",
      "I1008 23:49:25.997390 1 server.cc:583] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I1008 23:49:25.997487 1 server.cc:610] \n",
      "+------------+-------------------------------+-------------------------------+\n",
      "| Backend    | Path                          | Config                        |\n",
      "+------------+-------------------------------+-------------------------------+\n",
      "| tensorflow | /opt/tritonserver/backends/te | {\"cmdline\":{\"auto-complete-co |\n",
      "|            | nsorflow2/libtriton_tensorflo | nfig\":\"true\",\"min-compute-cap |\n",
      "|            | w2.so                         | ability\":\"6.000000\",\"backend- |\n",
      "|            |                               | directory\":\"/opt/tritonserver |\n",
      "|            |                               | /backends\",\"default-max-batch |\n",
      "|            |                               | -size\":\"4\"}}                  |\n",
      "|            |                               |                               |\n",
      "|            |                               |                               |\n",
      "+------------+-------------------------------+-------------------------------+\n",
      "\n",
      "I1008 23:49:25.998142 1 server.cc:653] \n",
      "+--------+---------+--------+\n",
      "| Model  | Version | Status |\n",
      "+--------+---------+--------+\n",
      "| 05_05  | 2       | READY  |\n",
      "| 05_05  | 3       | READY  |\n",
      "| 05_05  | 4       | READY  |\n",
      "| 05_05  | 5       | READY  |\n",
      "| 05_05  | 6       | READY  |\n",
      "| 05_05  | 7       | READY  |\n",
      "| 05_05  | 8       | READY  |\n",
      "| 05_05  | 9       | READY  |\n",
      "| 05_05  | 10      | READY  |\n",
      "| 05_05  | 11      | READY  |\n",
      "| 05_05  | 12      | READY  |\n",
      "| 05_05  | 13      | READY  |\n",
      "| 05_05a | 2       | READY  |\n",
      "| 05_05a | 3       | READY  |\n",
      "| 05_05a | 4       | READY  |\n",
      "| 05_05a | 5       | READY  |\n",
      "| 05_05a | 6       | READY  |\n",
      "| 05_05a | 7       | READY  |\n",
      "| 05_05a | 8       | READY  |\n",
      "| 05_05a | 9       | READY  |\n",
      "| 05_05a | 10      | READY  |\n",
      "| 05_05a | 11      | READY  |\n",
      "| 05_05a | 12      | READY  |\n",
      "| 05_05a | 13      | READY  |\n",
      "| 05_05a | 14      | READY  |\n",
      "| 05_05a | 15      | READY  |\n",
      "| 05_05a | 16      | READY  |\n",
      "| 05_05b | 2       | READY  |\n",
      "| 05_05b | 3       | READY  |\n",
      "| 05_05b | 4       | READY  |\n",
      "| 05_05c | 1       | READY  |\n",
      "| 05_05c | 2       | READY  |\n",
      "| 05_05c | 3       | READY  |\n",
      "| 05_05d | 1       | READY  |\n",
      "| 05_05d | 2       | READY  |\n",
      "| 05_05d | 3       | READY  |\n",
      "| 05_05e | 1       | READY  |\n",
      "| 05_05e | 2       | READY  |\n",
      "| 05_05e | 3       | READY  |\n",
      "| 05_05f | 1       | READY  |\n",
      "| 05_05f | 2       | READY  |\n",
      "| 05_05f | 3       | READY  |\n",
      "| 05_05f | 4       | READY  |\n",
      "| 05_05f | 5       | READY  |\n",
      "| 05_05f | 6       | READY  |\n",
      "| 05_05f | 8       | READY  |\n",
      "| 05_05f | 9       | READY  |\n",
      "| 05_05f | 10      | READY  |\n",
      "| 05_05f | 11      | READY  |\n",
      "| 05_05f | 12      | READY  |\n",
      "| 05_05f | 13      | READY  |\n",
      "| 05_05f | 14      | READY  |\n",
      "| 05_05f | 15      | READY  |\n",
      "| 05_05f | 16      | READY  |\n",
      "| 05_05f | 17      | READY  |\n",
      "| 05_05f | 18      | READY  |\n",
      "| 05_05f | 19      | READY  |\n",
      "| 05_05f | 20      | READY  |\n",
      "| 05_05f | 21      | READY  |\n",
      "| 05_05f | 22      | READY  |\n",
      "| 05_05f | 23      | READY  |\n",
      "| 05_05f | 24      | READY  |\n",
      "| 05_05f | 25      | READY  |\n",
      "| 05_05f | 26      | READY  |\n",
      "| 05_05f | 27      | READY  |\n",
      "| 05_05f | 28      | READY  |\n",
      "| 05_05f | 29      | READY  |\n",
      "| 05_05f | 30      | READY  |\n",
      "| 05_05f | 31      | READY  |\n",
      "| 05_05f | 32      | READY  |\n",
      "| 05_05f | 33      | READY  |\n",
      "| 05_05f | 34      | READY  |\n",
      "| 05_05f | 35      | READY  |\n",
      "| 05_05f | 36      | READY  |\n",
      "| 05_05f | 37      | READY  |\n",
      "| 05_05f | 38      | READY  |\n",
      "| 05_05f | 39      | READY  |\n",
      "| 05_05f | 40      | READY  |\n",
      "| 05_05f | 41      | READY  |\n",
      "| 05_05f | 42      | READY  |\n",
      "| 05_05f | 43      | READY  |\n",
      "| 05_05f | 44      | READY  |\n",
      "| 05_05f | 45      | READY  |\n",
      "| 05_05f | 46      | READY  |\n",
      "| 05_05f | 47      | READY  |\n",
      "| 05_05f | 48      | READY  |\n",
      "| 05_05f | 49      | READY  |\n",
      "| 05_05f | 50      | READY  |\n",
      "| 05_05f | 51      | READY  |\n",
      "| 05_05f | 52      | READY  |\n",
      "| 05_05f | 53      | READY  |\n",
      "| 05_05g | 1       | READY  |\n",
      "| 05_05g | 2       | READY  |\n",
      "| 05_05g | 3       | READY  |\n",
      "| 05_05h | 1       | READY  |\n",
      "| 05_05h | 2       | READY  |\n",
      "| 05_05h | 3       | READY  |\n",
      "| 05_05i | 1       | READY  |\n",
      "| 05_05i | 2       | READY  |\n",
      "| 05_05i | 3       | READY  |\n",
      "| 05_05i | 4       | READY  |\n",
      "+--------+---------+--------+\n",
      "\n",
      "I1008 23:49:25.998630 1 metrics.cc:640] Collecting CPU metrics\n",
      "I1008 23:49:25.999063 1 tritonserver.cc:2364] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.32.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data parameters statistics trace  |\n",
      "|                                  | logging                                  |\n",
      "| model_repository_path[0]         | gs://statmike-mlops-349915/05/triton/mod |\n",
      "|                                  | el_repo                                  |\n",
      "| model_control_mode               | MODE_NONE                                |\n",
      "| strict_model_config              | 0                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "| cache_enabled                    | 0                                        |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I1008 23:49:26.337505 1 grpc_server.cc:4977] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I1008 23:49:26.359687 1 http_server.cc:3518] Started HTTPService at 0.0.0.0:8000\n",
      "I1008 23:49:26.406918 1 http_server.cc:186] Started Metrics Service at 0.0.0.0:8002\n"
     ]
    }
   ],
   "source": [
    "def docker_runner():\n",
    "    !{command}\n",
    "\n",
    "def main():\n",
    "    p = multiprocessing.Process(target=docker_runner)\n",
    "    p.start()\n",
    "    return p\n",
    "    \n",
    "p = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bfaea4-1389-4f79-889a-c264f73b9dcb",
   "metadata": {},
   "source": [
    "### Check The Server Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "d01d52ae-7cb7-4e5c-8347-bd61bfa0d129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying ::1:8000...\n",
      "* Connected to localhost (::1) port 8000 (#0)\n",
      "> GET /v2/health/ready HTTP/1.1\n",
      "> Host: localhost:8000\n",
      "> User-Agent: curl/7.74.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Length: 0\n",
      "< Content-Type: text/plain\n",
      "< \n",
      "* Connection #0 to host localhost left intact\n"
     ]
    }
   ],
   "source": [
    "!curl -v localhost:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "572da9c8-1c97-4a6b-ba3b-53852b85ac56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200"
     ]
    }
   ],
   "source": [
    "!curl -s -o /dev/null -w \"%{http_code}\" http://localhost:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb7bafb-ca25-49f0-9874-83127a84cc3c",
   "metadata": {},
   "source": [
    "### Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "ae1daf72-2dc4-4973-b6ca-5f6d2f382c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<google.cloud.aiplatform.models.Model object at 0x7f9f784fb7f0> \n",
       " resource name: projects/1026793852137/locations/us-central1/models/model_05_05,\n",
       " [('2', 'gs://statmike-mlops-349915/05/05/models/20220927110007/model'),\n",
       "  ('3', 'gs://statmike-mlops-349915/05/05/models/20220927184222/model'),\n",
       "  ('4', 'gs://statmike-mlops-349915/05/05/models/20221023210622/model'),\n",
       "  ('5', 'gs://statmike-mlops-349915/05/05/models/20230209212046/model'),\n",
       "  ('6', 'gs://statmike-mlops-349915/05/05/models/20230210115433/model'),\n",
       "  ('7', 'gs://statmike-mlops-349915/05/05/models/20230308225745/model'),\n",
       "  ('8', 'gs://statmike-mlops-349915/05/05/models/20230324103811/model'),\n",
       "  ('9', 'gs://statmike-mlops-349915/05/05/models/20230324104933/model'),\n",
       "  ('10', 'gs://statmike-mlops-349915/05/05/models/20230325135459/model'),\n",
       "  ('11', 'gs://statmike-mlops-349915/05/05/models/20230325220538/model'),\n",
       "  ('12', 'gs://statmike-mlops-349915/05/05/models/20230327111418/model'),\n",
       "  ('13', 'gs://statmike-mlops-349915/05/05/models/20230327115749/model')])"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_artifacts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "ff8b4083-e67f-4d86-9b1b-cc5090e969ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    f'http://localhost:8000/v2/models/{models_artifacts[0][0].display_name}/versions/{models_artifacts[0][1][-1][0]}/infer',\n",
    "    data = json.dumps(instances),\n",
    "    headers = {\"content-type\": \"application/json; charset=utf-8\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "11e7450f-ab7e-45f2-ad37-d1b92941c0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': '05_05',\n",
       " 'model_version': '13',\n",
       " 'outputs': [{'name': 'logistic',\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [1, 2],\n",
       "   'data': [1.6803035407519928e-07, 0.9999998807907104]}]}"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = json.loads(response.text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "a971774e-c8f6-4eb6-8b96-254ce6e9ed51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class = np.argmax(result['outputs'][0]['data'])\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "93eb1f0d-3389-410d-9ae7-52f7486495f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999998807907104"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_proba = result['outputs'][0]['data'][predicted_class]\n",
    "predicted_class_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d6ce31-025c-4b4e-a561-883fb390cc1f",
   "metadata": {},
   "source": [
    "### Stop The Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "2708b6a8-42ee-4096-aa77-178883c4e6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.is_alive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "083c4ab4-8fa0-4a3a-a19c-8fbd11a95be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                                                                    COMMAND                  CREATED          STATUS          PORTS                                                           NAMES\n",
      "9e411609b03c   us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915/05_triton:23.03   \"/opt/nvidia/nvidia_…\"   19 minutes ago   Up 19 minutes   0.0.0.0:8000-8002->8000-8002/tcp, :::8000-8002->8000-8002/tcp   local_triton_server\n"
     ]
    }
   ],
   "source": [
    "!docker ps -f \"name=local_triton_server\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "992545ab-1d77-4fd0-b3b0-c3520f648670",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "8cdeaf76-ef58-4f07-ae8e-42549d26d314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.is_alive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "d1539b8c-c221-4166-9696-4b22e6739b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n"
     ]
    }
   ],
   "source": [
    "!docker ps -f \"name=local_triton_server\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "9db61324-a7e7-4ba1-82e9-f3a855b735f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, run this to stop the server\n",
    "#!docker stop local_triton_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc64443-ad0e-442c-b71b-1c193fe83bd5",
   "metadata": {},
   "source": [
    "---\n",
    "## Vertex AI Model Registry Entry For Triton Model Repository\n",
    "\n",
    "The NVIDIA Triton server model registry created above needs to be registred as a model in the Vertex AI Model Registry.  While the Triton server model registry could represent multiple models, versions of models, and ensembles, it represents a single model entity in the Vertex AI Model Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3f58eb-459b-4e76-b413-11bb7d6bc470",
   "metadata": {},
   "source": [
    "### Container Arguments For Vertex AI\n",
    "\n",
    "The TRITON Server has a series of command line arguments that include Vertex AI specific setting.  \n",
    "\n",
    "> Search the page at the following link to find \"VERTEX_AI\":\n",
    "> [TRITON Server Command Line Parser](https://github.com/triton-inference-server/server/blob/main/src/command_line_parser.cc)\n",
    "\n",
    "This notebook loads multiple models to TRITON server which may result in an error like the following on deployment to the endpoint:\n",
    "\n",
    "> `\"E0822 01:17:50.086439 1 main.cc:278] failed to start Vertex AI service: Invalid argument - Expect the model repository contains only a single model if default model is not specified\"`\n",
    "\n",
    "Multiple models can be used for inference but Vertex AI needs to know which is the default for when a model is not specified.  This is done using the `vertex-ai-default-model` flag.  See the prediction section for how to make prediction with the default model, and how to specify a specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "a55b0f75-af6e-4325-947b-2804d332fa3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--vertex-ai-default-model=05_05']"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_container_args = [\n",
    "    f\"--vertex-ai-default-model={models_artifacts[0][0].display_name}\"\n",
    "]\n",
    "serving_container_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e11148d-b554-440e-8aae-ec8ad07df64c",
   "metadata": {},
   "source": [
    "### Register In Vertex AI Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "6084ebbe-e2df-4d4f-bd44-305baccc23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "b94afa13-cbc0-4877-a10e-505d56df64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_NAME = f'run-{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15a16a3-1a98-4809-8231-79fd0835bccf",
   "metadata": {},
   "source": [
    "Check for existing version of the model in the model registry do one of the following:\n",
    "- Register as new model\n",
    "- Register as new version of existing model\n",
    "- Detect already registered model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "8b71993e-2e13-4aaa-93ab-906b28e5fda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Already in Registry:\n",
      "Loading model as new default version.\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/1026793852137/locations/us-central1/models/model_05_triton/operations/604599110284083200\n",
      "Model created. Resource name: projects/1026793852137/locations/us-central1/models/model_05_triton@17\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/1026793852137/locations/us-central1/models/model_05_triton@17')\n"
     ]
    }
   ],
   "source": [
    "modelmatch = aiplatform.Model.list(filter = f'display_name={SERIES}_{EXPERIMENT} AND labels.series={SERIES} AND labels.experiment={EXPERIMENT}')\n",
    "\n",
    "upload_model = True\n",
    "if modelmatch:\n",
    "    print(\"Model Already in Registry:\")\n",
    "    if RUN_NAME in modelmatch[0].version_aliases:\n",
    "        print(\"This version already loaded, no action taken.\")\n",
    "        upload_model = False\n",
    "        vertex_model = aiplatform.Model(model_name = modelmatch[0].resource_name)\n",
    "    else:\n",
    "        print('Loading model as new default version.')\n",
    "        parent_model = modelmatch[0].resource_name\n",
    "\n",
    "else:\n",
    "    print('This is a new model, creating in model registry')\n",
    "    parent_model = ''\n",
    "\n",
    "if upload_model:\n",
    "    vertex_model = aiplatform.Model.upload(\n",
    "        display_name = f'{SERIES}_{EXPERIMENT}',\n",
    "        model_id = f'model_{SERIES}_{EXPERIMENT}',\n",
    "        parent_model =  parent_model,\n",
    "        serving_container_image_uri = AR_IMAGE,\n",
    "        serving_container_args = serving_container_args,\n",
    "        artifact_uri = f\"gs://{BUCKET}/{SERIES}/{EXPERIMENT}/model_repo\",\n",
    "        is_default_version = True,\n",
    "        version_aliases = [RUN_NAME],\n",
    "        version_description = RUN_NAME,\n",
    "        labels = {'series' : f'{SERIES}', 'experiment' : f'{EXPERIMENT}'}        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "cb2722a5-23b2-481f-a392-f47dca50dc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_05_triton'"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertex_model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "4eaeae72-03dc-435b-b3f7-4ad7ca105d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17'"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertex_model.version_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "b8a4402a-957d-4716-850c-222c75e04ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review the model in the Vertex AI Model Registry:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/models/model_05_triton?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "print(f'Review the model in the Vertex AI Model Registry:\\nhttps://console.cloud.google.com/vertex-ai/locations/{REGION}/models/{vertex_model.name}?project={PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27badb06-ccf3-4508-85bd-a99c2e662957",
   "metadata": {},
   "source": [
    "---\n",
    "## Vertex AI Prediction Endpoint\n",
    "\n",
    "Create a prediction endpoint and deploy the model (version) from the Vertex AI model registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d70e8c-5758-436a-a227-f904b0749876",
   "metadata": {},
   "source": [
    "Create or detect existing endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "51028888-e3d6-456b-b42d-f3a58d5a98cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Exists: projects/1026793852137/locations/us-central1/endpoints/8971471723908038656\n",
      "Review the Endpoint in the Console:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/endpoints/8971471723908038656?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "endpoints = aiplatform.Endpoint.list(filter = f\"labels.series={SERIES} AND labels.experiment={EXPERIMENT}\")\n",
    "if endpoints:\n",
    "    endpoint = endpoints[0]\n",
    "    print(f\"Endpoint Exists: {endpoints[0].resource_name}\")\n",
    "else:\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name = f\"{SERIES}_{EXPERIMENT}\",\n",
    "        labels = {'series' : f\"{SERIES}\", 'experiment': f\"{EXPERIMENT}\"}    \n",
    "    )\n",
    "    print(f\"Endpoint Created: {endpoint.resource_name}\")\n",
    "    \n",
    "print(f'Review the Endpoint in the Console:\\nhttps://console.cloud.google.com/vertex-ai/locations/{REGION}/endpoints/{endpoint.name}?project={PROJECT_ID}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "03d5c74a-06ea-46ba-9f0b-27504c3a523f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05_triton'"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.display_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "9d9c8274-d443-4464-8211-0090d3704fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3686295251049250816': 100}"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.traffic_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "e434d55e-5790-4f47-9563-b7ebe66d3d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('05_triton', '9')]"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployed_models = endpoint.list_models()\n",
    "[(d.display_name, d.model_version_id) for d in deployed_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "f37d7372-690f-4a15-862a-66ab7a9e150e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1026793852137-compute@developer.gserviceaccount.com'"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "feeb3e2d-6367-4c96-a4f4-4af365b29cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROLE\n",
      "roles/bigquery.admin\n",
      "roles/owner\n",
      "roles/run.admin\n",
      "roles/storage.objectAdmin\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects get-iam-policy $PROJECT_ID --filter=\"bindings.members:$SERVICE_ACCOUNT\" --format='table(bindings.role)' --flatten=\"bindings[].members\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e03a6-5a1a-4db9-adc9-7c48dd5d17fb",
   "metadata": {},
   "source": [
    "### Deploy The Model To The Endpoint\n",
    "\n",
    "Note that the Vertex AI Model Registry has the information needed for the deployment:\n",
    "- URI of the serving container in Artifact Registry\n",
    "- URI of the model registry files in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "20a9100d-b6d1-489c-a42c-51ec597fab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model with 100% of traffic...\n",
      "Deploying Model projects/1026793852137/locations/us-central1/models/model_05_triton to Endpoint : projects/1026793852137/locations/us-central1/endpoints/8971471723908038656\n",
      "Deploy Endpoint model backing LRO: projects/1026793852137/locations/us-central1/endpoints/8971471723908038656/operations/8472387659300339712\n",
      "Endpoint model deployed. Resource name: projects/1026793852137/locations/us-central1/endpoints/8971471723908038656\n"
     ]
    }
   ],
   "source": [
    "if (vertex_model.display_name, vertex_model.version_id) not in [(d.display_name, d.model_version_id) for d in deployed_models]:\n",
    "    print(f'Deploying model with 100% of traffic...')\n",
    "    endpoint.deploy(\n",
    "        model = vertex_model,\n",
    "        deployed_model_display_name = vertex_model.display_name,\n",
    "        traffic_percentage = 100,\n",
    "        machine_type = 'n1-highmem-4',\n",
    "        min_replica_count = 1,\n",
    "        max_replica_count = 1,\n",
    "        #accelerator_type = 'NVIDIA_TESLA_T4',\n",
    "        accelerator_count = 0,\n",
    "        #service_account = SERVICE_ACCOUNT\n",
    "    )\n",
    "else:\n",
    "    print(f'Not deploying because model = {vertex_model.display_name} with version {vertex_model.version_id} is already on endpoint = {endpoint.display_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb82eeac-05e7-4b50-89ed-9cd6c888ce20",
   "metadata": {},
   "source": [
    "**NOTE On Memory Errors In Triton Container Logs**\n",
    "\n",
    "The model may fail to deploy.  In reviewing the logs an error related to shared memory in the environment like the one below may occur:\n",
    "\n",
    "> `E0824 10:49:33.972408 1 model_lifecycle.cc:597] failed to load 'postprocess' version 1: Internal: Unable to initialize shared memory key 'triton_python_backend_shm_region_4' to requested size (67108864 bytes). If you are running Triton inside docker, use '--shm-size' flag to control the shared memory region size. Each Python backend model instance requires at least 64MBs of shared memory. Error: No space left on device`\n",
    "\n",
    "With Vertex AI Endpoints you cannot provide a value for `--shm-size`.  If you contact support you can request the project to have an override for this error that changes the default shared memory size the a larger percentage of the nodes memory.  This does not necessarily mean chosing a different or larger node size configuration, rather, just allocating more of its memory to shared memory for the Triton Server processes to utilize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "056cefc6-01bd-4fc8-9b6a-0e42d2bb84e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4993465042893537280': 100}"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.traffic_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8435f-4218-4493-b187-9a6a87af45cb",
   "metadata": {},
   "source": [
    "#### Remove Deployed Models Without Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "aaed7a68-5618-416d-b314-2d4180a79214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/1026793852137/locations/us-central1/endpoints/8971471723908038656\n",
      "Undeploy Endpoint model backing LRO: projects/1026793852137/locations/us-central1/endpoints/8971471723908038656/operations/503831068621668352\n",
      "Endpoint model undeployed. Resource name: projects/1026793852137/locations/us-central1/endpoints/8971471723908038656\n",
      "Undeploying 05_triton with version 9 because it has no traffic.\n",
      "Model 05_triton with version 17 has traffic = 100\n"
     ]
    }
   ],
   "source": [
    "for deployed_model in endpoint.list_models():\n",
    "    if deployed_model.id in endpoint.traffic_split:\n",
    "        print(f\"Model {deployed_model.display_name} with version {deployed_model.model_version_id} has traffic = {endpoint.traffic_split[deployed_model.id]}\")\n",
    "    else:\n",
    "        endpoint.undeploy(deployed_model_id = deployed_model.id)\n",
    "        print(f\"Undeploying {deployed_model.display_name} with version {deployed_model.model_version_id} because it has no traffic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3bd242-a691-4d17-a228-cf69780cd918",
   "metadata": {},
   "source": [
    "---\n",
    "## Prediction\n",
    "\n",
    "Use prediction instances created before in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c5a2e-24e1-47b1-b2ca-d1af11b90c54",
   "metadata": {},
   "source": [
    "### Client For Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "6f5b1819-d826-4e21-84f2-d36c9d0c6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_options = {\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "8f3ae474-3c70-46ea-9b09-abda0ae7dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = aiplatform.gapic.PredictionServiceClient(client_options = client_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "a30ad3da-7f8f-4eb3-b94b-923e626cbe9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/1026793852137/locations/us-central1/endpoints/8971471723908038656'"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aea0e4-54b2-4394-8bb6-de58b214625d",
   "metadata": {},
   "source": [
    "### Make A Prediction Request: default model/version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "790ae8c8-1b43-40f8-8f7d-5b60c37394d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "http_body = httpbody_pb2.HttpBody(\n",
    "    data = json.dumps(instances).encode(\"utf-8\"),\n",
    "    content_type = \"application/json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "3370a9f2-6c57-4714-b465-e2bc2aec911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = aiplatform.gapic.RawPredictRequest(\n",
    "    endpoint = endpoint.resource_name,\n",
    "    http_body = http_body\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "d606b29f-c896-4fcd-9183-d3a3a2aa9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predictor.raw_predict(request = request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "72d78872-990b-4897-876e-ecd770cac477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content_type: \"application/json\"\n",
       "data: \"{\\\"model_name\\\":\\\"05_05\\\",\\\"model_version\\\":\\\"13\\\",\\\"outputs\\\":[{\\\"name\\\":\\\"logistic\\\",\\\"datatype\\\":\\\"FP32\\\",\\\"shape\\\":[1,2],\\\"data\\\":[1.6803035407519929e-7,0.9999998807907105]}]}\""
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "b9aabe80-6d25-4581-8a29-8677df20ed3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': '05_05',\n",
       " 'model_version': '13',\n",
       " 'outputs': [{'name': 'logistic',\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [1, 2],\n",
       "   'data': [1.6803035407519928e-07, 0.9999998807907104]}]}"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = json.loads(response.data)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "8de33d2c-38b7-4bb5-8ddb-2e8f9e66b5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class = np.argmax(result['outputs'][0]['data'])\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "eb444285-3983-4ecd-9a64-993ddd119d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999998807907104"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_proba = result['outputs'][0]['data'][predicted_class]\n",
    "predicted_class_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e101a3b-3894-4b40-8037-6fe4271adcbf",
   "metadata": {},
   "source": [
    "### Make A Prediction Request: specific model / latest version\n",
    "\n",
    "To make a request of a specific model and version, other than the default, the header value for `X-Vertex-Ai-Triton-Redirect` can be set to a model path on the Triton Inference Server.  To make this header modification the requests is made using the Python `requests` library where custom headers can be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "f825f80d-2560-41fd-a986-e6659ad58c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<google.cloud.aiplatform.models.Model object at 0x7f9f784fb7f0> \n",
       " resource name: projects/1026793852137/locations/us-central1/models/model_05_05,\n",
       " [('2', 'gs://statmike-mlops-349915/05/05/models/20220927110007/model'),\n",
       "  ('3', 'gs://statmike-mlops-349915/05/05/models/20220927184222/model'),\n",
       "  ('4', 'gs://statmike-mlops-349915/05/05/models/20221023210622/model'),\n",
       "  ('5', 'gs://statmike-mlops-349915/05/05/models/20230209212046/model'),\n",
       "  ('6', 'gs://statmike-mlops-349915/05/05/models/20230210115433/model'),\n",
       "  ('7', 'gs://statmike-mlops-349915/05/05/models/20230308225745/model'),\n",
       "  ('8', 'gs://statmike-mlops-349915/05/05/models/20230324103811/model'),\n",
       "  ('9', 'gs://statmike-mlops-349915/05/05/models/20230324104933/model'),\n",
       "  ('10', 'gs://statmike-mlops-349915/05/05/models/20230325135459/model'),\n",
       "  ('11', 'gs://statmike-mlops-349915/05/05/models/20230325220538/model'),\n",
       "  ('12', 'gs://statmike-mlops-349915/05/05/models/20230327111418/model'),\n",
       "  ('13', 'gs://statmike-mlops-349915/05/05/models/20230327115749/model')])"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_artifacts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "524860c1-1c6d-4261-a5bf-eddcfd54567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = !gcloud auth application-default print-access-token\n",
    "headers = {\n",
    "    \"content-type\": \"application/json; charset=utf-8\",\n",
    "    \"X-Vertex-Ai-Triton-Redirect\": f\"v2/models/{models_artifacts[0][0].display_name}/versions/{models_artifacts[0][1][-1][0]}/infer\",\n",
    "    \"Authorization\": f'Bearer {token[0]}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "963bd08d-e2e6-4a4d-b6fd-21bfb9f9ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    f'https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:rawPredict',\n",
    "    data = json.dumps(instances),\n",
    "    headers = headers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "65d9d407-b91e-418f-b1c2-180a901b513f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': '05_05',\n",
       " 'model_version': '13',\n",
       " 'outputs': [{'name': 'logistic',\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [1, 2],\n",
       "   'data': [1.6803035407519928e-07, 0.9999998807907104]}]}"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = json.loads(response.text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "3dba1206-745b-4465-9acc-683e4136a0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class = np.argmax(result['outputs'][0]['data'])\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "ad11529a-d818-4c5b-965c-8e72bbd5cefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999998807907104"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_proba = result['outputs'][0]['data'][predicted_class]\n",
    "predicted_class_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c923615-1453-480f-938b-8b66453a8593",
   "metadata": {},
   "source": [
    "### Make A Prediction Request: all models / latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "2ede897a-067f-48b8-a8a2-7d639e664971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('05_05@13', 1, 0.9999998807907104),\n",
       " ('05_05a@16', 1, 0.9999985694885254),\n",
       " ('05_05b@4', 1, 1.0),\n",
       " ('05_05c@3', 1, 0.9999970197677612),\n",
       " ('05_05d@3', 1, 1.0),\n",
       " ('05_05e@3', 1, 0.9999979734420776),\n",
       " ('05_05f@53', 1, 1.0),\n",
       " ('05_05g@3', 1, 1.0),\n",
       " ('05_05h@3', 1, 0.9987230896949768),\n",
       " ('05_05i@4', 1, 1.0)]"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for model in models_artifacts:\n",
    "    token = !gcloud auth application-default print-access-token\n",
    "    headers = {\n",
    "        \"content-type\": \"application/json; charset=utf-8\",\n",
    "        \"X-Vertex-Ai-Triton-Redirect\": f\"v2/models/{model[0].display_name}/versions/{model[1][-1][0]}/infer\",\n",
    "        \"Authorization\": f'Bearer {token[0]}'\n",
    "    }\n",
    "    response = requests.post(\n",
    "        f'https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:rawPredict',\n",
    "        data = json.dumps(instances),\n",
    "        headers = headers\n",
    "    )\n",
    "    result = json.loads(response.text)\n",
    "    predicted_class = np.argmax(result['outputs'][0]['data'])\n",
    "    results.append(\n",
    "        (\n",
    "            f\"{result['model_name']}@{result['model_version']}\",\n",
    "            predicted_class,\n",
    "            result['outputs'][0]['data'][predicted_class]\n",
    "        )\n",
    "    )\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1d07f5-2a0f-4889-bcff-e7079b4b27f4",
   "metadata": {},
   "source": [
    "### Make A Prediction Request: all models / all versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "8d26ea5e-93c5-4448-ad4f-790879ddcaca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<google.cloud.aiplatform.models.Model object at 0x7f9f784fb7f0> \n",
       " resource name: projects/1026793852137/locations/us-central1/models/model_05_05,\n",
       " [('2', 'gs://statmike-mlops-349915/05/05/models/20220927110007/model'),\n",
       "  ('3', 'gs://statmike-mlops-349915/05/05/models/20220927184222/model'),\n",
       "  ('4', 'gs://statmike-mlops-349915/05/05/models/20221023210622/model'),\n",
       "  ('5', 'gs://statmike-mlops-349915/05/05/models/20230209212046/model'),\n",
       "  ('6', 'gs://statmike-mlops-349915/05/05/models/20230210115433/model'),\n",
       "  ('7', 'gs://statmike-mlops-349915/05/05/models/20230308225745/model'),\n",
       "  ('8', 'gs://statmike-mlops-349915/05/05/models/20230324103811/model'),\n",
       "  ('9', 'gs://statmike-mlops-349915/05/05/models/20230324104933/model'),\n",
       "  ('10', 'gs://statmike-mlops-349915/05/05/models/20230325135459/model'),\n",
       "  ('11', 'gs://statmike-mlops-349915/05/05/models/20230325220538/model'),\n",
       "  ('12', 'gs://statmike-mlops-349915/05/05/models/20230327111418/model'),\n",
       "  ('13', 'gs://statmike-mlops-349915/05/05/models/20230327115749/model')])"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_artifacts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "e2468c4c-f740-4893-a95d-75283cf669e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('05_05@2', 1, 0.9997386336326599),\n",
       " ('05_05@3', 1, 0.9996007084846497),\n",
       " ('05_05@4', 1, 0.9997250437736511),\n",
       " ('05_05@5', 1, 0.9999992847442627),\n",
       " ('05_05@6', 1, 1.0),\n",
       " ('05_05@7', 1, 1.0),\n",
       " ('05_05@8', 1, 0.9999997615814209),\n",
       " ('05_05@9', 1, 0.9999983310699463),\n",
       " ('05_05@10', 1, 0.9999998807907104),\n",
       " ('05_05@11', 1, 0.9999986886978149),\n",
       " ('05_05@12', 1, 1.0),\n",
       " ('05_05@13', 1, 0.9999998807907104),\n",
       " ('05_05a@2', 1, 0.9989877343177795),\n",
       " ('05_05a@3', 1, 0.9999998807907104),\n",
       " ('05_05a@4', 1, 0.9999881982803345),\n",
       " ('05_05a@5', 1, 0.9999998807907104),\n",
       " ('05_05a@6', 1, 0.9999980926513672),\n",
       " ('05_05a@7', 1, 0.9999998807907104),\n",
       " ('05_05a@8', 1, 0.9999998807907104),\n",
       " ('05_05a@9', 1, 0.9999971389770508),\n",
       " ('05_05a@10', 1, 1.0),\n",
       " ('05_05a@11', 1, 1.0),\n",
       " ('05_05a@12', 1, 0.9999991655349731),\n",
       " ('05_05a@13', 1, 1.0),\n",
       " ('05_05a@14', 1, 1.0),\n",
       " ('05_05a@15', 1, 0.9999988079071045),\n",
       " ('05_05a@16', 1, 0.9999985694885254),\n",
       " ('05_05b@2', 1, 0.9929108023643494),\n",
       " ('05_05b@3', 1, 0.9999990463256836),\n",
       " ('05_05b@4', 1, 1.0),\n",
       " ('05_05c@1', 1, 0.9999518394470215),\n",
       " ('05_05c@2', 1, 0.9999997615814209),\n",
       " ('05_05c@3', 1, 0.9999970197677612),\n",
       " ('05_05d@1', 1, 0.992363452911377),\n",
       " ('05_05d@2', 1, 1.0),\n",
       " ('05_05d@3', 1, 1.0),\n",
       " ('05_05e@1', 1, 0.9999854564666748),\n",
       " ('05_05e@2', 1, 0.9999994039535522),\n",
       " ('05_05e@3', 1, 0.9999979734420776),\n",
       " ('05_05f@1', 1, 0.9998890161514282),\n",
       " ('05_05f@2', 1, 1.0),\n",
       " ('05_05f@3', 1, 0.9999978542327881),\n",
       " ('05_05f@4', 1, 0.9999995231628418),\n",
       " ('05_05f@5', 1, 1.0),\n",
       " ('05_05f@6', 1, 1.0),\n",
       " ('05_05f@8', 1, 1.0),\n",
       " ('05_05f@9', 1, 1.0),\n",
       " ('05_05f@10', 1, 0.9999841451644897),\n",
       " ('05_05f@11', 1, 1.0),\n",
       " ('05_05f@12', 1, 0.9999991655349731),\n",
       " ('05_05f@13', 1, 0.999998927116394),\n",
       " ('05_05f@14', 1, 0.9999953508377075),\n",
       " ('05_05f@15', 1, 1.0),\n",
       " ('05_05f@16', 1, 0.9999997615814209),\n",
       " ('05_05f@17', 1, 0.9999996423721313),\n",
       " ('05_05f@18', 1, 0.9999998807907104),\n",
       " ('05_05f@19', 1, 0.9999997615814209),\n",
       " ('05_05f@20', 1, 0.9999997615814209),\n",
       " ('05_05f@21', 1, 1.0),\n",
       " ('05_05f@22', 1, 1.0),\n",
       " ('05_05f@23', 1, 0.9999997615814209),\n",
       " ('05_05f@24', 1, 0.9999996423721313),\n",
       " ('05_05f@25', 1, 1.0),\n",
       " ('05_05f@26', 1, 0.9999995231628418),\n",
       " ('05_05f@27', 1, 1.0),\n",
       " ('05_05f@28', 1, 0.9999971389770508),\n",
       " ('05_05f@29', 1, 0.9999998807907104),\n",
       " ('05_05f@30', 1, 0.9999960660934448),\n",
       " ('05_05f@31', 1, 0.9999998807907104),\n",
       " ('05_05f@32', 1, 0.9999988079071045),\n",
       " ('05_05f@33', 1, 1.0),\n",
       " ('05_05f@34', 1, 1.0),\n",
       " ('05_05f@35', 1, 1.0),\n",
       " ('05_05f@36', 1, 0.9999990463256836),\n",
       " ('05_05f@37', 1, 0.9999912977218628),\n",
       " ('05_05f@38', 1, 0.9999998807907104),\n",
       " ('05_05f@39', 1, 0.9999911785125732),\n",
       " ('05_05f@40', 1, 0.9999889135360718),\n",
       " ('05_05f@41', 1, 0.9999979734420776),\n",
       " ('05_05f@42', 1, 0.9999995231628418),\n",
       " ('05_05f@43', 1, 0.9999964237213135),\n",
       " ('05_05f@44', 1, 1.0),\n",
       " ('05_05f@45', 1, 0.9999994039535522),\n",
       " ('05_05f@46', 1, 0.9999998807907104),\n",
       " ('05_05f@47', 1, 0.9999996423721313),\n",
       " ('05_05f@48', 1, 0.9999963045120239),\n",
       " ('05_05f@49', 1, 0.9999967813491821),\n",
       " ('05_05f@50', 1, 0.9999997615814209),\n",
       " ('05_05f@51', 1, 1.0),\n",
       " ('05_05f@52', 1, 0.9999998807907104),\n",
       " ('05_05f@53', 1, 1.0),\n",
       " ('05_05g@1', 1, 0.9999880790710449),\n",
       " ('05_05g@2', 1, 1.0),\n",
       " ('05_05g@3', 1, 1.0),\n",
       " ('05_05h@1', 1, 0.9998008608818054),\n",
       " ('05_05h@2', 1, 1.0),\n",
       " ('05_05h@3', 1, 0.9987230896949768),\n",
       " ('05_05i@1', 1, 0.9999951124191284),\n",
       " ('05_05i@2', 1, 1.0),\n",
       " ('05_05i@3', 1, 0.9999959468841553),\n",
       " ('05_05i@4', 1, 1.0)]"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for model in models_artifacts:\n",
    "    for version in model[1]:\n",
    "        token = !gcloud auth application-default print-access-token\n",
    "        headers = {\n",
    "            \"content-type\": \"application/json; charset=utf-8\",\n",
    "            \"X-Vertex-Ai-Triton-Redirect\": f\"v2/models/{model[0].display_name}/versions/{version[0]}/infer\",\n",
    "            \"Authorization\": f'Bearer {token[0]}'\n",
    "        }\n",
    "        response = requests.post(\n",
    "            f'https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:rawPredict',\n",
    "            data = json.dumps(instances),\n",
    "            headers = headers\n",
    "        )\n",
    "        result = json.loads(response.text)\n",
    "        if 'error' in result.keys():\n",
    "            results.append(\n",
    "                (\n",
    "                    f\"{model[0].display_name}@{version[0]}\",\n",
    "                    'Missing',\n",
    "                    'Missing'\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            predicted_class = np.argmax(result['outputs'][0]['data'])\n",
    "            results.append(\n",
    "                (\n",
    "                    f\"{result['model_name']}@{result['model_version']}\",\n",
    "                    predicted_class,\n",
    "                    result['outputs'][0]['data'][predicted_class]\n",
    "                )\n",
    "            )\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ccf99dbd-1f7d-485c-9b8c-17c6d7b972af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#endpoint.delete(force = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "163600ad-8d71-4abb-aa48-d595bb347434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vertex_model.delete()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

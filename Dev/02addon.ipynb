{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac78c44-356e-404d-808e-d5110f1deb64",
   "metadata": {},
   "source": [
    "--- \n",
    "# 02b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0523ffaa-9dfb-41cb-b7b5-6cc6ddae1613",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Evaluation\n",
    "While the model above was trained using AutoML with the API, it is still possible to review the evaluation metrics directly in the Google Cloud Console.  Just visit the Models section of Vertex AI service and select the model and it will present the evaluation metrics with many helpful visuals.\n",
    "\n",
    "It is also possible to retrieve the evaluation metrics for you model using the API.  This section shows how to use the API.\n",
    "\n",
    "<p><center><a href=\"https://youtu.be/0vhviqmH8Gg\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"architectures/thumbnails/playbutton/02b.png\" width=\"50%\"></a></center></p>\n",
    "<p><center>Part 2 Video</center></p>\n",
    "\n",
    "For more information review [this page](https://cloud.google.com/vertex-ai/docs/training/evaluating-automl-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106b740-1d3b-43b5-ac3d-311fb298b2bf",
   "metadata": {},
   "source": [
    "Setup a model client for the model create by this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a4f7b61b-ad0f-487b-8dc6-ac8ed12e6afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/715288179162/locations/us-central1/models/3283726660725112832'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "02bbdfaa-75dc-4482-8781-d73f0043aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client = aiplatform.gapic.ModelServiceClient(\n",
    "    client_options = {\n",
    "        'api_endpoint' : f'{REGION}-aiplatform.googleapis.com'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71227e04-8bb0-46d5-862e-3fddfe0361ae",
   "metadata": {},
   "source": [
    "Retrives the aggregate model evalution metrics for the model as a whole.  First, use `.list_model_evaluations` to retrieve the evaluation id, then use `.get_model_evaluation` for the evaluation id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "974b597e-f01d-4147-a824-9a6ac8246a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = model_client.list_model_evaluations(parent = model.resource_name)\n",
    "evals = iter(evaluations)\n",
    "eval_id = next(evals).name\n",
    "geteval = model_client.get_model_evaluation(name = eval_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aa9551-2e4a-4673-b0d5-1ddec1233e0e",
   "metadata": {},
   "source": [
    "Review several of the metrics include in the evaluation.  Also, compare these to the results in the console view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8dd6e04f-c0ec-4ad1-a8be-460cddcb0267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99965274"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geteval.metrics['auPrc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "edd9892b-eeac-4a64-aa9b-68a7a45a3e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label =  0  has Predicted labels =  [28366.0, 4.0]\n",
      "True Label =  1  has Predicted labels =  [15.0, 38.0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(geteval.metrics['confusionMatrix']['annotationSpecs'])):\n",
    "    print('True Label = ', geteval.metrics['confusionMatrix']['annotationSpecs'][i]['displayName'], ' has Predicted labels = ', geteval.metrics['confusionMatrix']['rows'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2266afe6-be3f-4565-afd9-cb839682df2c",
   "metadata": {},
   "source": [
    "For models with labels you can retrieve the evaluation metrics for each slice of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "045a6f0d-ab29-4825-a29e-335f5a1727e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = model_client.list_model_evaluation_slices(parent = eval_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4b7b5c5a-d43c-4c77-8806-fa92f6438e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  1 has auPrc =  0.8105681\n",
      "Label =  0 has auPrc =  0.9997379\n"
     ]
    }
   ],
   "source": [
    "for slice in slices:\n",
    "    print('Label = ', slice.slice_.value, 'has auPrc = ', slice.metrics['auPrc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef18c83-c330-4721-a845-620abdcf4c36",
   "metadata": {},
   "source": [
    "---\n",
    "# 02C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d4f1af-7d2d-4bf2-ae2d-5032b8f074b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Evaluation\n",
    "While the model above was trained using AutoML with the API, it is still possible to review the evaluation metrics directly in the Google Cloud Console.  Just visit the Models section of Vertex AI service and select the model and it will present the evaluation metrics with many helpful visuals.\n",
    "\n",
    "It is also possible to retrieve the evaluation metrics for you model using the API.  This section shows how to use the API.\n",
    "\n",
    "<p><center><a href=\"https://youtu.be/0vhviqmH8Gg\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"architectures/thumbnails/playbutton/02b.png\" width=\"50%\"></a></center></p>\n",
    "<p><center>Part 2 Video</center></p>\n",
    "\n",
    "For more information review [this page](https://cloud.google.com/vertex-ai/docs/training/evaluating-automl-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddbd470-f19f-42b3-84a9-9a8d7308142d",
   "metadata": {},
   "source": [
    "Setup a model client for the model create by this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c0837562-ee88-4dc2-a4cd-b70c99f93f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/715288179162/locations/us-central1/models/3283726660725112832'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cfcd6297-4962-4e3e-9626-4ac6c7d75f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client = aiplatform.gapic.ModelServiceClient(\n",
    "    client_options = {\n",
    "        'api_endpoint' : f'{REGION}-aiplatform.googleapis.com'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f792b33-09a6-4a34-8216-7a85743a6f2e",
   "metadata": {},
   "source": [
    "Retrives the aggregate model evalution metrics for the model as a whole.  First, use `.list_model_evaluations` to retrieve the evaluation id, then use `.get_model_evaluation` for the evaluation id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "764f1cbf-c2b4-4ef9-869a-004dba2d4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = model_client.list_model_evaluations(parent = model.resource_name)\n",
    "evals = iter(evaluations)\n",
    "eval_id = next(evals).name\n",
    "geteval = model_client.get_model_evaluation(name = eval_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea4963-456d-403c-9d39-b7f82d1f9b9b",
   "metadata": {},
   "source": [
    "Review several of the metrics include in the evaluation.  Also, compare these to the results in the console view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "19312af9-0e63-4100-a193-55cc1a687c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99965274"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geteval.metrics['auPrc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7d81931d-109a-404f-a0a7-deb53f646dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label =  0  has Predicted labels =  [28366.0, 4.0]\n",
      "True Label =  1  has Predicted labels =  [15.0, 38.0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(geteval.metrics['confusionMatrix']['annotationSpecs'])):\n",
    "    print('True Label = ', geteval.metrics['confusionMatrix']['annotationSpecs'][i]['displayName'], ' has Predicted labels = ', geteval.metrics['confusionMatrix']['rows'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914266c8-6585-4ef9-8ba8-1200df150d26",
   "metadata": {},
   "source": [
    "For models with labels you can retrieve the evaluation metrics for each slice of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "80633d3b-2ae7-40ab-a3f2-9b160e1bc9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = model_client.list_model_evaluation_slices(parent = eval_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4c39e17f-0eee-47eb-91d1-03e6955bed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label =  1 has auPrc =  0.8105681\n",
      "Label =  0 has auPrc =  0.9997379\n"
     ]
    }
   ],
   "source": [
    "for slice in slices:\n",
    "    print('Label = ', slice.slice_.value, 'has auPrc = ', slice.metrics['auPrc'])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11f5c7a1-37b6-4bae-8e4a-89b4d8bd7af7",
   "metadata": {},
   "source": [
    "# Python Client for Cloud Build\n",
    "\n",
    "Containers are helpful.\n",
    "\n",
    "> TL;DR\n",
    "> Use containers to bring together **software** and training **code** so that you can easily launch jobs on different **compute** with different **parameters** to simplify the operations of ML training.\n",
    "\n",
    "At the moment we train an ML model a lot of things come together to make it happen:\n",
    "- **compute** in running: CPUs, memory, networking, GPUs on one or more instances\n",
    "- **software** is running on the compute\n",
    "    - the required packages are installed with the software\n",
    "- training **code**/script is launched with the software\n",
    "- training **data** is read by the training code\n",
    "- **parameters** that the code uses to configure the training run\n",
    "\n",
    "It's tempting to develop in an IDE, like JupyterLab here, and then just make the VM behind it much larger.  This note book here is running in JupyterLab hosted on **compute** running **software** and is being used to author **code** that reads **data** using **parameters** set as Python variables.  One of the issues with that is that typing these words just cost `$$$$` and this instance might not be able to run this notebook 10 times in parallel with different **parameters**.  \n",
    "\n",
    "A better way?  Keep using an enviorment like this to develop our **code** and make sure it works. Just use smaller **compute** and **data** during this development process.  Then, launch a sepearate, managed job, that runs the full training.  How? What if we could instruct a service to take the list of inputs above and run a job and only charge for the compute used for the duration of training? That is exactly what Vertex AI Training is used for.  With that in mind it also helps scale the usefulness of training as a next step:\n",
    "- specify distributed training, pools of compute instances\n",
    "- manage hyperparameter tuning with multiple parallel training jobs focusing in on the right values for hyperparameters\n",
    "- run many training jobs at the same time without managing compute but also controling cost of this scale\n",
    "\n",
    "Vertex AI has a [list of provided pre-built training containers](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) for the most popular frameworks.  They are made available in multiple release versions of the frameworks and with/without [CUDA](https://developer.nvidia.com/cuda-toolkit) already configured and setup for GPU based training.\n",
    "\n",
    "For Vertex AI Training Custom Jobs you:\n",
    "- specify the **compute** to use in parameters or as worker pool specs\n",
    "- provide a URI for a container with the **software** to use\n",
    "- provide training **code** in one of three ways\n",
    "    - as a link to a Python script (file.py)\n",
    "    - as URI to GCS for a Python Source Distribution\n",
    "    - as a starting point to code already included on the container with the **software**\n",
    "- provide **data**\n",
    "    - as a **parameter** specifying the location the **code** can use to retrieve it\n",
    "    - or build the logic for connecting to the data source into the **code**\n",
    "\n",
    "If we learn the skill of building a derivative containers that packages our desired **software** and install additional packages while also holding a copy of our **code** and maybe even our **parameters** then this ML training jobs become very simple to incorporate in our workflow!\n",
    "\n",
    "That is this notebooks goal.\n",
    "\n",
    "---\n",
    "\n",
    "We will use [Cloud Build]() to construct containers.  We are here in a notebook so we probably prefer Python and for that reason we will use the Python Client for Cloud Build.  \n",
    "\n",
    "incorporate links to \n",
    "product\n",
    "APIs\n",
    "the python client for cloud\n",
    "the python api for build\n",
    "the doc for python build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c286fc-bb8d-4d5d-bae4-3ec7419c53ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11f5c7a1-37b6-4bae-8e4a-89b4d8bd7af7",
   "metadata": {},
   "source": [
    "# Python Custom Containers\n",
    "### IN ACTIVE DEVELOPMENT - not complete\n",
    "\n",
    "Containers are helpful.\n",
    "\n",
    "> TL;DR\n",
    "> Use containers to bring together **software** and training **code** so that you can easily launch jobs on different **compute** with different **parameters** to simplify the operations of ML training.\n",
    "\n",
    "At the point in our workflow where we train an ML model a lot of things come together to make it happen:\n",
    "- **compute** in running: CPUs, memory, networking, GPUs on one or more instances\n",
    "- **software** is running on the compute\n",
    "    - the required packages are installed with the software\n",
    "- training **code**/script is launched with the software\n",
    "- training **data** is read by the training code\n",
    "- **parameters** that the code uses to configure the training run\n",
    "\n",
    "It's tempting to develop in an IDE, like JupyterLab here, and then just make the VM behind it much larger.  The notebook here is running in JupyterLab hosted on **compute** running **software** and is being used to author **code** that reads **data** using **parameters** set as Python variables.  One of the issues with this is that typing these words just cost `$$$$` and this instance might not be able to run this notebook 10 times in parallel with different **parameters**.  \n",
    "\n",
    "A better way?  Keep using an enviorment like this to develop our **code** and make sure it works. Just use smaller **compute** and **data** during this development process.  Then, launch a sepearate, managed job, that runs the full training.  How? What if we could instruct a service to take the list of inputs above and run a job and only charge for the compute used during the duration of training? That is exactly what Vertex AI Training is used for.  With this in mind it also helps scale the usefulness of training as a next step:\n",
    "- specify distributed training, pools of compute instances\n",
    "- manage hyperparameter tuning with multiple parallel training jobs focusing in on the right values for hyperparameters\n",
    "- run many training jobs at the same time without managing compute but also controling the cost of scaling\n",
    "\n",
    "Vertex AI has a [list of provided pre-built training containers](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) for the most popular frameworks.  They are made available in multiple release versions of common frameworks both with and without [CUDA](https://developer.nvidia.com/cuda-toolkit) already configured and setup for GPU based training.\n",
    "\n",
    "For Vertex AI Training Custom Jobs you:\n",
    "- specify the **compute** to use as input parameters or as worker pool specs\n",
    "- provide a URI for a container with the **software** to use on each worker\n",
    "- provide training **code** in one of three ways\n",
    "    - as a link to a Python script (file.py)\n",
    "    - as URI to GCS for a Python Source Distribution\n",
    "    - as a starting point to code already included on the container with the **software**\n",
    "- provide **data**\n",
    "    - as a **parameter** specifying the location the **code** can use to read/retrieve it\n",
    "    - or build the logic for connecting to the data source into the **code**\n",
    "\n",
    "If we learn the skill of building a derivative containers that packages our desired **software** and installs additional packages while also holding a copy of our **code**, and maybe even our **parameters**, then this ML training job become very simple to incorporate in our workflow!\n",
    "\n",
    "That is this notebooks goal.\n",
    "\n",
    "---\n",
    "## Notes:\n",
    "\n",
    "**Prerequisite:**\n",
    "\n",
    "This notebook depends on ML training code prepared in multiple forms by the [Python Packages](./Python%20Packages.ipynb) notebook.  Please run that notebook first before running this notebook. \n",
    "\n",
    "**We will use [Cloud Build](https://cloud.google.com/build) to construct containers.**\n",
    "\n",
    "- [API Overview](https://cloud.google.com/build/docs/api)\n",
    "    - REST API, gcloud CLI, and Client Libraries for Go, Java, Node.js, and Python\n",
    "- [Python Client for Cloud Build API](https://github.com/googleapis/python-cloudbuild)\n",
    "- [Python Client Library Documentation](https://cloud.google.com/python/docs/reference/cloudbuild/latest)\n",
    "\n",
    "**We will store built containers in [Artifact Registry](https://cloud.google.com/artifact-registry).**\n",
    "\n",
    "- [API Overview](https://cloud.google.com/artifact-registry/docs/apis)\n",
    "- [Python Client for Artifact Registry API](https://github.com/googleapis/python-artifact-registry)\n",
    "- [Python Client Library Documentation](https://cloud.google.com/python/docs/reference/artifactregistry/latest)\n",
    "\n",
    "**Notes on Python and Google Cloud:**\n",
    "\n",
    "Google Cloud APIs can be used with the [Google Cloud Python Client](https://github.com/googleapis/google-cloud-python).  The client has [libraries](https://github.com/googleapis/google-cloud-python#libraries) for Google Cloud services.  The documentation for each library is centralized in the [Python Cloud Client Libraries](https://cloud.google.com/python/docs/reference) reference documentation.\n",
    "- Also helpful: [Getting started with Python](https://cloud.google.com/python/docs/getting-started) in Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1414d7-e68f-44b9-a331-4fb9a04e2538",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6084a455-7eb6-4173-ac0d-b908a5f34123",
   "metadata": {},
   "source": [
    "### Package Installs (if needed)\n",
    "\n",
    "This notebook uses the Python Clients for\n",
    "- Google Service Usage\n",
    "    - to enable APIs (Artifact Registry and Cloud Build)\n",
    "- Artifact Registry\n",
    "    - to create repositories for Python packages and Docker containers\n",
    "- Cloud Build\n",
    "    - To build custom Docker containers\n",
    "\n",
    "The cells below check to see if the required Python libraries are installed.  If any are not it will print a message to do the install with the associated pip command to use.  These installs must be completed before continuing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e517d857-e3fb-431a-a426-bcad424353d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.cloud.service_usage_v1\n",
    "except ImportError:\n",
    "    print('You need to pip install google-cloud-service-usage')\n",
    "    !pip install google-cloud-service-usage -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "18845f24-9ddf-4b64-9eb6-40c77783136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.cloud.artifactregistry_v1\n",
    "except ImportError:\n",
    "    print('You need to pip install google-cloud-artifact-registry')\n",
    "    !pip install google-cloud-artifact-registry -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2c0b4b36-b779-4d57-a4a3-bb87a862892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.cloud.devtools.cloudbuild\n",
    "except ImportError:\n",
    "    print('You need to pip install google-cloud-build')\n",
    "    !pip install google-cloud-build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa3440-12a7-4beb-a99f-d85c04a78d6a",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed111b6-43f9-49e7-9770-63d2359779b2",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "09680104-3052-4adb-b247-8ecaa4672a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statmike-mlops-349915'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b2a88a5e-0238-47ac-9e70-c761892c602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "EXPERIMENT = 'containers'\n",
    "SERIES = 'tips'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7510728-c1d5-4015-b692-9867e894daa1",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7629a781-519d-48b7-a135-812b31dea62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import pkg_resources\n",
    "from google.cloud import service_usage_v1\n",
    "from google.cloud.devtools import cloudbuild_v1\n",
    "from google.cloud import artifactregistry_v1\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94eba86-0904-47f4-b818-ac140346303f",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "31447726-a55a-4ef5-bd2d-2465215d22a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "su_client = service_usage_v1.ServiceUsageClient()\n",
    "ar_client = artifactregistry_v1.ArtifactRegistryClient()\n",
    "cb_client = cloudbuild_v1.CloudBuildClient()\n",
    "aiplatform.init(project = PROJECT_ID, location = REGION)\n",
    "gcs = storage.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a5cfbe-2353-4418-901d-fa291c408233",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f01a4cfe-8ade-4272-92a2-d78bc532cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = f'temp/{EXPERIMENT}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe1aaa9-983b-4ae8-a604-1f0993c7a159",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3d3912e8-7365-4bda-8105-88f8fa298524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIR exists?  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['job-parms', 'gcs', 'containers', 'multiprocess', 'packages']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove directory named DIR if exists\n",
    "shutil.rmtree(DIR, ignore_errors = True)\n",
    "\n",
    "# create directory DIR\n",
    "os.makedirs(DIR)\n",
    "\n",
    "# check for existance of DIR\n",
    "print('DIR exists? ', os.path.exists(DIR))\n",
    "\n",
    "# list contents of directory one level higher than DIR\n",
    "os.listdir(DIR + '/../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b67477-62b0-44ee-bfd5-c562d2622dbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Enable APIs\n",
    "\n",
    "Using Cloud Build and Artifact Registry requires enabling these APIs for the Google Cloud Project.\n",
    "\n",
    "Options for enabeling these.  In this notebook (2) is used.\n",
    " 1. Use the APIs & Services page in the console: https://console.cloud.google.com/apis\n",
    "     - `+ Enable APIs and Services`\n",
    "     - Search for Cloud Build and Enable\n",
    "     - Search for Artifact Registry and Enable\n",
    " 2. Use [Google Service Usage](https://cloud.google.com/service-usage/docs) API from Python\n",
    "     - [Python Client For Service Usage](https://github.com/googleapis/python-service-usage)\n",
    "     - [Python Client Library Documentation](https://cloud.google.com/python/docs/reference/serviceusage/latest)\n",
    "     \n",
    "The following code cells use the Service Usage Client to:\n",
    "- get the state of the service\n",
    "- if 'DISABLED':\n",
    "    - Try enabling the service and return the state after trying\n",
    "- if 'ENABLED' print the state for confirmation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c486d8c-e051-4412-9da3-9cbcd3d8c786",
   "metadata": {},
   "source": [
    "### Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "139c385c-4b8b-46d7-8adc-9cad30562385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact Registry already enabled for project: statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "artifactregistry = su_client.get_service(\n",
    "    request = service_usage_v1.GetServiceRequest(\n",
    "        name = f'projects/{PROJECT_ID}/services/artifactregistry.googleapis.com'\n",
    "    )\n",
    ").state.name\n",
    "\n",
    "\n",
    "if artifactregistry == 'DISABLED':\n",
    "    print(f'Artifact Registry is currently {artifactregistry} for project: {PROJECT_ID}')\n",
    "    print(f'Trying to Enable...')\n",
    "    operation = su_client.enable_service(\n",
    "        request = service_usage_v1.EnableServiceRequest(\n",
    "            name = f'projects/{PROJECT_ID}/services/artifactregistry.googleapis.com'\n",
    "        )\n",
    "    )\n",
    "    response = operation.result()\n",
    "    if response.service.state.name == 'ENABLED':\n",
    "        print(f'Artifact Registry is now enabled for project: {PROJECT_ID}')\n",
    "    else:\n",
    "        print(response)\n",
    "else:\n",
    "    print(f'Artifact Registry already enabled for project: {PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416ccf7-8817-486f-a42d-09769e087fa0",
   "metadata": {},
   "source": [
    "### Cloud Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c1c4464c-0ede-4961-9759-21a2b3e11f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloud Build already enabled for project: statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "cloudbuild = su_client.get_service(\n",
    "    request = service_usage_v1.GetServiceRequest(\n",
    "        name = f'projects/{PROJECT_ID}/services/cloudbuild.googleapis.com'\n",
    "    )\n",
    ").state.name\n",
    "\n",
    "\n",
    "if cloudbuild == 'DISABLED':\n",
    "    print(f'Cloud Build is currently {cloudbuild} for project: {PROJECT_ID}')\n",
    "    print(f'Trying to Enable...')\n",
    "    operation = su_client.enable_service(\n",
    "        request = service_usage_v1.EnableServiceRequest(\n",
    "            name = f'projects/{PROJECT_ID}/services/cloudbuild.googleapis.com'\n",
    "        )\n",
    "    )\n",
    "    response = operation.result()\n",
    "    if response.service.state.name == 'ENABLED':\n",
    "        print(f'Cloud Build is now enabled for project: {PROJECT_ID}')\n",
    "    else:\n",
    "        print(response)\n",
    "else:\n",
    "    print(f'Cloud Build already enabled for project: {PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d021aab2-f348-432a-be9a-648428132ed7",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup Artifact Registry\n",
    "\n",
    "Artifact registry organizes artifacts with repositories.  Each repository contains packages and is designated to hold a partifcular format of package: Docker images, Python Packages and [others](https://cloud.google.com/artifact-registry/docs/supported-formats#package)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e64b5-e813-4f51-add1-63292d5f227e",
   "metadata": {},
   "source": [
    "### List Repositories\n",
    "\n",
    "This may be empty if no repositories have been created for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "37d928c9-4968-4e8a-84f5-2c0e1cd67cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915\n",
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-python\n"
     ]
    }
   ],
   "source": [
    "for repo in ar_client.list_repositories(parent = f'projects/{PROJECT_ID}/locations/{REGION}'):\n",
    "    print(repo.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e69c2a28-2c2d-4499-aa88-387f4ac5c7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-python\"\n",
       "format_: PYTHON\n",
       "description: \"A repository for the statmike-mlops-349915 experiment that holds Python Packages.\"\n",
       "labels {\n",
       "  key: \"experiment\"\n",
       "  value: \"packages\"\n",
       "}\n",
       "labels {\n",
       "  key: \"series\"\n",
       "  value: \"tips\"\n",
       "}\n",
       "create_time {\n",
       "  seconds: 1663696036\n",
       "  nanos: 276742000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1663768642\n",
       "  nanos: 771288000\n",
       "}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e12ea93-29e5-4b97-a8ae-6758006db523",
   "metadata": {},
   "source": [
    "### Create Docker Image Repository\n",
    "\n",
    "Create an Artifact Registry Repository to hold Docker Images crated by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "00dbbac5-ab76-4bfc-83ec-71a4d7e0ca80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Repository ...\n",
      "Completed creating repo: projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-docker\n"
     ]
    }
   ],
   "source": [
    "docker_repo = None\n",
    "for repo in ar_client.list_repositories(parent = f'projects/{PROJECT_ID}/locations/{REGION}'):\n",
    "    if f'{PROJECT_ID}-docker' in repo.name:\n",
    "        docker_repo = repo\n",
    "        print(f'Retrieved existing repo: {docker_repo.name}')\n",
    "\n",
    "if not docker_repo:\n",
    "    operation = ar_client.create_repository(\n",
    "        request = artifactregistry_v1.CreateRepositoryRequest(\n",
    "            parent = f'projects/{PROJECT_ID}/locations/{REGION}',\n",
    "            repository_id = f'{PROJECT_ID}-docker',\n",
    "            repository = artifactregistry_v1.Repository(\n",
    "                description = f'A repository for the {EXPERIMENT} experiment that holds docker images.',\n",
    "                name = f'{PROJECT_ID}-docker',\n",
    "                format_ = artifactregistry_v1.Repository.Format.DOCKER,\n",
    "                labels = {'series': SERIES, 'experiment': EXPERIMENT}\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print('Creating Repository ...')\n",
    "    docker_repo = operation.result()\n",
    "    print(f'Completed creating repo: {docker_repo.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9d29aa6f-ff91-473a-97f3-b7c04ce0c9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-docker',\n",
       " 'DOCKER')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docker_repo.name, docker_repo.format_.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f3e6f-d5b5-4bdf-bd3d-85f93c0fecac",
   "metadata": {},
   "source": [
    "### Create Python Package Repository\n",
    "\n",
    "Create an Artifact Registry Repository to hold Python Packages created by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2739bb70-6bb3-4c86-bd4e-6930b9217560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved existing repo: projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-python\n"
     ]
    }
   ],
   "source": [
    "python_repo = None\n",
    "for repo in ar_client.list_repositories(parent = f'projects/{PROJECT_ID}/locations/{REGION}'):\n",
    "    if f'{PROJECT_ID}-python' in repo.name:\n",
    "        python_repo = repo\n",
    "        print(f'Retrieved existing repo: {python_repo.name}')\n",
    "\n",
    "if not python_repo:\n",
    "    operation = ar_client.create_repository(\n",
    "        request = artifactregistry_v1.CreateRepositoryRequest(\n",
    "            parent = f'projects/{PROJECT_ID}/locations/{REGION}',\n",
    "            repository_id = f'{PROJECT_ID}-python',\n",
    "            repository = artifactregistry_v1.Repository(\n",
    "                description = f'A repository for the {PROJECT_ID} experiment that holds Python Packages.',\n",
    "                name = f'{PROJECT_ID}-python',\n",
    "                format_ = artifactregistry_v1.Repository.Format.PYTHON,\n",
    "                labels = {'series': SERIES, 'experiment': EXPERIMENT}\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print('Creating Repository ...')\n",
    "    python_repo = operation.result()\n",
    "    print(f'Completed creating repo: {python_repo.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "36ab9f9d-7a2a-4f48-b3d2-c279f93247a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-python',\n",
       " 'PYTHON')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_repo.name, python_repo.format_.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f1519-0d36-48d0-822a-78084b5d1b5d",
   "metadata": {},
   "source": [
    "### List Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "814c8ba4-556f-41be-ad24-4aebf360f983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915\n",
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-docker\n",
      "projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-python\n"
     ]
    }
   ],
   "source": [
    "for repo in ar_client.list_repositories(parent = f'projects/{PROJECT_ID}/locations/{REGION}'):\n",
    "    print(repo.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5382175c-d5a3-46d2-90d8-0c4b2e282676",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Code\n",
    "\n",
    "ML Training code can take the form of a single script, a folder of scripts/modules, a Python Package distribution file. These could be located on the local disk, in GCS buckets, GitHub repository, or in a repository like Artifact Registry.  This notebook will explore many workflows for getting training code in all these forms and locations into a custom container.  \n",
    "\n",
    "The example training code being used here is from the [05 - TensorFlow](../05%20-%20TensorFlow/readme.md) series has a model training file named [05_train.py](../05%20-%20TensorFlow/05_train.py).\n",
    "\n",
    "These training code has already been prepared as local files, GCS hosted files, GitHub hosted files, and the source distribution is stored in Artifact Registry as a Python Package.\n",
    "\n",
    "**Please pause here and review and run the tip in the [Python Packages](./Python%20Packages.ipynb) notebook first to prepare these versions for use in this notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a55f85e-f262-4ae3-a588-c52166a8c67f",
   "metadata": {},
   "source": [
    "### Local: Files/Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b89d9935-abd6-4f9d-9037-36cbb8a86f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code/tips_trainer/pyproject.toml\n",
      "code/tips_trainer/.ipynb_checkpoints/pyproject-checkpoint.toml\n",
      "code/tips_trainer/src/tips_trainer/__init__.py\n",
      "code/tips_trainer/src/tips_trainer/train.py\n",
      "code/tips_trainer/src/tips_trainer.egg-info/top_level.txt\n",
      "code/tips_trainer/src/tips_trainer.egg-info/SOURCES.txt\n",
      "code/tips_trainer/src/tips_trainer.egg-info/requires.txt\n",
      "code/tips_trainer/src/tips_trainer.egg-info/dependency_links.txt\n",
      "code/tips_trainer/src/tips_trainer.egg-info/PKG-INFO\n",
      "code/tips_trainer/dist/tips_trainer-0.1.tar.gz\n",
      "code/tips_trainer/dist/tips_trainer-0.1-py3-none-any.whl\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk('code'):\n",
    "    for f in files:\n",
    "        print(os.path.join(root, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dded355-fcbd-4b88-9986-8608a07f0f17",
   "metadata": {},
   "source": [
    "### GCS Bucket: Files/Folders/Source Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9e58e8bd-2b54-4053-828a-6c43d8bba473",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = gcs.lookup_bucket(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "185a89d3-6d27-4509-8523-fa64c2ad6662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tips/code/tips_trainer/.ipynb_checkpoints/pyproject-checkpoint.toml\n",
      "tips/code/tips_trainer/dist/tips_trainer-0.1-py3-none-any.whl\n",
      "tips/code/tips_trainer/dist/tips_trainer-0.1.tar.gz\n",
      "tips/code/tips_trainer/pyproject.toml\n",
      "tips/code/tips_trainer/src/tips_trainer.egg-info/PKG-INFO\n",
      "tips/code/tips_trainer/src/tips_trainer.egg-info/SOURCES.txt\n",
      "tips/code/tips_trainer/src/tips_trainer.egg-info/dependency_links.txt\n",
      "tips/code/tips_trainer/src/tips_trainer.egg-info/requires.txt\n",
      "tips/code/tips_trainer/src/tips_trainer.egg-info/top_level.txt\n",
      "tips/code/tips_trainer/src/tips_trainer/__init__.py\n",
      "tips/code/tips_trainer/src/tips_trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "for blob in list(bucket.list_blobs(prefix = f'{SERIES}/code')):\n",
    "    print(blob.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8711345-fdff-4ffc-9425-6370a88a8d13",
   "metadata": {},
   "source": [
    "### Artifact Registry: Python Package Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8ac4fec8-8525-49cd-9fac-56d8bcafe599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListPackagesPager<packages {\n",
       "  name: \"projects/statmike-mlops-349915/locations/us-central1/repositories/statmike-mlops-349915-python/packages/tips-trainer\"\n",
       "  create_time {\n",
       "    seconds: 1663768642\n",
       "    nanos: 502115000\n",
       "  }\n",
       "  update_time {\n",
       "    seconds: 1663768642\n",
       "    nanos: 771288000\n",
       "  }\n",
       "}\n",
       ">"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_client.list_packages(\n",
    "    parent = python_repo.name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4920042-fdaa-4741-8bff-1e9eee01b20b",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating a Custom Container with Cloud Build\n",
    "\n",
    "Cloud Build creates and manages the build on GCP.  The API creates a build by providing:\n",
    "- location of the source\n",
    "- instructions\n",
    "- location to store the built artifacts\n",
    "\n",
    "The instruction part of Cloud Build has options:\n",
    "- Dockerfile\n",
    "- Build Config file (YAML or JSON)\n",
    "- Cloud Native Buildpacks\n",
    "\n",
    "This notebook uses the approach of using the Python Client for Cloud Build and not referencing any local files.  For that reason, the first step is creating a Dockerfile for the workflow and storing it in GCS. The next step is running Cloud Build and using the client to specify the Build config rather than a config file.  The steps of the build config start with getting the code (git clone, or copy from GCS) and copying the Dockerfile).  \n",
    "\n",
    "Other options:\n",
    "If you store the Dockerfile(s) in repository folder then you could go as far as having GitHub trigger a build on commit.  It could also simply the process below by not needing the extra steps of storing the Dockerfile separately and copying it before building.  \n",
    "\n",
    "There are multiple stand-alone subsections here that illustrate different workflows for building a custom container depending on where and how the training code is stored.\n",
    "\n",
    "- [Workflow 1: copy script to container](#workflow1)\n",
    "- [Workflow 2: copy folder to container](#workflow2)\n",
    "- [Workflow 3: copy package to container](#workflow3)\n",
    "- [Workflow 4: pip install package from GCS to container](#workflow4)\n",
    "- [Workflow 5: pip install package from GitHub to container](#workflow5)\n",
    "- [Workflow 6: pip install package from Artifact Registry to container](#workflow6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6a417-1c37-424d-a020-4c2c0b6adebb",
   "metadata": {},
   "source": [
    "### Common Parameters for All Workflows\n",
    "\n",
    "Choose a Vertex AI Pre-Built container for ML Training from the list [here](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) and store in in the `TRAIN_IMAGE` parameter below.  Also, create a parameter the references the Artifact Registry custom repository for Docker images created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "29afa08b-22a2-48d8-9444-effe1df00789",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-7:latest'\n",
    "REPOSITORY = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{docker_repo.name.split('/')[-1]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d540b389-563b-48e4-a7aa-db8c040dcee1",
   "metadata": {},
   "source": [
    "---\n",
    "### Workflow 1: copy script to container\n",
    "<a id = 'workflow1'></a>\n",
    "\n",
    "This workflow builds a custom container using a Vertex AI Pre-Built container for traning as the base.  The code below stores the `requirements.txt`, `Dockerfile` and training script in the project GCS bucket and then launches a Cloud Build job that copies these and runs the Docker build process with Cloud Build.\n",
    "\n",
    "To see an an example Vertex AI Training Custom Job running with the custom container created here go to [Python Training - workflow 1](./Python%20Training.ipynb#workflow1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "60ef34f2-6329-4ba0-b52a-11fe3dd1c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKFLOW = 'workflow_1' # name for this workflow\n",
    "CONTAINER = f'tips_trainer_{WORKFLOW}' # name for custom container\n",
    "SOURCEPATH = f'{SERIES}/{EXPERIMENT}/{WORKFLOW}' # in gcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde31c46-41d3-492e-902c-d4ec2047a6fd",
   "metadata": {},
   "source": [
    "#### Source Code\n",
    "A copy of just the training code script that will be copied into the custom container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "d4aee090-2f63-46bf-8fc6-3069d1161403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Blob: statmike-mlops-349915, tips/containers/workflow_1/train.py, 1663861926053053>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket.copy_blob(\n",
    "    blob = bucket.get_blob(blob_name = f'{SERIES}/code/tips_trainer/src/tips_trainer/train.py'),\n",
    "    destination_bucket = bucket,\n",
    "    new_name = f'{SOURCEPATH}/train.py'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c576dfd-b8bc-4fc2-8c44-f38107c543a3",
   "metadata": {},
   "source": [
    "#### Requirements.txt\n",
    "A list of requirments for the training code to run.  These packages will be added/updated on the pre-built container when the `Dockerfile` instructions runs `pip install ...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "cc560708-68f1-4f18-96ac-895f716f8800",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = f\"\"\"\n",
    "tensorflow_io\n",
    "google-cloud-aiplatform>={aiplatform.__version__}\n",
    "protobuf=={pkg_resources.get_distribution('protobuf').version}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "7f31f127-b618-4005-9459-26161c63045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = bucket.blob(f'{SOURCEPATH}/requirements.txt')\n",
    "blob.upload_from_string(requirements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929797c9-034c-40e9-b079-1c2a3b1c2378",
   "metadata": {},
   "source": [
    "#### Dockerfile\n",
    "The Docker build instructions.  This copies the `requirements.txt` and training script into the containers, `pip installs ...` the requirements, sets the entrypoint for the container to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "57be688e-132e-4100-8b2b-9a4061ff68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile = f\"\"\"\n",
    "FROM {TRAIN_IMAGE}\n",
    "WORKDIR /training\n",
    "# copy requirements and install them\n",
    "COPY requirements.txt ./\n",
    "RUN pip install --no-cache-dir --upgrade pip \\\n",
    "  && pip install --no-cache-dir -r requirements.txt\n",
    "## Copies the trainer code to the docker image\n",
    "COPY train.py .\n",
    "## Sets up the entry point to invoke the trainer\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b801ea8c-1b58-40d2-af00-a14be730edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = bucket.blob(f'{SOURCEPATH}/Dockerfile')\n",
    "blob.upload_from_string(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2ba39d-cfd4-4636-a6dd-0837fe7f3972",
   "metadata": {},
   "source": [
    "#### Build Custom Container\n",
    "Use the Cloud Build client to construct and run the build instructions.  Here the files collected in GCS are copied to the build instance, then the Docker build in run in the folder with the `Dockerfile`.  The resulting image is pushed to Artifact Registry (setup above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "05a0ff3a-dd11-4aee-8e74-2619484326f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the build config with empty list of steps - these will be added sequentially\n",
    "build = cloudbuild_v1.Build(\n",
    "    steps = []\n",
    ")\n",
    "# retrieve the source\n",
    "build.steps.append(\n",
    "    {\n",
    "        'name': 'gcr.io/cloud-builders/gsutil',\n",
    "        'args': ['cp', '-r', f'gs://{PROJECT_ID}/{SOURCEPATH}/*', '/workspace']\n",
    "    }\n",
    ")\n",
    "# docker build\n",
    "build.steps.append(\n",
    "    {\n",
    "        'name': 'gcr.io/cloud-builders/docker',\n",
    "        'args': ['build', '-t', f'{REPOSITORY}/{CONTAINER}', '/workspace']\n",
    "    }    \n",
    ")\n",
    "# docker push\n",
    "build.images = [f\"{REPOSITORY}/tips_trainer_{WORKFLOW}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "d671c828-6c1c-4a79-ad58-0c312b6fba01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "steps {\n",
       "  name: \"gcr.io/cloud-builders/gsutil\"\n",
       "  args: \"cp\"\n",
       "  args: \"-r\"\n",
       "  args: \"gs://statmike-mlops-349915/tips/containers/workflow_1/*\"\n",
       "  args: \"/workspace\"\n",
       "}\n",
       "steps {\n",
       "  name: \"gcr.io/cloud-builders/docker\"\n",
       "  args: \"build\"\n",
       "  args: \"-t\"\n",
       "  args: \"us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915-docker/tips_trainer_workflow_1\"\n",
       "  args: \"/workspace\"\n",
       "}\n",
       "images: \"us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915-docker/tips_trainer_workflow_1\""
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "f8aa3013-348c-42c9-8a1e-a9d0bffb7202",
   "metadata": {},
   "outputs": [],
   "source": [
    "operation = cb_client.create_build(\n",
    "    project_id = PROJECT_ID,\n",
    "    build = build\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c0186c43-3bd9-4ec2-a097-cc224f352d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Status.SUCCESS: 3>,\n",
       " images: \"us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915-docker/tips_trainer_workflow_1\")"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = operation.result()\n",
    "response.status, response.artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "eec92e2b-cddd-4fec-b348-3af3446377e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review the Custom Container with Artifact Registry in the Google Cloud Console:\n",
      "https://console.cloud.google.com/artifacts/docker/statmike-mlops-349915/us-central1/statmike-mlops-349915-docker?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "print(f\"Review the Custom Container with Artifact Registry in the Google Cloud Console:\\nhttps://console.cloud.google.com/artifacts/docker/{PROJECT_ID}/{REGION}/{PROJECT_ID}-docker?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f2640-bdd3-4317-9bc2-7dbd95beaf37",
   "metadata": {},
   "source": [
    "### Workflow 2: copy folder to container\n",
    "<a id = 'workflow2'></a>\n",
    "\n",
    "This workflow builds a custom container using a Vertex AI Pre-Built container for traning as the base.  The code below stores the `requirements.txt`, `Dockerfile` and training script folder in the project GCS bucket and then launches a Cloud Build job that copies these and runs the Docker build process with Cloud Build.\n",
    "\n",
    "To see an an example Vertex AI Training Custom Job running with the custom container created here go to [Python Training - workflow 2](./Python%20Training.ipynb#workflow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "80da5501-6fe8-4c9b-a9f7-03daddd7ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKFLOW = 'workflow_2' # name for this workflow\n",
    "CONTAINER = f'tips_trainer_{WORKFLOW}' # name for custom container\n",
    "SOURCEPATH = f'{SERIES}/{EXPERIMENT}/{WORKFLOW}' # in gcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f20b664-369c-400c-9ce1-567f395130f0",
   "metadata": {},
   "source": [
    "#### Source Code\n",
    "A copy of folder of training code that will be copied into the custom container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "71210fa2-3265-458c-a1c5-ffd3c32c7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for blob in list(bucket.list_blobs(prefix = f'{SERIES}/code/tips_trainer/src/tips_trainer/')):\n",
    "    foldername = '/'.join(blob.name.split('/')[-2:])\n",
    "    bucket.copy_blob(\n",
    "        blob = blob,\n",
    "        destination_bucket = bucket,\n",
    "        new_name = f\"{SOURCEPATH}/{foldername}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "05e7cd3e-6a95-4ed8-bb03-6734b5460529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tips/containers/workflow_2/tips_trainer/__init__.py\n",
      "tips/containers/workflow_2/tips_trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "for blob in list(bucket.list_blobs(prefix = SOURCEPATH)):\n",
    "    print(blob.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee895974-49cc-4b00-895b-0572ad3ec21e",
   "metadata": {},
   "source": [
    "#### Requirements.txt\n",
    "A list of requirments for the training code to run.  These packages will be added/updated on the pre-built container when the `Dockerfile` instructions runs `pip install ...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "352de6fa-3f49-40de-a309-69acd6a207bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = f\"\"\"\n",
    "tensorflow_io\n",
    "google-cloud-aiplatform>={aiplatform.__version__}\n",
    "protobuf=={pkg_resources.get_distribution('protobuf').version}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "e0289c92-d14a-426f-bf6b-aa8fc8e9f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = bucket.blob(f'{SOURCEPATH}/requirements.txt')\n",
    "blob.upload_from_string(requirements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da8d70-4351-4ae6-9594-1fd066304559",
   "metadata": {},
   "source": [
    "#### Dockerfile\n",
    "The Docker build instructions.  This copies the `requirements.txt` and training script into the containers, `pip installs ...` the requirements, sets the entrypoint for the container to the training script.\n",
    "\n",
    "Note: the `COPY` statement is moving multiple files so the destination must end in `/` here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "56e7869a-6802-4ef7-872f-23872a987ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile = f\"\"\"\n",
    "FROM {TRAIN_IMAGE}\n",
    "WORKDIR /training\n",
    "# copy requirements and install them\n",
    "COPY requirements.txt ./\n",
    "RUN pip install --no-cache-dir --upgrade pip \\\n",
    "  && pip install --no-cache-dir -r requirements.txt\n",
    "## Copies the trainer code to the docker image\n",
    "COPY tips_trainer/* ./tips_trainer/\n",
    "## Sets up the entry point to invoke the trainer\n",
    "ENTRYPOINT [\"python\", \"-m\", \"tips_trainer.train\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "16af78fc-3396-4595-bd65-6891190bba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = bucket.blob(f'{SOURCEPATH}/Dockerfile')\n",
    "blob.upload_from_string(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714f2b6-0e64-485a-9d3f-9d6e8341129d",
   "metadata": {},
   "source": [
    "#### Build Custom Container\n",
    "Use the Cloud Build client to construct and run the build instructions.  Here the files collected in GCS are copied to the build instance, then the Docker build in run in the folder with the `Dockerfile`.  The resulting image is pushed to Artifact Registry (setup above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "c5e178d7-b094-4f64-81d3-f99bc5e3c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the build config with empty list of steps - these will be added sequentially\n",
    "build = cloudbuild_v1.Build(\n",
    "    steps = []\n",
    ")\n",
    "# retrieve the source\n",
    "build.steps.append(\n",
    "    {\n",
    "        'name': 'gcr.io/cloud-builders/gsutil',\n",
    "        'args': ['cp', '-r', f'gs://{PROJECT_ID}/{SOURCEPATH}/*', '/workspace']\n",
    "    }\n",
    ")\n",
    "# docker build\n",
    "build.steps.append(\n",
    "    {\n",
    "        'name': 'gcr.io/cloud-builders/docker',\n",
    "        'args': ['build', '-t', f'{REPOSITORY}/{CONTAINER}', '/workspace']\n",
    "    }    \n",
    ")\n",
    "# docker push\n",
    "build.images = [f\"{REPOSITORY}/tips_trainer_{WORKFLOW}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "05b1acb1-65fd-4db5-adbc-9a552c1155d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "steps {\n",
       "  name: \"gcr.io/cloud-builders/gsutil\"\n",
       "  args: \"cp\"\n",
       "  args: \"-r\"\n",
       "  args: \"gs://statmike-mlops-349915/tips/containers/workflow_2/*\"\n",
       "  args: \"/workspace\"\n",
       "}\n",
       "steps {\n",
       "  name: \"gcr.io/cloud-builders/docker\"\n",
       "  args: \"build\"\n",
       "  args: \"-t\"\n",
       "  args: \"us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915-docker/tips_trainer_workflow_2\"\n",
       "  args: \"/workspace\"\n",
       "}\n",
       "images: \"us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915-docker/tips_trainer_workflow_2\""
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "5694ba96-33a7-4d2c-8e38-8063e51ae40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "operation = cb_client.create_build(\n",
    "    project_id = PROJECT_ID,\n",
    "    build = build\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "1d2f4ca1-b31e-4ea1-870c-36b2e9e5e073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Status.SUCCESS: 3>,\n",
       " images: \"us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915-docker/tips_trainer_workflow_2\")"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = operation.result()\n",
    "response.status, response.artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "783902e1-b34b-4d45-b5bd-57b07ca1fab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review the Custom Container with Artifact Registry in the Google Cloud Console:\n",
      "https://console.cloud.google.com/artifacts/docker/statmike-mlops-349915/us-central1/statmike-mlops-349915-docker?project=statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "print(f\"Review the Custom Container with Artifact Registry in the Google Cloud Console:\\nhttps://console.cloud.google.com/artifacts/docker/{PROJECT_ID}/{REGION}/{PROJECT_ID}-docker?project={PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6449b75e-8597-495f-a26d-966a004dec87",
   "metadata": {},
   "source": [
    "### Workflow 3: copy package to container\n",
    "<a id = 'workflow3'></a>\n",
    "\n",
    "This workflow builds a custom container using a Vertex AI Pre-Built container for traning as the base.  The code below stores the Python package distribution and `Dockerfile` in the project GCS bucket and then launches a Cloud Build job that copies these and runs the Docker build process with Cloud Build.\n",
    "\n",
    "To see an an example Vertex AI Training Custom Job running with the custom container created here go to [Python Training - workflow 3](./Python%20Training.ipynb#workflow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f615030b-c7ea-42a5-96e4-8ae889617b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fef6db-4b1e-4ec5-a687-c32ff366befb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d8305a4-b746-4a8a-a494-fc72ed0b842a",
   "metadata": {},
   "source": [
    "### Workflow 4: pip install package from GCS to container\n",
    "<a id = 'workflow4'></a>\n",
    "\n",
    "This workflow builds a custom container using a Vertex AI Pre-Built container for traning as the base.  The code below stores the `Dockerfile` in the project GCS bucket and then launches a Cloud Build job that copies this and runs the Docker build process with Cloud Build.\n",
    "\n",
    "To see an an example Vertex AI Training Custom Job running with the custom container created here go to [Python Training - workflow 4](./Python%20Training.ipynb#workflow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e5e2b-6dfd-4732-a6b2-edddb097388e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27597f2-6d01-4de9-ad03-8c7ce83c1707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c16c5b27-7edc-448a-a7a5-023176e3300a",
   "metadata": {},
   "source": [
    "### Workflow 5: pip install package from GitHub to container\n",
    "<a id = 'workflow5'></a>\n",
    "\n",
    "This workflow builds a custom container using a Vertex AI Pre-Built container for traning as the base.  The code below stores the `Dockerfile` in the project GCS bucket and then launches a Cloud Build job that copies this and runs the Docker build process with Cloud Build.\n",
    "\n",
    "To see an an example Vertex AI Training Custom Job running with the custom container created here go to [Python Training - workflow 5](./Python%20Training.ipynb#workflow5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdcd513-db7d-401d-900e-4c6bd1626304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad673a-b7ad-4994-a493-65af9670b37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21a491a4-432b-4f1a-b964-b3976de880f0",
   "metadata": {},
   "source": [
    "### Workflow 6: pip install package from Artifact Registry to container\n",
    "<a id = 'workflow6'></a>\n",
    "\n",
    "This workflow builds a custom container using a Vertex AI Pre-Built container for traning as the base.  The code below stores the `Dockerfile` in the project GCS bucket and then launches a Cloud Build job that copies this and runs the Docker build process with Cloud Build.\n",
    "\n",
    "To see an an example Vertex AI Training Custom Job running with the custom container created here go to [Python Training - workflow 6](./Python%20Training.ipynb#workflow6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f577b5-88d7-495b-a985-ed28688d3992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe3dbf2-4bbd-482e-b6e9-495a6efdd85b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975fd39a-e922-45af-b4c1-1e8b578eecfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d4b4c9-9116-4c1c-9fab-b51ed26a9490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4870f-6d7c-4a68-8407-0783baab3d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05fa434-439d-487b-b11a-f3a211dc582f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd542d-1fab-43a7-950d-af801e430754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc38312-09fd-4235-b18b-3f450cb8880a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c79e95-9026-48a3-be4f-b01c3ca73101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d9f6d-fec8-45c5-a5e0-1573649e7032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5046674a-3a92-46fa-ab77-2b8ea4339166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6eaf6d-46d9-4a0c-aa22-499f45af8f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080487d5",
   "metadata": {},
   "source": [
    "# Vertex AI - Training > Hyperparameter Tuning Job\n",
    "## Using A Python Package\n",
    "## With Vertex AI - Experiments > Experiments (Tensorboard)\n",
    "\n",
    "This notebook creates a python package for a TensorFlow training project that uses the `<PROJECT_ID>.digits.digits_prepped` BigQuery table. Vertex AI clients are used to setup a Vertex AI custom training job to run the training package.  Then Vertex AI clients are used to upload the model and deploy it to an endpoint for online predictions.\n",
    "\n",
    "**Prerequisites**\n",
    "- `00 - Initial Setup`\n",
    "- `01 - BigQuery - Data`\n",
    "- `05 - Vertex AI - Tensorboard`\n",
    "- `05b - Vertex AI - Training Job from Python Package` (helpful to understand)\n",
    "    - Contains the Python Package used in this notebooks Pipeline Job\n",
    "    \n",
    "**Resources**\n",
    "- Adopted From:\n",
    "    - https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/custom_job_image_classification_model_for_online_prediction.ipynb\n",
    "- Using Google Cloud Client Libraries (Python for Vertex AI):\n",
    "    - https://googleapis.dev/python/aiplatform/latest/index.html\n",
    "\n",
    "**Overview**\n",
    "\n",
    "<img src=\"architectures/statmike-mlops-05c.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f57784",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19db78b",
   "metadata": {},
   "source": [
    "Setup the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06bb768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0739d23",
   "metadata": {},
   "source": [
    "Define Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9807b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations\n",
    "REGION = 'us-central1'\n",
    "PROJECT_ID='statmike-mlops'\n",
    "BUCKET_NAME='gs://{}/digits/model/05e_aip_train_job'.format(PROJECT_ID)\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "EXPERIMENT_NAME = '05E_AIP_DIGITS_'\n",
    "JOB_NAME = EXPERIMENT_NAME+TIMESTAMP\n",
    "MODEL_DIR = '{}/{}'.format(BUCKET_NAME, JOB_NAME)\n",
    "PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION\n",
    "\n",
    "# files\n",
    "PACKAGE = 'custom_5e'\n",
    "\n",
    "# Resources\n",
    "TRAIN_IMAGE='us-docker.pkg.dev/cloud-aiplatform/training/tf-cpu.2-4:latest'\n",
    "DEPLOY_IMAGE ='us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-3:latest'\n",
    "TRAIN_COMPUTE='n1-standard-4'\n",
    "DEPLOY_COMPUTE='n1-standard-4'\n",
    "\n",
    "# TF Parameters to pass\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "# Tensorboard Info\n",
    "TENSORBOARD_ID = 'digits_tensorboard'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92991a62",
   "metadata": {},
   "source": [
    "Get Service Account email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226991a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/googleapiclient/discovery_cache/__init__.py\", line 44, in autodetect\n",
      "    from . import file_cache\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
      "    \"file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\"\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "\n",
    "service = discovery.build('iam', 'v1', credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1325f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n"
     ]
    }
   ],
   "source": [
    "request = service.projects().serviceAccounts().list(name=\"projects/\"+PROJECT_ID)\n",
    "response = request.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe7a6544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'691911073727-compute@developer.gserviceaccount.com'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SERVICE_ACCOUNT = response['accounts'][0]['email']\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b28eb",
   "metadata": {},
   "source": [
    "Setup AI Platform Python Clients\n",
    "- https://googleapis.dev/python/aiplatform/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1ff1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "clients = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1f375ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84669e0",
   "metadata": {},
   "source": [
    "---\n",
    "## Get Tensorboard Instance Name\n",
    "The training job will show up as an experiment for the Tensorboard instance and have the same name as the training job ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb68314",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = aiplatform.Tensorboard.list(filter='display_name={}'.format(TENSORBOARD_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a765b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_NAME = tb[0].resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7126f77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/691911073727/locations/us-central1/tensorboards/3436703912520843264'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSORBOARD_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a713e1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'digits_tensorboard'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb[0].display_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b5a6b",
   "metadata": {},
   "source": [
    "---\n",
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ce9d5",
   "metadata": {},
   "source": [
    "### Assemble Python Package for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73564b",
   "metadata": {},
   "source": [
    "Make a subdirectory, called `custom_5*`. If the directory has been previously created then delete it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "888f1bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {PACKAGE}\n",
    "!mkdir {PACKAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246715cc",
   "metadata": {},
   "source": [
    "Create the `setup.py` file in the subdirectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a5d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PACKAGE}/setup.py\n",
    "import setuptools\n",
    "REQUIRED_PACKAGES = ['tensorflow_io']\n",
    "setuptools.setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    packages=['trainer'],\n",
    "    description='Digit Training Package')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86396e4",
   "metadata": {},
   "source": [
    "Make a subdirectory, called `trainer`.  Include a `__init__.py` file in the subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d594f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {PACKAGE}/trainer\n",
    "!touch {PACKAGE}/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5816d8",
   "metadata": {},
   "source": [
    "Create the main python trainer file as `/trainer/task.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6970185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom_5e/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PACKAGE}/trainer/task.py\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp \n",
    "from google.cloud import bigquery\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import hypertune\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
    "parser.add_argument('--epochs',dest='epochs', default=10, type=int, help='Number of Epochs')\n",
    "parser.add_argument('--batch_size',dest='batch_size', default=32, type=int, help='Batch Size')\n",
    "parser.add_argument('--lr',dest='learning_rate', required=True, type=float, help='Learning Rate')\n",
    "parser.add_argument('--m',dest='momentum', required=True, type=float, help='Momentum')\n",
    "#parser.add_argument('',dest='', default=, type=, help='')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# built in parameters for data source:\n",
    "PROJECT_ID='statmike-mlops'\n",
    "BQDATASET_ID='digits'\n",
    "BQTABLE_ID='digits_prepped'\n",
    "\n",
    "selected_fields = ['p0', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8', 'p9', 'p10', 'p11', 'p12', 'p13', 'p14', 'p15', 'p16', 'p17', 'p18', 'p19', 'p20', 'p21', 'p22', 'p23', 'p24', 'p25', 'p26', 'p27', 'p28', 'p29', 'p30', 'p31', 'p32', 'p33', 'p34', 'p35', 'p36', 'p37', 'p38', 'p39', 'p40', 'p41', 'p42', 'p43', 'p44', 'p45', 'p46', 'p47', 'p48', 'p49', 'p50', 'p51', 'p52', 'p53', 'p54', 'p55', 'p56', 'p57', 'p58', 'p59', 'p60', 'p61', 'p62', 'p63', 'target']\n",
    "output_types = ['FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'INT64']\n",
    "\n",
    "feature_columns = []\n",
    "feature_layer_inputs = {}\n",
    "for header in selected_fields:\n",
    "    if header != 'target':\n",
    "        feature_columns.append(tf.feature_column.numeric_column(header))\n",
    "        feature_layer_inputs[header] = tf.keras.Input(shape=(1,),name=header)\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "output_types = [dtypes.float64 if x=='FLOAT64' else dtypes.int64 for x in output_types]\n",
    "\n",
    "def transTable(row_dict):\n",
    "    target=row_dict.pop('target')\n",
    "    target = tf.one_hot(tf.cast(target,tf.int64),10)\n",
    "    target = tf.cast(target,tf.float32)\n",
    "    return(row_dict,target)\n",
    "\n",
    "client = BigQueryClient()\n",
    "session = client.read_session(\"projects/\"+PROJECT_ID,PROJECT_ID,BQTABLE_ID,BQDATASET_ID,selected_fields,output_types,row_restriction=\"SPLITS='TRAIN'\",requested_streams=3)\n",
    "table = session.parallel_read_rows()\n",
    "table = table.map(transTable)\n",
    "train = table.shuffle(100000).batch(args.batch_size)\n",
    "\n",
    "client = BigQueryClient()\n",
    "session = client.read_session(\"projects/\"+PROJECT_ID,PROJECT_ID,BQTABLE_ID,BQDATASET_ID,selected_fields,output_types,row_restriction=\"SPLITS='TEST'\",requested_streams=3)\n",
    "table = session.parallel_read_rows()\n",
    "table = table.map(transTable)\n",
    "test = table.batch(args.batch_size)\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "feature_layer_outputs = feature_layer(feature_layer_inputs)\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=args.learning_rate, momentum=args.momentum)\n",
    "model = tf.keras.Model(inputs=[v for v in feature_layer_inputs.values()],outputs=tf.keras.layers.Dense(10,activation=tf.nn.softmax)(feature_layer_outputs))\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "tf.keras.utils.plot_model(model,show_shapes=True, show_dtype=True)\n",
    "\n",
    "# setup tensorboard logs\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'], histogram_freq=1) #(log_dir=model_dir+'/logs', histogram_freq=1)\n",
    "\n",
    "history = model.fit(train,epochs=args.epochs, callbacks=[tensorboard_callback])\n",
    "\n",
    "# report hypertune info back to Vertex AI Training > Hyperparamter Tuning Job\n",
    "hpt = hypertune.HyperTune()\n",
    "hpt.report_hyperparameter_tuning_metric(\n",
    "    hyperparameter_metric_tag = 'loss',\n",
    "    metric_value = history.history['loss'][-1],\n",
    "    global_step = 1)\n",
    "\n",
    "#model.save(os.getenv(\"AIP_MODEL_DIR\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b953ec8d",
   "metadata": {},
   "source": [
    "Store the training package in a Cloud Storage Bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dc253dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_5e/\n",
      "custom_5e/setup.py\n",
      "custom_5e/trainer/\n",
      "custom_5e/trainer/task.py\n",
      "custom_5e/trainer/__init__.py\n",
      "Copying file://custom_5e.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  1.8 KiB/  1.8 KiB]                                                \n",
      "Operation completed over 1 objects/1.8 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!rm -f {PACKAGE}.tar {PACKAGE}.tar.gz\n",
    "!tar cvf {PACKAGE}.tar {PACKAGE}\n",
    "!gzip {PACKAGE}.tar\n",
    "!gsutil cp {PACKAGE}.tar.gz $BUCKET_NAME/{PACKAGE}.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bddbece",
   "metadata": {},
   "source": [
    "### Setup Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8525ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MACHINE_SPEC = {\n",
    "    \"machine_type\": TRAIN_COMPUTE,\n",
    "    \"accelerator_count\": 0\n",
    "}\n",
    "\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE)\n",
    "]\n",
    "\n",
    "WORKER_POOL_SPEC = [\n",
    "    {\n",
    "        \"replica_count\": 3,\n",
    "        \"machine_spec\": MACHINE_SPEC,\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": TRAIN_IMAGE,\n",
    "            \"package_uris\": [BUCKET_NAME + \"/{}.tar.gz\".format(PACKAGE)],\n",
    "            \"python_module\": \"trainer.task\",\n",
    "            \"args\": CMDARGS\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5874be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customJob = aiplatform.CustomJob(\n",
    "    display_name = JOB_NAME,\n",
    "    worker_pool_specs = WORKER_POOL_SPEC,\n",
    "    base_output_dir = MODEL_DIR,\n",
    "    staging_bucket = MODEL_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e130f",
   "metadata": {},
   "source": [
    "### Setup Hyperparameter Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83c14650",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_SPEC = {\n",
    "    \"loss\": \"minimize\"\n",
    "}\n",
    "\n",
    "PARAMETER_SPEC = {\n",
    "    \"lr\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=0.001, max=0.1, scale=\"log\"),\n",
    "    \"m\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=1e-7, max=0.9, scale=\"linear\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "961b2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "htJob = aiplatform.HyperparameterTuningJob(\n",
    "    display_name = JOB_NAME,\n",
    "    custom_job = customJob,\n",
    "    metric_spec = METRIC_SPEC,\n",
    "    parameter_spec = PARAMETER_SPEC,\n",
    "    max_trial_count = 12,\n",
    "    parallel_trial_count = 3,\n",
    "    search_algorithm = 'random'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeaa369",
   "metadata": {},
   "source": [
    "### Run Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a330a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating HyperparameterTuningJob\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob created. Resource name: projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720\n",
      "INFO:google.cloud.aiplatform.jobs:To use this HyperparameterTuningJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:hpt_job = aiplatform.HyperparameterTuningJob.get('projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720')\n",
      "INFO:google.cloud.aiplatform.jobs:View HyperparameterTuningJob:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/4116538549044510720?project=691911073727\n",
      "INFO:google.cloud.aiplatform.jobs:View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+691911073727+locations+us-central1+tensorboards+3436703912520843264+experiments+4116538549044510720\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:HyperparameterTuningJob run completed. Resource name: projects/691911073727/locations/us-central1/hyperparameterTuningJobs/4116538549044510720\n"
     ]
    }
   ],
   "source": [
    "htJob.run(\n",
    "    service_account = SERVICE_ACCOUNT,\n",
    "    tensorboard = TENSORBOARD_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "274fb979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09431399405002594"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htJob.trials[0].final_measurement.metrics[0].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0801eff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id: \"11\"\n",
       "state: SUCCEEDED\n",
       "parameters {\n",
       "  parameter_id: \"lr\"\n",
       "  value {\n",
       "    number_value: 0.052648029934358465\n",
       "  }\n",
       "}\n",
       "parameters {\n",
       "  parameter_id: \"m\"\n",
       "  value {\n",
       "    number_value: 0.036914641886908954\n",
       "  }\n",
       "}\n",
       "final_measurement {\n",
       "  step_count: 1\n",
       "  metrics {\n",
       "    metric_id: \"loss\"\n",
       "    value: 0.0048503512516617775\n",
       "  }\n",
       "}\n",
       "start_time {\n",
       "  seconds: 1630609701\n",
       "  nanos: 256115055\n",
       "}\n",
       "end_time {\n",
       "  seconds: 1630610103\n",
       "}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = [trial.final_measurement.metrics[0].value for trial in htJob.trials]\n",
    "htJob.trials[losses.index(min(losses))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2befa097",
   "metadata": {},
   "source": [
    "---\n",
    "## Remove Resources\n",
    "see notebook \"XX - Cleanup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06805138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

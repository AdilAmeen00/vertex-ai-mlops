{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05i - Vertex AI > Training > Hyperparameter Tuning Jobs - With Custom Container\n",
    "\n",
    "### 05 Series Overview\n",
    "Where a model gets trained is where it consumes computing resources.  With Vertex AI, you have choices for configuring the computing resources available at training.  This notebook is an example of an execution environment.  When it was set up there were choices for machine type and accelerators (GPUs).  \n",
    "\n",
    "In the `05` notebook, the model training happened directly in the notebook.  The models were then imported to Vertex AI and deployed to an endpoint for online predictions. \n",
    "\n",
    "In this `05a-05i` series of demonstrations, the same model is trained using managed computing resources in Vertex AI as custom training jobs.  These jobs will be demonstrated as:\n",
    "\n",
    "-  Custom Job from a python script (`05a`), python source distribution (`05b`), and custom container (`05c`)\n",
    "-  Training Pipeline that trains and saves models from a python script (`05d`), python source distribution (`05e`), and custom container (`05f`)\n",
    "-  Hyperparameter Tuning Jobs from a python script (`05g`), python source distribution (`05h`), and custom container (`05i`)\n",
    "\n",
    "### This Notebook (`05i`): An extension of `05c` with Hyperparmeter Tuning - And Tensorboard HParams  \n",
    "This notebook trains the same Tensorflow Keras model from `05` by first modifying and saving the training code as a Python module on a custom container (same as `05c`).  While this example fits nicely in a single script, larger examples will benefit from the flexibility offered by source distributions or module storage and this notebook gives an example of making the shift. \n",
    "\n",
    "The training code is stored directly on the custom container as part of the Docker build process.  This build process uses a pre-built container as the base image and adds both packages and the training code as a Python module.  This container is specified in the setup of a custom training job and also assigned compute resources for executing the training in a managed service.  This is done with the [Vertex AI Python SDK](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#) using the class [`aiplatform.CustomJob()`](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.CustomJob).\n",
    "\n",
    "The Custom Job is then used as the input for a Vertex AI > Training > Hyperparameter Tuning Job.  This runs and manages the tuning loops for the number of trials in each loop, collects the metric(s) and manages the parameters with the selected search algorithm for parameter modification.  This is done with the [Vertex AI Python SDK](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#) using the class [`aiplatform.HyperparameterTuningJob()`](https://googleapis.dev/python/aiplatform/latest/aiplatform.html#google.cloud.aiplatform.HyperparameterTuningJob).\n",
    "\n",
    "The training can be reviewed with Vertex AI's managed Tensorboard under Experiments > Experiments, or by clicking on the `05i...` job under Training > Hyperparameter Tuning Jobs and then clicking the 'Open Tensorboard' link.  **Click on the HParams tab in Tensorboard to review the hyperparameters and metrics.**\n",
    "\n",
    "<img src=\"architectures/overview/Training.png\">\n",
    "\n",
    "### Prerequisites:\n",
    "-  01 - BigQuery - Table Data Source\n",
    "-  Understanding:\n",
    "    -  05 - Vertex AI > Notebooks - Models Built in Notebooks with Tensorflow\n",
    "        -  Contains a more granular review of the Tensorflow model training\n",
    "\n",
    "### Resources:\n",
    "- [Vertex AI Custom Container For Training](https://cloud.google.com/vertex-ai/docs/training/containers-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Vertex AI - Conceptual Flow\n",
    "\n",
    "<img src=\"architectures/slides/05i_arch.png\">\n",
    "\n",
    "---\n",
    "## Vertex AI - Workflow\n",
    "\n",
    "<img src=\"architectures/slides/05i_console.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'statmike-mlops-349915'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "DATANAME = 'fraud'\n",
    "NOTEBOOK = '05i'\n",
    "SERIES = '05'\n",
    "\n",
    "# Resources\n",
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf-cpu.2-3'\n",
    "DEPLOY_IMAGE ='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-7:latest'\n",
    "TRAIN_COMPUTE = 'n1-standard-4'\n",
    "DEPLOY_COMPUTE = 'n1-standard-4'\n",
    "\n",
    "# Model Training\n",
    "VAR_TARGET = 'Class'\n",
    "VAR_OMIT = 'transaction_id' # add more variables to the string with space delimiters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "bigquery = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET = PROJECT_ID\n",
    "URI = f\"gs://{BUCKET}/{DATANAME}/models/{NOTEBOOK}\"\n",
    "DIR = f\"temp/{NOTEBOOK}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1026793852137-compute@developer.gserviceaccount.com'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Give service account roles/storage.objectAdmin permissions\n",
    "# Console > IMA > Select Account <projectnumber>-compute@developer.gserviceaccount.com > edit - give role\n",
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {DIR}\n",
    "!mkdir -p {DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = 'classification'\n",
    "MODEL_TYPE = 'dnn'\n",
    "EXPERIMENT_NAME = f'experiment-{SERIES}-{NOTEBOOK}-{TASK}-{MODEL_TYPE}'\n",
    "RUN_NAME = f'run-{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Get Vertex AI Experiments Tensorboard Instance Name\n",
    "[Vertex AI Experiments](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview) has managed [Tensorboard](https://www.tensorflow.org/tensorboard) instances that you can track Tensorboard Experiments (a training run or hyperparameter tuning sweep).  \n",
    "\n",
    "The training job will show up as an experiment for the Tensorboard instance and have the same name as the training job ID.\n",
    "\n",
    "This code checks to see if a Tensorboard Instance has been created in the project, retrieves it if so, creates it otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = aiplatform.Tensorboard.list(filter=f\"labels.series={SERIES}\")\n",
    "if tb:\n",
    "    tb = tb[0]\n",
    "else: \n",
    "    tb = aiplatform.Tensorboard.create(display_name = SERIES, labels = {'series' : f'{SERIES}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/1026793852137/locations/us-central1/tensorboards/3514619704511561728'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Setup Vertex AI Experiments\n",
    "\n",
    "The code in this section initializes the experiment and starts a run that represents this notebook.  Throughout the notebook sections for model training and evaluation information will be logged to the experiment using:\n",
    "- [.log_params](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_params)\n",
    "- [.log_metrics](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_metrics)\n",
    "- [.log_time_series_metrics](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_time_series_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(experiment = EXPERIMENT_NAME, experiment_tensorboard = tb.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble Python File for Training\n",
    "\n",
    "Create the main python trainer file as `/train.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {DIR}/source/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp/05i/source/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {DIR}/source/trainer/train.py\n",
    "\n",
    "# package import\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import hypertune\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "# import argument to local variables\n",
    "parser = argparse.ArgumentParser()\n",
    "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
    "parser.add_argument('--epochs', dest = 'epochs', default = 10, type = int, help = 'Number of Epochs')\n",
    "parser.add_argument('--batch_size', dest = 'batch_size', default = 32, type = int, help = 'Batch Size')\n",
    "parser.add_argument('--var_target', dest = 'var_target', type=str)\n",
    "parser.add_argument('--var_omit', dest = 'var_omit', type=str, nargs='*')\n",
    "parser.add_argument('--project_id', dest = 'project_id', type=str)\n",
    "parser.add_argument('--dataname', dest = 'dataname', type=str)\n",
    "parser.add_argument('--region', dest = 'region', type=str)\n",
    "parser.add_argument('--notebook', dest = 'notebook', type=str)\n",
    "parser.add_argument('--series', dest = 'series', type=str)\n",
    "parser.add_argument('--experiment_name', dest = 'experiment_name', type=str)\n",
    "parser.add_argument('--run_name', dest = 'run_name', type=str)\n",
    "# hyperparameters\n",
    "parser.add_argument('--lr', dest='learning_rate', required=True, type=float, help='Learning Rate')\n",
    "parser.add_argument('--m', dest='momentum', required=True, type=float, help='Momentum')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# setup tensorboard hparams\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.0, 1.0))\n",
    "HP_MOMENTUM = hp.HParam('momentum', hp.RealInterval(0.0,1.0))\n",
    "hparams = {\n",
    "    HP_LEARNING_RATE: args.learning_rate,\n",
    "    HP_MOMENTUM: args.momentum\n",
    "}\n",
    "\n",
    "# clients\n",
    "bigquery = bigquery.Client(project = args.project_id)\n",
    "aiplatform.init(project = args.project_id, location = args.region)\n",
    "hpt = hypertune.HyperTune()\n",
    "args.run_name = f'{args.run_name}-{hpt.trial_id}'\n",
    "\n",
    "# Vertex AI Experiment\n",
    "expRun = aiplatform.ExperimentRun.create(run_name = args.run_name, experiment = args.experiment_name)\n",
    "expRun.log_params({'DATANAME': args.dataname, 'NOTEBOOK': args.notebook, 'SERIES': args.series, 'PROJECT_ID': args.project_id, 'VAR_TARGET': args.var_target})\n",
    "expRun.log_params({'hyperparameter.learning_rate': args.learning_rate, 'hyperparameter.momentum': args.momentum})\n",
    "\n",
    "# get schema from bigquery source\n",
    "query = f\"SELECT * FROM {args.dataname}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{args.dataname}_prepped'\"\n",
    "schema = bigquery.query(query).to_dataframe()\n",
    "\n",
    "# get number of classes from bigquery source\n",
    "nclasses = bigquery.query(query = f'SELECT DISTINCT {args.var_target} FROM {args.dataname}.{args.dataname}_prepped WHERE {args.var_target} is not null').to_dataframe()\n",
    "nclasses = nclasses.shape[0]\n",
    "expRun.log_params({'data_source': f'bq://{args.dataname}.{args.dataname}_prepped', 'nclasses': nclasses, 'var_split': 'splits'})\n",
    "\n",
    "# Make a list of columns to omit\n",
    "OMIT = args.var_omit + ['splits']\n",
    "\n",
    "# use schema to prepare a list of columns to read from BigQuery\n",
    "selected_fields = schema[~schema.column_name.isin(OMIT)].column_name.tolist()\n",
    "\n",
    "# all the columns in this data source are either float64 or int64\n",
    "output_types = [dtypes.float64 if x=='FLOAT64' else dtypes.int64 for x in schema[~schema.column_name.isin(OMIT)].data_type.tolist()]\n",
    "\n",
    "# remap input data to Tensorflow inputs of features and target\n",
    "def transTable(row_dict):\n",
    "    target = row_dict.pop(args.var_target)\n",
    "    target = tf.one_hot(tf.cast(target, tf.int64), nclasses)\n",
    "    target = tf.cast(target, tf.float32)\n",
    "    return(row_dict, target)\n",
    "\n",
    "# function to setup a bigquery reader with Tensorflow I/O\n",
    "def bq_reader(split):\n",
    "    reader = BigQueryClient()\n",
    "\n",
    "    training = reader.read_session(\n",
    "        parent = f\"projects/{args.project_id}\",\n",
    "        project_id = args.project_id,\n",
    "        table_id = f\"{args.dataname}_prepped\",\n",
    "        dataset_id = args.dataname,\n",
    "        selected_fields = selected_fields,\n",
    "        output_types = output_types,\n",
    "        row_restriction = f\"splits='{split}'\",\n",
    "        requested_streams = 3\n",
    "    )\n",
    "    \n",
    "    return training\n",
    "\n",
    "# setup feed for train, validate and test\n",
    "train = bq_reader('TRAIN').parallel_read_rows().prefetch(1).map(transTable).shuffle(args.batch_size*10).batch(args.batch_size)\n",
    "validate = bq_reader('VALIDATE').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
    "test = bq_reader('TEST').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
    "expRun.log_params({'BATCH_SIZE': args.batch_size, 'SHUFFLE': 10*args.batch_size, 'PREFETCH': 1})\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "# model input definitions\n",
    "feature_columns = {header: tf.feature_column.numeric_column(header) for header in selected_fields if header != args.var_target}\n",
    "feature_layer_inputs = {header: tf.keras.layers.Input(shape = (1,), name = header) for header in selected_fields if header != args.var_target}\n",
    "\n",
    "# feature columns to a Dense Feature Layer\n",
    "feature_layer_outputs = tf.keras.layers.DenseFeatures(feature_columns.values(), name = 'feature_layer')(feature_layer_inputs)\n",
    "\n",
    "# batch normalization then Dense with softmax activation to nclasses\n",
    "layers = tf.keras.layers.BatchNormalization(name = 'batch_normalization_layer')(feature_layer_outputs)\n",
    "layers = tf.keras.layers.Dense(64, activation = 'relu', name = 'hidden_layer')(layers)\n",
    "layers = tf.keras.layers.Dense(32, activation = 'relu', name = 'embedding_layer')(layers)\n",
    "layers = tf.keras.layers.Dense(nclasses, activation = tf.nn.softmax, name = 'prediction_layer')(layers)\n",
    "\n",
    "# the model\n",
    "model = tf.keras.Model(\n",
    "    inputs = feature_layer_inputs,\n",
    "    outputs = layers,\n",
    "    name = args.dataname\n",
    ")\n",
    "opt = tf.keras.optimizers.SGD(learning_rate = args.learning_rate, momentum = args.momentum) #SGD or Adam\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "model.compile(\n",
    "    optimizer = opt,\n",
    "    loss = loss,\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(curve='PR', name = 'auprc')]\n",
    ")\n",
    "\n",
    "# setup tensorboard logs and train\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'], histogram_freq=1)\n",
    "hparams_callback = hp.KerasCallback(os.environ['AIP_TENSORBOARD_LOG_DIR'] + 'train/', hparams, trial_id = args.run_name)\n",
    "history = model.fit(train, epochs = args.epochs, callbacks = [tensorboard_callback, hparams_callback], validation_data = validate)\n",
    "expRun.log_params({'epochs': history.params['epochs']})\n",
    "for e in range(0, history.params['epochs']):\n",
    "    expRun.log_time_series_metrics(\n",
    "        {\n",
    "            'train_loss': history.history['loss'][e],\n",
    "            'train_accuracy': history.history['accuracy'][e],\n",
    "            'train_auprc': history.history['auprc'][e],\n",
    "            'val_loss': history.history['val_loss'][e],\n",
    "            'val_accuracy': history.history['val_accuracy'][e],\n",
    "            'val_auprc': history.history['val_auprc'][e]\n",
    "        }\n",
    "    )\n",
    "\n",
    "# evaluations:\n",
    "loss, accuracy, auprc = model.evaluate(test)\n",
    "expRun.log_metrics({'test_loss': loss, 'test_accuracy': accuracy, 'test_auprc': auprc})\n",
    "loss, accuracy, auprc = model.evaluate(validate)\n",
    "expRun.log_metrics({'val_loss': loss, 'val_accuracy': accuracy, 'val_auprc': auprc})\n",
    "loss, accuracy, auprc = model.evaluate(train)\n",
    "expRun.log_metrics({'train_loss': loss, 'train_accuracy': accuracy, 'train_auprc': auprc})\n",
    "\n",
    "# output the model save files\n",
    "model.save(os.getenv(\"AIP_MODEL_DIR\"))\n",
    "expRun.log_params({'MODEL_URI': os.getenv(\"AIP_MODEL_DIR\")})\n",
    "expRun.end_run()\n",
    "\n",
    "# report hypertune info back to Vertex AI Training > Hyperparamter Tuning Job\n",
    "hpt.report_hyperparameter_tuning_metric(\n",
    "    hyperparameter_metric_tag = 'auprc',\n",
    "    metric_value = history.history['auprc'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Requirements.txt File for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = f\"\"\"tensorflow_io\n",
    "google-cloud-aiplatform>={aiplatform.__version__}\n",
    "cloudml-hypertune\n",
    "\"\"\"\n",
    "with open(f'{DIR}/source/requirements.txt', 'w') as f:\n",
    "    f.write(requirements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Container\n",
    "- https://cloud.google.com/vertex-ai/docs/training/create-custom-container\n",
    "- https://cloud.google.com/vertex-ai/docs/training/pre-built-containers\n",
    "- https://cloud.google.com/vertex-ai/docs/general/deep-learning\n",
    "    - https://cloud.google.com/deep-learning-containers/docs/choosing-container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a Base Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/deeplearning-platform-release/tf-cpu.2-3'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_IMAGE # Defined above in Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Dockerfile\n",
    "A basic dockerfile thats take the base image and copies the code in and define an entrypoint - what python script to run first in this case.  Add RUN entries to pip install additional packages.\n",
    "\n",
    "In this case, hyperparameter tuning uses [reports metrics to Vertex AI](https://cloud.google.com/vertex-ai/docs/training/using-hyperparameter-tuning#report-metrics) using the [cloudml-hypertune Python package](https://github.com/GoogleCloudPlatform/cloudml-hypertune) and is missing from the base image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile = f\"\"\"\n",
    "FROM {BASE_IMAGE}\n",
    "WORKDIR /\n",
    "# copy requirements and install them\n",
    "COPY requirements.txt ./\n",
    "RUN pip install --no-cache-dir --upgrade pip \\\n",
    "  && pip install --no-cache-dir -r requirements.txt\n",
    "## Copies the trainer code to the docker image\n",
    "COPY trainer /trainer\n",
    "## Sets up the entry point to invoke the trainer\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.train\"]\n",
    "\"\"\"\n",
    "with open(f'{DIR}/source/Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Artifact Registry\n",
    "\n",
    "The container will need to be stored in Artifact Registry, Container Registry or Docker Hub in order to be used by Vertex AI Training jobs.  This notebook will setup Artifact registry and push a local (to this notebook) built container to it. \n",
    "\n",
    "https://cloud.google.com/artifact-registry/docs/docker/store-docker-container-images#gcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Enable Artifact Registry API:\n",
    "Check to see if the api is enabled, if not then enable it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact Registry is Enabled for This Project: statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "services = !gcloud services list --format=\"json\" --available --filter=name:artifactregistry.googleapis.com\n",
    "services = json.loads(\"\".join(services))\n",
    "\n",
    "if (services[0]['config']['name'] == 'artifactregistry.googleapis.com') & (services[0]['state'] == 'ENABLED'):\n",
    "    print(f\"Artifact Registry is Enabled for This Project: {PROJECT_ID}\")\n",
    "else:\n",
    "    print(f\"Enabeling Artifact Registry for this Project: {PROJECT_ID}\")\n",
    "    !gcloud services enable artifactregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create A Repository\n",
    "Check to see if the registry is already created, if not then create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is already a repository named statmike-mlops-349915\n"
     ]
    }
   ],
   "source": [
    "check_for_repo = !gcloud artifacts repositories describe {PROJECT_ID} --location={REGION}\n",
    "\n",
    "if check_for_repo[0].startswith('ERROR'):\n",
    "    print(f'Creating a repository named {PROJECT_ID}')\n",
    "    !gcloud  artifacts repositories create {PROJECT_ID} --repository-format=docker --location={REGION} --description=\"Vertex AI Training Custom Containers\"\n",
    "else:\n",
    "    print(f'There is already a repository named {PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure Local Docker to Use GCLOUD CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build The Custom Container (local to notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915/05i_fraud:latest'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{PROJECT_ID}/{NOTEBOOK}_{DATANAME}:latest\"\n",
    "IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  12.29kB\n",
      "Step 1/6 : FROM gcr.io/deeplearning-platform-release/tf-cpu.2-3\n",
      " ---> 7c0738e47d7d\n",
      "Step 2/6 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> d7460d021e89\n",
      "Step 3/6 : COPY requirements.txt ./\n",
      " ---> 30e431326c08\n",
      "Step 4/6 : RUN pip install --no-cache-dir --upgrade pip   && pip install --no-cache-dir -r requirements.txt\n",
      " ---> Running in 9a6d814b427f\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (22.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-22.2.2-py3-none-any.whl (2.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 34.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.1.2\n",
      "    Uninstalling pip-22.1.2:\n",
      "      Successfully uninstalled pip-22.1.2\n",
      "Successfully installed pip-22.2.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRequirement already satisfied: tensorflow_io in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (0.15.0)\n",
      "Collecting google-cloud-aiplatform>=1.15.1\n",
      "  Downloading google_cloud_aiplatform-1.16.1-py2.py3-none-any.whl (2.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 35.8 MB/s eta 0:00:00\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tensorflow<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_io->-r requirements.txt (line 1)) (2.3.4)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: protobuf<4.0.0dev,>=3.19.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (3.20.1)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (2.4.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (2.34.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (1.20.6)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (1.5.1)\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (2.28.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (1.56.3)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (2.9.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (1.47.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (1.47.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (2.3.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (0.12.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (3.0.9)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (0.3.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (2.10.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (1.14.1)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.1/20.1 MB 156.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (1.1.2)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (0.2.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (3.3.7)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (59.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (2.1.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (1.8.1)\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.9/152.9 kB 169.1 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (4.11.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.15.1->-r requirements.txt (line 2)) (2.21)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (4.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (3.8.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4.0,>=2.3.0->tensorflow_io->-r requirements.txt (line 1)) (3.2.0)\n",
      "Building wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=1e686aa4b5fd6f97e4edc319f6bf309bbb072d4c7edd147a7d3fc058a9c5cfa4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-y9fv8uek/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune, numpy, cachetools, google-auth, google-auth-oauthlib, google-cloud-aiplatform\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.0.0\n",
      "    Uninstalling cachetools-5.0.0:\n",
      "      Successfully uninstalled cachetools-5.0.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.9.0\n",
      "    Uninstalling google-auth-2.9.0:\n",
      "      Successfully uninstalled google-auth-2.9.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 0.5.2\n",
      "    Uninstalling google-auth-oauthlib-0.5.2:\n",
      "      Successfully uninstalled google-auth-oauthlib-0.5.2\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.15.0\n",
      "    Uninstalling google-cloud-aiplatform-1.15.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.15.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx-bsl 0.26.1 requires absl-py<0.11,>=0.9, but you have absl-py 1.1.0 which is incompatible.\n",
      "tfx-bsl 0.26.1 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.52.0 which is incompatible.\n",
      "tfx-bsl 0.26.1 requires pyarrow<0.18,>=0.17, but you have pyarrow 8.0.0 which is incompatible.\n",
      "tensorflow-transform 0.26.0 requires absl-py<0.11,>=0.9, but you have absl-py 1.1.0 which is incompatible.\n",
      "tensorflow-transform 0.26.0 requires pyarrow<0.18,>=0.17, but you have pyarrow 8.0.0 which is incompatible.\n",
      "tensorflow-probability 0.11.0 requires cloudpickle==1.3, but you have cloudpickle 2.1.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.26.1 requires absl-py<0.11,>=0.9, but you have absl-py 1.1.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.26.1 requires pyarrow<0.18,>=0.17, but you have pyarrow 8.0.0 which is incompatible.\n",
      "tensorflow-data-validation 0.26.1 requires absl-py<0.11,>=0.9, but you have absl-py 1.1.0 which is incompatible.\n",
      "tensorflow-data-validation 0.26.1 requires joblib<0.15,>=0.12, but you have joblib 1.0.1 which is incompatible.\n",
      "tensorflow-data-validation 0.26.1 requires pyarrow<0.18,>=0.17, but you have pyarrow 8.0.0 which is incompatible.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.52.0 which is incompatible.\n",
      "apache-beam 2.28.0 requires httplib2<0.18.0,>=0.8, but you have httplib2 0.20.4 which is incompatible.\n",
      "apache-beam 2.28.0 requires pyarrow<3.0.0,>=0.15.1, but you have pyarrow 8.0.0 which is incompatible.\n",
      "apache-beam 2.28.0 requires typing-extensions<3.8.0,>=3.7.0, but you have typing-extensions 4.2.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed cachetools-4.2.4 cloudml-hypertune-0.1.0.dev6 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-cloud-aiplatform-1.16.1 numpy-1.18.5\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 9a6d814b427f\n",
      " ---> efae6da9c9a7\n",
      "Step 5/6 : COPY trainer /trainer\n",
      " ---> 6f7e234031f2\n",
      "Step 6/6 : ENTRYPOINT [\"python\", \"-m\", \"trainer.train\"]\n",
      " ---> Running in 93cc6e84c9e1\n",
      "Removing intermediate container 93cc6e84c9e1\n",
      " ---> 83df8aea3d8a\n",
      "Successfully built 83df8aea3d8a\n",
      "Successfully tagged us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915/05i_fraud:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build {DIR}/source/. -t $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test The Custom Container (local to notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!docker run {IMAGE_URI} --PROJECT_ID {PROJECT_ID} --DATANAME {DATANAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push The Custom Container To Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [us-central1-docker.pkg.dev/statmike-mlops-349915/statmike-mlops-349915/05i_fraud]\n",
      "\n",
      "\u001b[1Bd1c4012a: Preparing \n",
      "\u001b[1Be876ed82: Preparing \n",
      "\u001b[1B2dddd539: Preparing \n",
      "\u001b[1B44a0e0fb: Preparing \n",
      "\u001b[1Bb70226ae: Preparing \n",
      "\u001b[1Ba2906ddf: Preparing \n",
      "\u001b[1B56d0c008: Preparing \n",
      "\u001b[1B59dfa907: Preparing \n",
      "\u001b[1B668df2d8: Preparing \n",
      "\u001b[1B767a76ae: Preparing \n",
      "\u001b[1B559b3e11: Preparing \n",
      "\u001b[1Bc5f28369: Preparing \n",
      "\u001b[1Beeca4cbf: Preparing \n",
      "\u001b[1Bc2b66f65: Preparing \n",
      "\u001b[1B0bba959a: Preparing \n",
      "\u001b[1B677fbd36: Preparing \n",
      "\u001b[1B713472f0: Preparing \n",
      "\u001b[1B33654a88: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B5cfc6aa2: Preparing \n",
      "\u001b[3Bbf18a086: Preparing \n",
      "\u001b[1B4b178955: Preparing \n",
      "\u001b[22B876ed82: Pushed   120.5MB/117.6MB\u001b[21A\u001b[2K\u001b[18A\u001b[2K\u001b[16A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[22A\u001b[2K\u001b[8A\u001b[2K\u001b[22A\u001b[2K\u001b[3A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[21A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2K\u001b[22A\u001b[2Klatest: digest: sha256:97f4e61b483b61ac9ed3509f85b8bec6390119855e96120daabf5a75e6550b63 size: 5338\n"
     ]
    }
   ],
   "source": [
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "    \"--var_target=\" + VAR_TARGET,\n",
    "    \"--var_omit=\" + VAR_OMIT,\n",
    "    \"--project_id=\" + PROJECT_ID,\n",
    "    \"--dataname=\" + DATANAME,\n",
    "    \"--region=\" + REGION,\n",
    "    \"--notebook=\" + NOTEBOOK,\n",
    "    \"--series=\" + SERIES,\n",
    "    \"--experiment_name=\" + EXPERIMENT_NAME,\n",
    "    \"--run_name=\" + RUN_NAME\n",
    "]\n",
    "\n",
    "MACHINE_SPEC = {\n",
    "    \"machine_type\": TRAIN_COMPUTE,\n",
    "    \"accelerator_count\": 0\n",
    "}\n",
    "\n",
    "WORKER_POOL_SPEC = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": MACHINE_SPEC,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": IMAGE_URI,\n",
    "            \"command\": [],\n",
    "            \"args\": CMDARGS\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "customJob = aiplatform.CustomJob(\n",
    "    display_name = f'{NOTEBOOK}_{DATANAME}_{TIMESTAMP}',\n",
    "    worker_pool_specs = WORKER_POOL_SPEC,\n",
    "    base_output_dir = f\"{URI}/{TIMESTAMP}\",\n",
    "    staging_bucket = f\"{URI}/{TIMESTAMP}\",\n",
    "    labels = {'series' : f'{SERIES}', 'notebook' : f'{NOTEBOOK}', 'experiment_name' : f'{EXPERIMENT_NAME}', 'run_name' : f'{RUN_NAME}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Hyperparameter Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_SPEC = {\n",
    "    \"auprc\": \"maximize\"\n",
    "}\n",
    "\n",
    "PARAMETER_SPEC = {\n",
    "    \"lr\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=0.001, max=0.1, scale=\"log\"),\n",
    "    \"m\": aiplatform.hyperparameter_tuning.DoubleParameterSpec(min=1e-7, max=0.9, scale=\"linear\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuningJob = aiplatform.HyperparameterTuningJob(\n",
    "    display_name = f'{NOTEBOOK}_{DATANAME}_{TIMESTAMP}',\n",
    "    custom_job = customJob,\n",
    "    metric_spec = METRIC_SPEC,\n",
    "    parameter_spec = PARAMETER_SPEC,\n",
    "    max_trial_count = 10,\n",
    "    parallel_trial_count = 5,\n",
    "    search_algorithm = None,\n",
    "    labels = {'series' : f'{SERIES}', 'notebook' : f'{NOTEBOOK}', 'experiment_name' : f'{EXPERIMENT_NAME}', 'run_name' : f'{RUN_NAME}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HyperparameterTuningJob\n",
      "HyperparameterTuningJob created. Resource name: projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880\n",
      "To use this HyperparameterTuningJob in another session:\n",
      "hpt_job = aiplatform.HyperparameterTuningJob.get('projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880')\n",
      "View HyperparameterTuningJob:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/8190989167367290880?project=1026793852137\n",
      "View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+1026793852137+locations+us-central1+tensorboards+3514619704511561728+experiments+8190989167367290880\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "HyperparameterTuningJob projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "HyperparameterTuningJob run completed. Resource name: projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880\n"
     ]
    }
   ],
   "source": [
    "tuningJob.run(\n",
    "    service_account = SERVICE_ACCOUNT,\n",
    "    tensorboard = tb.resource_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('projects/1026793852137/locations/us-central1/hyperparameterTuningJobs/8190989167367290880',\n",
       " '05i_fraud_20220824133413')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuningJob.resource_name, tuningJob.display_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create hyperlinks to job and tensorboard here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_link = f\"https://console.cloud.google.com/ai/platform/locations/{REGION}/training/{tuningJob.resource_name.split('/')[-1]}?project={PROJECT_ID}\"\n",
    "board_link = f\"https://{REGION}.tensorboard.googleusercontent.com/experiment/{tb.resource_name.replace('/', '+')}+experiments+{tuningJob.resource_name.split('/')[-1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review the Job here:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/8190989167367290880?project=statmike-mlops-349915\n",
      "Review the TensorBoard From the Job here:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+1026793852137+locations+us-central1+tensorboards+3514619704511561728+experiments+8190989167367290880\n",
      "Review the TensorBoard From the Job here (direct link to HPARAMS dashboard):\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+1026793852137+locations+us-central1+tensorboards+3514619704511561728+experiments+8190989167367290880/#hparams\n"
     ]
    }
   ],
   "source": [
    "print(f'Review the Job here:\\n{job_link}')\n",
    "print(f'Review the TensorBoard From the Job here:\\n{board_link}')\n",
    "print(f'Review the TensorBoard From the Job here (direct link to HPARAMS dashboard):\\n{board_link}/#hparams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Best Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9997153282165527,\n",
       " 0.9996100664138794,\n",
       " 0.9994343519210815,\n",
       " 0.9995657205581665,\n",
       " 0.9997324347496033,\n",
       " 0.9997726678848267,\n",
       " 0.9997557401657104,\n",
       " 0.9997502565383911,\n",
       " 0.9997791051864624,\n",
       " 0.9997440576553345]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if trial.state.name == 'SUCCEEDED'\n",
    "auprc = [trial.final_measurement.metrics[0].value if trial.state.name == 'SUCCEEDED' else 1 for trial in tuningJob.trials]\n",
    "auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id: \"9\"\n",
       "state: SUCCEEDED\n",
       "parameters {\n",
       "  parameter_id: \"lr\"\n",
       "  value {\n",
       "    number_value: 0.02811433627318535\n",
       "  }\n",
       "}\n",
       "parameters {\n",
       "  parameter_id: \"m\"\n",
       "  value {\n",
       "    number_value: 0.7739988558164979\n",
       "  }\n",
       "}\n",
       "final_measurement {\n",
       "  step_count: 1\n",
       "  metrics {\n",
       "    metric_id: \"auprc\"\n",
       "    value: 0.9997791051864624\n",
       "  }\n",
       "}\n",
       "start_time {\n",
       "  seconds: 1661365106\n",
       "  nanos: 934202735\n",
       "}\n",
       "end_time {\n",
       "  seconds: 1661365805\n",
       "}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = tuningJob.trials[auprc.index(max(auprc))]\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a new model, creating in model registry\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/1026793852137/locations/us-central1/models/model_05i_fraud/operations/5185829343816843264\n",
      "Model created. Resource name: projects/1026793852137/locations/us-central1/models/7033052515348250624@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/1026793852137/locations/us-central1/models/7033052515348250624@1')\n"
     ]
    }
   ],
   "source": [
    "modelmatch = aiplatform.Model.list(filter = f'labels.series={SERIES} AND labels.notebook={NOTEBOOK}')\n",
    "if modelmatch:\n",
    "    print(\"Model Already in Registry:\")\n",
    "    if f'{RUN_NAME}-{best.id}' in modelmatch[0].version_aliases:\n",
    "        print(\"This version already loaded, no action taken.\")\n",
    "        model = aiplatform.Model(model_name = modelmatch[0].resource_name)\n",
    "    else:\n",
    "        print('Loading model as new default version.')\n",
    "        model = aiplatform.Model.upload(\n",
    "            display_name = f'{NOTEBOOK}_{DATANAME}',\n",
    "            model_id = f'model_{NOTEBOOK}_{DATANAME}',\n",
    "            parent_model =  modelmatch[0].resource_name,\n",
    "            serving_container_image_uri = DEPLOY_IMAGE,\n",
    "            artifact_uri = f\"{URI}/{TIMESTAMP}/{best.id}/model\",\n",
    "            is_default_version = True,\n",
    "            version_aliases = [f'{RUN_NAME}-{best.id}'],\n",
    "            version_description = f'{RUN_NAME}-{best.id}',\n",
    "            labels = {'series' : f'{SERIES}', 'notebook' : f'{NOTEBOOK}', 'experiment_name' : f'{EXPERIMENT_NAME}', 'run_name' : f'{RUN_NAME}-{best.id}'}        \n",
    "        )\n",
    "else:\n",
    "    print('This is a new model, creating in model registry')\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name = f'{NOTEBOOK}_{DATANAME}',\n",
    "        model_id = f'model_{NOTEBOOK}_{DATANAME}',\n",
    "        serving_container_image_uri = DEPLOY_IMAGE,\n",
    "        artifact_uri = f\"{URI}/{TIMESTAMP}/{best.id}/model\",\n",
    "        is_default_version = True,\n",
    "        version_aliases = [f'{RUN_NAME}-{best.id}'],\n",
    "        version_description = f'{RUN_NAME}-{best.id}',\n",
    "        labels = {'series' : f'{SERIES}', 'notebook' : f'{NOTEBOOK}', 'experiment_name' : f'{EXPERIMENT_NAME}', 'run_name' : f'{RUN_NAME}-{best.id}'}\n",
    "    )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** on Version Aliases:\n",
    ">Expectation is a name starting with `a-z` that can include `[a-zA-Z0-9-]`\n",
    "\n",
    "**Retrieve a Model Resource**\n",
    "\n",
    "[Resource](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model)\n",
    "```Python\n",
    "model = aiplatform.Model(model_name = f'model_{NOTEBOOK}_{DATANAME}') # retrieves default version\n",
    "model = aiplatform.Model(model_name = f'model_{NOTEBOOK}_{DATANAME}@time-{TIMESTAMP}') # retrieves specific version\n",
    "model = aiplatform.Model(model_name = f'model_{NOTEBOOK}_{DATANAME}', version = f'time-{TIMESTAMP}') # retrieves specific version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex AI Experiment Update and Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "expRun = aiplatform.ExperimentRun(run_name = f'{RUN_NAME}-{best.id}', experiment = EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "expRun.log_params({\n",
    "    'model.display_name': model.display_name,\n",
    "    'model.versioned_resource_name': model.versioned_resource_name,\n",
    "    'hyperparameterTuningJobs.display_name': tuningJob.display_name,\n",
    "    'hyperparameterTuning.resource_name': tuningJob.resource_name,\n",
    "    'hyperparameterTuning.link': job_link,\n",
    "    'hyperparameterTuning.tensorboard': board_link\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "expRun.update_state(state = aiplatform.gapic.Execution.State.COMPLETE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to add the `hyperparameterTuning` job information to each run of the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial in tuningJob.trials:\n",
    "    expRun = aiplatform.ExperimentRun(run_name = f'{RUN_NAME}-{trial.id}', experiment = EXPERIMENT_NAME)\n",
    "    expRun.log_params({\n",
    "        'hyperparameterTuningJobs.display_name': tuningJob.display_name,\n",
    "        'hyperparameterTuning.resource_name': tuningJob.resource_name,\n",
    "        'hyperparameterTuning.link': job_link,\n",
    "        'hyperparameterTuning.tensorboard': board_link\n",
    "    })\n",
    "    expRun.update_state(state = aiplatform.gapic.Execution.State.COMPLETE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = aiplatform.Experiment(experiment_name = EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>run_type</th>\n",
       "      <th>state</th>\n",
       "      <th>param.hyperparameter.learning_rate</th>\n",
       "      <th>param.PROJECT_ID</th>\n",
       "      <th>param.epochs</th>\n",
       "      <th>param.hyperparameterTuning.link</th>\n",
       "      <th>param.PREFETCH</th>\n",
       "      <th>param.DATANAME</th>\n",
       "      <th>...</th>\n",
       "      <th>metric.test_auprc</th>\n",
       "      <th>metric.val_auprc</th>\n",
       "      <th>metric.train_loss</th>\n",
       "      <th>metric.val_accuracy</th>\n",
       "      <th>time_series_metric.train_loss</th>\n",
       "      <th>time_series_metric.val_loss</th>\n",
       "      <th>time_series_metric.val_auprc</th>\n",
       "      <th>time_series_metric.train_accuracy</th>\n",
       "      <th>time_series_metric.val_accuracy</th>\n",
       "      <th>time_series_metric.train_auprc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>experiment-05-05i-classification-dnn</td>\n",
       "      <td>run-20220824133413-10</td>\n",
       "      <td>system.ExperimentRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.019426</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>10.0</td>\n",
       "      <td>https://console.cloud.google.com/ai/platform/l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fraud</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999674</td>\n",
       "      <td>0.999577</td>\n",
       "      <td>0.003355</td>\n",
       "      <td>0.999221</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>0.999577</td>\n",
       "      <td>0.999452</td>\n",
       "      <td>0.999221</td>\n",
       "      <td>0.999744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>experiment-05-05i-classification-dnn</td>\n",
       "      <td>run-20220824133413-8</td>\n",
       "      <td>system.ExperimentRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.037255</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>10.0</td>\n",
       "      <td>https://console.cloud.google.com/ai/platform/l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fraud</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999674</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.003562</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.002655</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.999483</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.999750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>experiment-05-05i-classification-dnn</td>\n",
       "      <td>run-20220824133413-7</td>\n",
       "      <td>system.ExperimentRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.031686</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>10.0</td>\n",
       "      <td>https://console.cloud.google.com/ai/platform/l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fraud</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999673</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.003517</td>\n",
       "      <td>0.999256</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.005240</td>\n",
       "      <td>0.999623</td>\n",
       "      <td>0.999505</td>\n",
       "      <td>0.999256</td>\n",
       "      <td>0.999756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>experiment-05-05i-classification-dnn</td>\n",
       "      <td>run-20220824133413-6</td>\n",
       "      <td>system.ExperimentRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.068036</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>10.0</td>\n",
       "      <td>https://console.cloud.google.com/ai/platform/l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fraud</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>0.999762</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.999256</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>0.004317</td>\n",
       "      <td>0.999762</td>\n",
       "      <td>0.999408</td>\n",
       "      <td>0.999256</td>\n",
       "      <td>0.999773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>experiment-05-05i-classification-dnn</td>\n",
       "      <td>run-20220824133413-5</td>\n",
       "      <td>system.ExperimentRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.036201</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>10.0</td>\n",
       "      <td>https://console.cloud.google.com/ai/platform/l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fraud</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999628</td>\n",
       "      <td>0.999531</td>\n",
       "      <td>0.003324</td>\n",
       "      <td>0.999292</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.005286</td>\n",
       "      <td>0.999531</td>\n",
       "      <td>0.999478</td>\n",
       "      <td>0.999292</td>\n",
       "      <td>0.999732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>experiment-05-05i-classification-dnn</td>\n",
       "      <td>run-20220824133413-4</td>\n",
       "      <td>system.ExperimentRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>10.0</td>\n",
       "      <td>https://console.cloud.google.com/ai/platform/l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fraud</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>0.999395</td>\n",
       "      <td>0.004796</td>\n",
       "      <td>0.999221</td>\n",
       "      <td>0.004191</td>\n",
       "      <td>0.006119</td>\n",
       "      <td>0.999395</td>\n",
       "      <td>0.999290</td>\n",
       "      <td>0.999221</td>\n",
       "      <td>0.999566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>experiment-05-05i-classification-dnn</td>\n",
       "      <td>run-20220824133413-3</td>\n",
       "      <td>system.ExperimentRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>10.0</td>\n",
       "      <td>https://console.cloud.google.com/ai/platform/l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fraud</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>0.999405</td>\n",
       "      <td>0.007406</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>0.006495</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.999405</td>\n",
       "      <td>0.998935</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>0.999434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>experiment-05-05i-classification-dnn</td>\n",
       "      <td>run-20220824133413-2</td>\n",
       "      <td>system.ExperimentRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>10.0</td>\n",
       "      <td>https://console.cloud.google.com/ai/platform/l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fraud</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999671</td>\n",
       "      <td>0.999620</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.999221</td>\n",
       "      <td>0.003796</td>\n",
       "      <td>0.005568</td>\n",
       "      <td>0.999620</td>\n",
       "      <td>0.999355</td>\n",
       "      <td>0.999221</td>\n",
       "      <td>0.999610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>experiment-05-05i-classification-dnn</td>\n",
       "      <td>run-20220824133413-1</td>\n",
       "      <td>system.ExperimentRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>10.0</td>\n",
       "      <td>https://console.cloud.google.com/ai/platform/l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fraud</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999629</td>\n",
       "      <td>0.999624</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.999624</td>\n",
       "      <td>0.999412</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.999715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>experiment-05-05i-classification-dnn</td>\n",
       "      <td>run-20220824133413-9</td>\n",
       "      <td>system.ExperimentRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.028114</td>\n",
       "      <td>statmike-mlops-349915</td>\n",
       "      <td>10.0</td>\n",
       "      <td>https://console.cloud.google.com/ai/platform/l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>fraud</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999676</td>\n",
       "      <td>0.999671</td>\n",
       "      <td>0.003164</td>\n",
       "      <td>0.999292</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>0.004507</td>\n",
       "      <td>0.999671</td>\n",
       "      <td>0.999491</td>\n",
       "      <td>0.999292</td>\n",
       "      <td>0.999779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        experiment_name               run_name  \\\n",
       "0  experiment-05-05i-classification-dnn  run-20220824133413-10   \n",
       "1  experiment-05-05i-classification-dnn   run-20220824133413-8   \n",
       "2  experiment-05-05i-classification-dnn   run-20220824133413-7   \n",
       "3  experiment-05-05i-classification-dnn   run-20220824133413-6   \n",
       "4  experiment-05-05i-classification-dnn   run-20220824133413-5   \n",
       "5  experiment-05-05i-classification-dnn   run-20220824133413-4   \n",
       "6  experiment-05-05i-classification-dnn   run-20220824133413-3   \n",
       "7  experiment-05-05i-classification-dnn   run-20220824133413-2   \n",
       "8  experiment-05-05i-classification-dnn   run-20220824133413-1   \n",
       "9  experiment-05-05i-classification-dnn   run-20220824133413-9   \n",
       "\n",
       "               run_type     state  param.hyperparameter.learning_rate  \\\n",
       "0  system.ExperimentRun  COMPLETE                            0.019426   \n",
       "1  system.ExperimentRun  COMPLETE                            0.037255   \n",
       "2  system.ExperimentRun  COMPLETE                            0.031686   \n",
       "3  system.ExperimentRun  COMPLETE                            0.068036   \n",
       "4  system.ExperimentRun  COMPLETE                            0.036201   \n",
       "5  system.ExperimentRun  COMPLETE                            0.003988   \n",
       "6  system.ExperimentRun  COMPLETE                            0.001205   \n",
       "7  system.ExperimentRun  COMPLETE                            0.003634   \n",
       "8  system.ExperimentRun  COMPLETE                            0.010000   \n",
       "9  system.ExperimentRun  COMPLETE                            0.028114   \n",
       "\n",
       "        param.PROJECT_ID  param.epochs  \\\n",
       "0  statmike-mlops-349915          10.0   \n",
       "1  statmike-mlops-349915          10.0   \n",
       "2  statmike-mlops-349915          10.0   \n",
       "3  statmike-mlops-349915          10.0   \n",
       "4  statmike-mlops-349915          10.0   \n",
       "5  statmike-mlops-349915          10.0   \n",
       "6  statmike-mlops-349915          10.0   \n",
       "7  statmike-mlops-349915          10.0   \n",
       "8  statmike-mlops-349915          10.0   \n",
       "9  statmike-mlops-349915          10.0   \n",
       "\n",
       "                     param.hyperparameterTuning.link  param.PREFETCH  \\\n",
       "0  https://console.cloud.google.com/ai/platform/l...             1.0   \n",
       "1  https://console.cloud.google.com/ai/platform/l...             1.0   \n",
       "2  https://console.cloud.google.com/ai/platform/l...             1.0   \n",
       "3  https://console.cloud.google.com/ai/platform/l...             1.0   \n",
       "4  https://console.cloud.google.com/ai/platform/l...             1.0   \n",
       "5  https://console.cloud.google.com/ai/platform/l...             1.0   \n",
       "6  https://console.cloud.google.com/ai/platform/l...             1.0   \n",
       "7  https://console.cloud.google.com/ai/platform/l...             1.0   \n",
       "8  https://console.cloud.google.com/ai/platform/l...             1.0   \n",
       "9  https://console.cloud.google.com/ai/platform/l...             1.0   \n",
       "\n",
       "  param.DATANAME  ... metric.test_auprc  metric.val_auprc metric.train_loss  \\\n",
       "0          fraud  ...          0.999674          0.999577          0.003355   \n",
       "1          fraud  ...          0.999674          0.999623          0.003562   \n",
       "2          fraud  ...          0.999673          0.999623          0.003517   \n",
       "3          fraud  ...          0.999720          0.999762          0.002870   \n",
       "4          fraud  ...          0.999628          0.999531          0.003324   \n",
       "5          fraud  ...          0.999400          0.999395          0.004796   \n",
       "6          fraud  ...          0.999403          0.999405          0.007406   \n",
       "7          fraud  ...          0.999671          0.999620          0.004333   \n",
       "8          fraud  ...          0.999629          0.999624          0.003425   \n",
       "9          fraud  ...          0.999676          0.999671          0.003164   \n",
       "\n",
       "  metric.val_accuracy time_series_metric.train_loss  \\\n",
       "0            0.999221                      0.002788   \n",
       "1            0.999327                      0.002655   \n",
       "2            0.999256                      0.002652   \n",
       "3            0.999256                      0.002624   \n",
       "4            0.999292                      0.002791   \n",
       "5            0.999221                      0.004191   \n",
       "6            0.999009                      0.006495   \n",
       "7            0.999221                      0.003796   \n",
       "8            0.999327                      0.003024   \n",
       "9            0.999292                      0.002490   \n",
       "\n",
       "  time_series_metric.val_loss time_series_metric.val_auprc  \\\n",
       "0                    0.005233                     0.999577   \n",
       "1                    0.004548                     0.999623   \n",
       "2                    0.005240                     0.999623   \n",
       "3                    0.004317                     0.999762   \n",
       "4                    0.005286                     0.999531   \n",
       "5                    0.006119                     0.999395   \n",
       "6                    0.007752                     0.999405   \n",
       "7                    0.005568                     0.999620   \n",
       "8                    0.004140                     0.999624   \n",
       "9                    0.004507                     0.999671   \n",
       "\n",
       "   time_series_metric.train_accuracy  time_series_metric.val_accuracy  \\\n",
       "0                           0.999452                         0.999221   \n",
       "1                           0.999483                         0.999327   \n",
       "2                           0.999505                         0.999256   \n",
       "3                           0.999408                         0.999256   \n",
       "4                           0.999478                         0.999292   \n",
       "5                           0.999290                         0.999221   \n",
       "6                           0.998935                         0.999009   \n",
       "7                           0.999355                         0.999221   \n",
       "8                           0.999412                         0.999327   \n",
       "9                           0.999491                         0.999292   \n",
       "\n",
       "  time_series_metric.train_auprc  \n",
       "0                       0.999744  \n",
       "1                       0.999750  \n",
       "2                       0.999756  \n",
       "3                       0.999773  \n",
       "4                       0.999732  \n",
       "5                       0.999566  \n",
       "6                       0.999434  \n",
       "7                       0.999610  \n",
       "8                       0.999715  \n",
       "9                       0.999779  \n",
       "\n",
       "[10 rows x 40 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.get_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the Experiments TensorBoard to compare runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Experiment TensorBoard Link:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+1026793852137+locations+us-central1+tensorboards+3514619704511561728+experiments+experiment-05-05i-classification-dnn\n"
     ]
    }
   ],
   "source": [
    "print(f\"The Experiment TensorBoard Link:\\nhttps://{REGION}.tensorboard.googleusercontent.com/experiment/{tb.resource_name.replace('/', '+')}+experiments+{exp.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create An Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Exists: projects/1026793852137/locations/us-central1/endpoints/7252545822577917952\n"
     ]
    }
   ],
   "source": [
    "endpoints = aiplatform.Endpoint.list(filter = f\"labels.series={SERIES}\")\n",
    "if endpoints:\n",
    "    endpoint = endpoints[0]\n",
    "    print(f\"Endpoint Exists: {endpoints[0].resource_name}\")\n",
    "else:\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name = f\"{SERIES}_{DATANAME}\",\n",
    "        labels = {'notebook':f\"{SERIES}\"}    \n",
    "    )\n",
    "    print(f\"Endpoint Created: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05_fraud'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.display_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model To Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model with 100% of traffic...\n",
      "Deploying Model projects/1026793852137/locations/us-central1/models/model_05i_fraud to Endpoint : projects/1026793852137/locations/us-central1/endpoints/7252545822577917952\n",
      "Deploy Endpoint model backing LRO: projects/1026793852137/locations/us-central1/endpoints/7252545822577917952/operations/1743912921546620928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/proto/marshal/rules/enums.py:40: UserWarning: Unrecognized DeploymentResourcesType enum value: 3\n",
      "  value=value,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint model deployed. Resource name: projects/1026793852137/locations/us-central1/endpoints/7252545822577917952\n"
     ]
    }
   ],
   "source": [
    "dmodels = endpoint.list_models()\n",
    "\n",
    "check = 0\n",
    "if dmodels:\n",
    "    for dmodel in dmodels:\n",
    "        if dmodel.model == model.resource_name and dmodel.model_version_id == model.version_id and dmodel.id in endpoint.traffic_split:\n",
    "            print(f'This model (and version) already deployed with {endpoint.traffic_split[dmodel.id]}% of traffic')\n",
    "            check = 1\n",
    "    \n",
    "if check == 0:\n",
    "    print(f'Deploying model with 100% of traffic...')\n",
    "    endpoint.deploy(\n",
    "        model = model,\n",
    "        deployed_model_display_name = f'{NOTEBOOK}_{DATANAME}',\n",
    "        traffic_percentage = 100,\n",
    "        machine_type = DEPLOY_COMPUTE,\n",
    "        min_replica_count = 1,\n",
    "        max_replica_count = 1\n",
    "    )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Deployed Models without Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/1026793852137/locations/us-central1/endpoints/7252545822577917952\n",
      "Undeploy Endpoint model backing LRO: projects/1026793852137/locations/us-central1/endpoints/7252545822577917952/operations/280243042651209728\n",
      "Endpoint model undeployed. Resource name: projects/1026793852137/locations/us-central1/endpoints/7252545822577917952\n",
      "Undeployed 05h_fraud version 1 as it has no traffic.\n",
      "Model 05i_fraud has traffic = 100\n"
     ]
    }
   ],
   "source": [
    "for dmodel in endpoint.list_models():\n",
    "    if dmodel.id in endpoint.traffic_split:\n",
    "        print(f\"Model {dmodel.display_name} has traffic = {endpoint.traffic_split[dmodel.id]}\")\n",
    "    else:\n",
    "        endpoint.undeploy(deployed_model_id = dmodel.id)\n",
    "        print(f\"Undeployed {dmodel.display_name} version {dmodel.model_version_id} as it has no traffic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'7077164921854623744': 100}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.traffic_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: \"7077164921854623744\"\n",
       " model: \"projects/1026793852137/locations/us-central1/models/model_05i_fraud\"\n",
       " display_name: \"05i_fraud\"\n",
       " create_time {\n",
       "   seconds: 1659091859\n",
       "   nanos: 91291000\n",
       " }\n",
       " dedicated_resources {\n",
       "   machine_spec {\n",
       "     machine_type: \"n1-standard-4\"\n",
       "   }\n",
       "   min_replica_count: 1\n",
       "   max_replica_count: 1\n",
       " }\n",
       " model_version_id: \"1\"]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a record for prediction: instance and parameters lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = bigquery.query(query = f\"SELECT * FROM {DATANAME}.{DATANAME}_prepped WHERE splits='TEST' LIMIT 10\").to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32799</td>\n",
       "      <td>1.153477</td>\n",
       "      <td>-0.047859</td>\n",
       "      <td>1.358363</td>\n",
       "      <td>1.480620</td>\n",
       "      <td>-1.222598</td>\n",
       "      <td>-0.481690</td>\n",
       "      <td>-0.654461</td>\n",
       "      <td>0.128115</td>\n",
       "      <td>0.907095</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025964</td>\n",
       "      <td>0.701843</td>\n",
       "      <td>0.417245</td>\n",
       "      <td>-0.257691</td>\n",
       "      <td>0.060115</td>\n",
       "      <td>0.035332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>e9d16028-4b41-4753-87ee-041d33642ae9</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35483</td>\n",
       "      <td>1.286640</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.212182</td>\n",
       "      <td>-0.269732</td>\n",
       "      <td>-0.283961</td>\n",
       "      <td>-0.663306</td>\n",
       "      <td>-0.016385</td>\n",
       "      <td>-0.120297</td>\n",
       "      <td>-0.135962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052674</td>\n",
       "      <td>0.076792</td>\n",
       "      <td>0.209208</td>\n",
       "      <td>0.847617</td>\n",
       "      <td>-0.086559</td>\n",
       "      <td>-0.008262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8b319d3a-2b2d-445b-a9a2-0da3d664ec2a</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163935</td>\n",
       "      <td>1.961967</td>\n",
       "      <td>-0.247295</td>\n",
       "      <td>-1.751841</td>\n",
       "      <td>-0.268689</td>\n",
       "      <td>0.956431</td>\n",
       "      <td>0.707211</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>0.189433</td>\n",
       "      <td>0.455055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186420</td>\n",
       "      <td>-1.621368</td>\n",
       "      <td>-0.131098</td>\n",
       "      <td>0.034276</td>\n",
       "      <td>-0.004909</td>\n",
       "      <td>-0.090859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>788afb87-60aa-4482-8b48-c924bec634aa</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30707</td>\n",
       "      <td>-0.964364</td>\n",
       "      <td>0.176372</td>\n",
       "      <td>2.464128</td>\n",
       "      <td>2.672539</td>\n",
       "      <td>0.145676</td>\n",
       "      <td>-0.152913</td>\n",
       "      <td>-0.591983</td>\n",
       "      <td>0.305066</td>\n",
       "      <td>-0.148034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024200</td>\n",
       "      <td>0.365226</td>\n",
       "      <td>-0.745369</td>\n",
       "      <td>-0.060544</td>\n",
       "      <td>0.095692</td>\n",
       "      <td>0.217639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>473d0936-1974-4ae8-ab70-230e7599bd3f</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0   32799  1.153477 -0.047859  1.358363  1.480620 -1.222598 -0.481690   \n",
       "1   35483  1.286640  0.072917  0.212182 -0.269732 -0.283961 -0.663306   \n",
       "2  163935  1.961967 -0.247295 -1.751841 -0.268689  0.956431  0.707211   \n",
       "3   30707 -0.964364  0.176372  2.464128  2.672539  0.145676 -0.152913   \n",
       "\n",
       "         V7        V8        V9  ...       V23       V24       V25       V26  \\\n",
       "0 -0.654461  0.128115  0.907095  ... -0.025964  0.701843  0.417245 -0.257691   \n",
       "1 -0.016385 -0.120297 -0.135962  ...  0.052674  0.076792  0.209208  0.847617   \n",
       "2  0.020675  0.189433  0.455055  ...  0.186420 -1.621368 -0.131098  0.034276   \n",
       "3 -0.591983  0.305066 -0.148034  ... -0.024200  0.365226 -0.745369 -0.060544   \n",
       "\n",
       "        V27       V28  Amount  Class                        transaction_id  \\\n",
       "0  0.060115  0.035332     0.0      0  e9d16028-4b41-4753-87ee-041d33642ae9   \n",
       "1 -0.086559 -0.008262     0.0      0  8b319d3a-2b2d-445b-a9a2-0da3d664ec2a   \n",
       "2 -0.004909 -0.090859     0.0      0  788afb87-60aa-4482-8b48-c924bec634aa   \n",
       "3  0.095692  0.217639     0.0      0  473d0936-1974-4ae8-ab70-230e7599bd3f   \n",
       "\n",
       "   splits  \n",
       "0    TEST  \n",
       "1    TEST  \n",
       "2    TEST  \n",
       "3    TEST  \n",
       "\n",
       "[4 rows x 33 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "newob = pred[pred.columns[~pred.columns.isin(VAR_OMIT.split()+[VAR_TARGET, 'splits'])]].to_dict(orient='records')[0]\n",
    "#newob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [json_format.ParseDict(newob, Value())]\n",
    "parameters = json_format.ParseDict({}, Value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions: Python Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[[0.999275744, 0.000724321057]], deployed_model_id='7077164921854623744', model_version_id='', model_resource_name='projects/1026793852137/locations/us-central1/models/model_05i_fraud', explanations=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = endpoint.predict(instances=instances, parameters=parameters)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.999275744, 0.000724321057]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(prediction.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions: REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DIR}/request.json','w') as file:\n",
    "    file.write(json.dumps({\"instances\": [newob]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"predictions\": [\n",
      "    [\n",
      "      0.999275744,\n",
      "      0.000724321057\n",
      "    ]\n",
      "  ],\n",
      "  \"deployedModelId\": \"7077164921854623744\",\n",
      "  \"model\": \"projects/1026793852137/locations/us-central1/models/model_05i_fraud\",\n",
      "  \"modelDisplayName\": \"05i_fraud\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST \\\n",
    "-H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n",
    "-H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "-d @{DIR}/request.json \\\n",
    "https://{REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions: gcloud (CLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-prediction-aiplatform.googleapis.com/]\n",
      "[[0.999275744, 0.000724321057]]\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai endpoints predict {endpoint.name.rsplit('/',1)[-1]} --region={REGION} --json-request={DIR}/request.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Remove Resources\n",
    "see notebook \"99 - Cleanup\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

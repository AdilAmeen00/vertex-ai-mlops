{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "governmental-mother",
   "metadata": {},
   "source": [
    "# TFX on AI Platform - Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dressed-chart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.3.2\n",
      "TFX version: 0.28.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "import tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-reply",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "necessary-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "chemical-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = 'my-pipeline'\n",
    "\n",
    "# Make the Directory Structure for the Pipeline\n",
    "LOCAL_DIR = PIPELINE_NAME\n",
    "DATA_PATH = 'gs://statmike-mlops/digits/data'\n",
    "TRAINER_PATH = '{}/trainer'.format(LOCAL_DIR)\n",
    "PIPELINE_PATH = '{}/pipeline'.format(LOCAL_DIR)\n",
    "METADATA_PATH = '{}/metadata/metadata.db'.format(LOCAL_DIR)\n",
    "MODEL_PATH = '{}/model'.format(LOCAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "general-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {LOCAL_DIR}\n",
    "!mkdir {LOCAL_DIR}\n",
    "!mkdir {TRAINER_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "facial-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing my-pipeline/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINER_PATH}/task.py\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
    "parser.add_argument('--model-dir', dest='model_dir', default=os.getenv(\"AIP_MODEL_DIR\"), type=str, help='Model dir.')\n",
    "parser.add_argument('--epochs',dest='epochs', default=10, type=int, help='Number of Epochs')\n",
    "parser.add_argument('--batch_size',dest='batch_size', default=32, type=int, help='Batch Size')\n",
    "#parser.add_argument('',dest='', default=, type=, help='')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# built in parameters for data source:\n",
    "#PROJECT_ID='statmike-mlops'\n",
    "#BQDATASET_ID='digits'\n",
    "#BQTABLE_ID='digits_prepped'\n",
    "\n",
    "selected_fields = ['p0', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8', 'p9', 'p10', 'p11', 'p12', 'p13', 'p14', 'p15', 'p16', 'p17', 'p18', 'p19', 'p20', 'p21', 'p22', 'p23', 'p24', 'p25', 'p26', 'p27', 'p28', 'p29', 'p30', 'p31', 'p32', 'p33', 'p34', 'p35', 'p36', 'p37', 'p38', 'p39', 'p40', 'p41', 'p42', 'p43', 'p44', 'p45', 'p46', 'p47', 'p48', 'p49', 'p50', 'p51', 'p52', 'p53', 'p54', 'p55', 'p56', 'p57', 'p58', 'p59', 'p60', 'p61', 'p62', 'p63', 'target']\n",
    "output_types = ['FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'FLOAT64', 'INT64']\n",
    "\n",
    "feature_columns = []\n",
    "feature_layer_inputs = {}\n",
    "for header in selected_fields:\n",
    "    if header != 'target':\n",
    "        feature_columns.append(tf.feature_column.numeric_column(header))\n",
    "        feature_layer_inputs[header] = tf.keras.Input(shape=(1,),name=header)\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "output_types = [dtypes.float64 if x=='FLOAT64' else dtypes.int64 for x in output_types]\n",
    "\n",
    "def transTable(row_dict):\n",
    "    target=row_dict.pop('target')\n",
    "    target = tf.one_hot(tf.cast(target,tf.int64),10)\n",
    "    target = tf.cast(target,tf.float32)\n",
    "    return(row_dict,target)\n",
    "\n",
    "#client = BigQueryClient()\n",
    "#session = client.read_session(\"projects/\"+PROJECT_ID,PROJECT_ID,BQTABLE_ID,BQDATASET_ID,selected_fields,output_types,row_restriction=\"SPLITS='TRAIN'\",requested_streams=3)\n",
    "#table = session.parallel_read_rows()\n",
    "table = table.map(transTable)\n",
    "train = table.shuffle(100000).batch(args.batch_size)\n",
    "\n",
    "#client = BigQueryClient()\n",
    "#session = client.read_session(\"projects/\"+PROJECT_ID,PROJECT_ID,BQTABLE_ID,BQDATASET_ID,selected_fields,output_types,row_restriction=\"SPLITS='TEST'\",requested_streams=3)\n",
    "#table = session.parallel_read_rows()\n",
    "table = table.map(transTable)\n",
    "test = table.batch(args.batch_size)\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "feature_layer_outputs = feature_layer(feature_layer_inputs)\n",
    "model = tf.keras.Model(inputs=[v for v in feature_layer_inputs.values()],outputs=tf.keras.layers.Dense(10,activation=tf.nn.softmax)(feature_layer_outputs))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#tf.keras.utils.plot_model(model,show_shapes=True, show_dtype=True)\n",
    "\n",
    "history = model.fit(train,epochs=args.epochs)\n",
    "\n",
    "model.save(args.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import CsvExampleGen\n",
    "from tfx.extensions.google_cloud_big_query.example_gen import component as big_query_example_gen_component\n",
    "from tfx.components import Pusher\n",
    "from tfx.components import Trainer\n",
    "from tfx.components.trainer.executor import GenericExecutor\n",
    "from tfx.dsl.components.base import executor_spec\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.orchestration import pipeline\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.orchestration.local import local_dag_runner\n",
    "\n",
    "\n",
    "from typing import Optional, List, Text\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_CLOUD_REGION = 'us-central1'\n",
    "GOOGLE_CLOUD_PROJECT='statmike-mlops'\n",
    "GCS_BUCKET_NAME=GOOGLE_CLOUD_PROJECT\n",
    "BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS = [\n",
    "    '--project=' + GOOGLE_CLOUD_PROJECT,\n",
    "    '--temp_location=' + os.path.join('gs://', GCS_BUCKET_NAME, 'tmp'),\n",
    "    ]\n",
    "BIG_QUERY_QUERY = \"\"\"\n",
    "      SELECT * FROM `statmike-mlops.digits.digits_prepped`\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "going-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mypipeline(pipeline_name: str, pipeline_path: str, data_path: str, query: str,\n",
    "                     train_file: str, model_path: str,\n",
    "                     metadata_path: str, beam_pipeline_args: Optional[List[Text]] = None) -> pipeline.Pipeline:\n",
    "\n",
    "  # Brings data into the pipeline.\n",
    "  #example_gen = CsvExampleGen(input_base=data_path)\n",
    "  example_gen = big_query_example_gen_component.BigQueryExampleGen(query=query)\n",
    "    \n",
    "  # Uses user-provided Python function that trains a model.\n",
    "  trainer = Trainer(\n",
    "      module_file=train_file,\n",
    "      custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      train_args=trainer_pb2.TrainArgs(num_steps=100),\n",
    "      eval_args=trainer_pb2.EvalArgs(num_steps=5))\n",
    "\n",
    "  # Pushes the model to a filesystem destination.\n",
    "  pusher = Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      push_destination=pusher_pb2.PushDestination(\n",
    "          filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "              base_directory=model_path)))\n",
    "\n",
    "  # Following three components will be included in the pipeline.\n",
    "  components = [\n",
    "      example_gen,\n",
    "      #trainer,\n",
    "      #pusher,\n",
    "  ]\n",
    "\n",
    "  return pipeline.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_path,\n",
    "      metadata_connection_config=metadata.sqlite_metadata_connection_config(metadata_path),\n",
    "      components=components,\n",
    "      beam_pipeline_args=beam_pipeline_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "coated-caution",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Running pipeline:\n",
      " pipeline_info {\n",
      "  id: \"my-pipeline\"\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "      }\n",
      "      id: \"BigQueryExampleGen\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"my-pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"2021-04-15T12:15:12.374768\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"my-pipeline.BigQueryExampleGen\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Examples\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "              properties {\n",
      "                key: \"version\"\n",
      "                value: INT\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"input_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n      SELECT * FROM `statmike-mlops.digits.digits_prepped`\\\\n  \\\"\\n    }\\n  ]\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_data_format\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 6\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "runtime_spec {\n",
      "  pipeline_root {\n",
      "    field_value {\n",
      "      string_value: \"my-pipeline/pipeline\"\n",
      "    }\n",
      "  }\n",
      "  pipeline_run_id {\n",
      "    field_value {\n",
      "      string_value: \"2021-04-15T12:15:12.374768\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "execution_mode: SYNC\n",
      "deployment_config {\n",
      "  type_url: \"type.googleapis.com/tfx.orchestration.IntermediateDeploymentConfig\"\n",
      "  value: \"\\n\\363\\001\\n\\022BigQueryExampleGen\\022\\334\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\022\\210\\001\\nCtfx.extensions.google_cloud_big_query.example_gen.executor.Executor\\022\\030--project=statmike-mlops\\022\\'--temp_location=gs://statmike-mlops/tmp*Z\\n0type.googleapis.com/ml_metadata.ConnectionConfig\\022&\\032$\\n my-pipeline/metadata/metadata.db\\020\\003\"\n",
      "}\n",
      "\n",
      "INFO:absl:Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"BigQueryExampleGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.extensions.google_cloud_big_query.example_gen.executor.Executor\"\n",
      "      extra_flags: \"--project=statmike-mlops\"\n",
      "      extra_flags: \"--temp_location=gs://statmike-mlops/tmp\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  sqlite {\n",
      "    filename_uri: \"my-pipeline/metadata/metadata.db\"\n",
      "    connection_mode: READWRITE_OPENCREATE\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"my-pipeline/metadata/metadata.db\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "INFO:absl:Component BigQueryExampleGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "  }\n",
      "  id: \"BigQueryExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"my-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2021-04-15T12:15:12.374768\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"my-pipeline.BigQueryExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n      SELECT * FROM `statmike-mlops.digits.digits_prepped`\\\\n  \\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 1\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=1, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"my-pipeline/pipeline/BigQueryExampleGen/examples/1\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"my-pipeline:2021-04-15T12:15:12.374768:BigQueryExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      ")]}), exec_properties={'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'output_data_format': 6, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"\\\\n      SELECT * FROM `statmike-mlops.digits.digits_prepped`\\\\n  \"\\n    }\\n  ]\\n}'}, execution_output_uri='my-pipeline/pipeline/BigQueryExampleGen/.system/executor_execution/1/executor_output.pb', stateful_working_dir='my-pipeline/pipeline/BigQueryExampleGen/.system/stateful_working_dir/2021-04-15T12:15:12.374768', tmp_dir='my-pipeline/pipeline/BigQueryExampleGen/.system/executor_execution/1/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.extensions.google_cloud_big_query.example_gen.component.BigQueryExampleGen\"\n",
      "  }\n",
      "  id: \"BigQueryExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"my-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2021-04-15T12:15:12.374768\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"my-pipeline.BigQueryExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"\\\\n      SELECT * FROM `statmike-mlops.digits.digits_prepped`\\\\n  \\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"my-pipeline\"\n",
      ", pipeline_run_id='2021-04-15T12:15:12.374768')\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-8e2424ea-5ef1-4a2a-885e-bec76182bb80.json']\n",
      "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
      "INFO:absl:Copying all content from install dir /opt/conda/lib/python3.7/site-packages/tfx to temp dir /tmp/tmpbsxogagi/build/tfx\n",
      "INFO:absl:Generating a temp setup file at /tmp/tmpbsxogagi/build/tfx/setup.py\n",
      "INFO:absl:Creating temporary sdist package, logs available at /tmp/tmpbsxogagi/build/tfx/setup.log\n",
      "INFO:absl:Added --extra_package=/tmp/tmpbsxogagi/build/tfx/dist/tfx_ephemeral-0.28.0.tar.gz to beam args\n",
      "INFO:absl:Generating examples.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-8e2424ea-5ef1-4a2a-885e-bec76182bb80.json']\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset statmike-mlops:temp_dataset_b86247e60d8548208fc5c790ebf87b9a does not exist so we will create it as temporary with location=US\n",
      "INFO:absl:Examples generated.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 1 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"my-pipeline/pipeline/BigQueryExampleGen/examples/1\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"my-pipeline:2021-04-15T12:15:12.374768:BigQueryExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      ")]}) for exeuction 1\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component BigQueryExampleGen is finished.\n"
     ]
    }
   ],
   "source": [
    "local_dag_runner.LocalDagRunner().run(mypipeline(\n",
    "    pipeline_name=PIPELINE_NAME,\n",
    "    pipeline_path=PIPELINE_PATH,\n",
    "    data_path=DATA_PATH,\n",
    "    query = BIG_QUERY_QUERY,\n",
    "    train_file=TRAINER_PATH+'/task.py',\n",
    "    model_path=MODEL_PATH,\n",
    "    beam_pipeline_args=BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS,\n",
    "    metadata_path=METADATA_PATH\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-adaptation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-ranking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-plastic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-reviewer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-oxford",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

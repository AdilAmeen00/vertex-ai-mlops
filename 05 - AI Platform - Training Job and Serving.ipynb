{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "enhanced-cookbook",
   "metadata": {},
   "source": [
    "# AI Platform - Training Job and Serving\n",
    "\n",
    "Adopted From: https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/custom_job_image_classification_model_for_online_prediction.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-costume",
   "metadata": {},
   "source": [
    "## Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "PROJECT_ID='statmike-mlops'\n",
    "BUCKET_NAME='gs://statmike-models/digits/aip_train_job' #BUCKET_NAME\n",
    "JOB_NAME='AIP_DIGITS_1'\n",
    "\n",
    "PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION\n",
    "TRAIN_IMAGE='us-docker.pkg.dev/cloud-aiplatform/training/tf-cpu.2-4:latest'\n",
    "DEPLOY_IMAGE ='us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-3:latest'\n",
    "TRAIN_COMPUTE='n1-standard-4'\n",
    "DEPLOY_COMPUTE='n1-standard-4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "clients = {}\n",
    "clients['job'] = aiplatform.gapic.JobServiceClient(client_options=client_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-drink",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-content",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_DIR = '{}/{}'.format(BUCKET_NAME, JOB_NAME)\n",
    "\n",
    "CONTAINER_SPEC = {\n",
    "    \"image_uri\": TRAIN_IMAGE,\n",
    "    \"args\": [\n",
    "        \"--model-dir=\" + MODEL_DIR\n",
    "    ],\n",
    "}\n",
    "\n",
    "machine_spec = {\n",
    "        \"machine_type\": TRAIN_COMPUTE,\n",
    "        \"accelerator_count\": 0\n",
    "    }\n",
    "\n",
    "WORKER_POOL_SPEC = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"container_spec\": CONTAINER_SPEC,\n",
    "    }\n",
    "]\n",
    "\n",
    "CUSTOM_JOB = {\n",
    "    \"display_name\": JOB_NAME,\n",
    "    \"job_spec\": {\n",
    "        \"worker_pool_specs\": WORKER_POOL_SPEC,\n",
    "        \"base_output_directory\": {\"output_uri_prefix\": MODEL_DIR}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_job(custom_job):\n",
    "    response = clients['job'].create_custom_job(parent=PARENT, custom_job=CUSTOM_JOB)\n",
    "    print(\"name:\", response.name)\n",
    "    print(\"display_name:\", response.display_name)\n",
    "    print(\"state:\", response.state)\n",
    "    print(\"create_time:\", response.create_time)\n",
    "    print(\"update_time:\", response.update_time)\n",
    "    return response.name\n",
    "\n",
    "def list_custom_jobs():\n",
    "    response = clients['job'].list_custom_jobs(parent=PARENT)\n",
    "    for job in response:\n",
    "        print(response)\n",
    "        \n",
    "def get_custom_job(name, silent=False):\n",
    "    response = clients['job'].get_custom_job(name=name)\n",
    "    if silent:\n",
    "        return response\n",
    "\n",
    "    print(\"name:\", response.name)\n",
    "    print(\"display_name:\", response.display_name)\n",
    "    print(\"state:\", response.state)\n",
    "    print(\"create_time:\", response.create_time)\n",
    "    print(\"update_time:\", response.update_time)\n",
    "    return response\n",
    "\n",
    "def cancel_job(name):\n",
    "    try:\n",
    "        response = clients['job'].cancel_custom_job(name=name)\n",
    "        print(response)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-funds",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-fortune",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pleasant-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf custom\n",
    "!mkdir custom\n",
    "!mkdir custom/trainer\n",
    "!touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alleged-bathroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "\n",
    "PROJECT_ID='statmike-mlops'\n",
    "BQDATASET_ID='digits'\n",
    "BQTABLE_ID='digits_prepped'\n",
    "MODEL_DIR='gs://statmike-models/digits/keras'\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "bqclient = bigquery.Client()\n",
    "bqjob = bqclient.query(\n",
    "\"\"\"\n",
    "SELECT * FROM `\"\"\"+BQDATASET_ID+\"\"\".INFORMATION_SCHEMA.COLUMN_FIELD_PATHS`\n",
    "WHERE TABLE_NAME = '\"\"\"+BQTABLE_ID+\"\"\"' \"\"\"\n",
    ")\n",
    "schema = bqjob.result().to_dataframe()\n",
    "\n",
    "\n",
    "OMIT = ['target_OE','SPLITS']\n",
    "selected_fields=schema[~schema.column_name.isin(OMIT)].column_name.tolist()\n",
    "\n",
    "feature_columns = []\n",
    "feature_layer_inputs = {}\n",
    "for header in selected_fields:\n",
    "    if header != 'target':\n",
    "        feature_columns.append(tf.feature_column.numeric_column(header))\n",
    "        feature_layer_inputs[header] = tf.keras.Input(shape=(1,),name=header)\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "output_types=schema[~schema.column_name.isin(OMIT)].data_type.tolist()\n",
    "output_types = [dtypes.float64 if x=='FLOAT64' else dtypes.int64 for x in output_types]\n",
    "\n",
    "def transTable(row_dict):\n",
    "    target=row_dict.pop('target')\n",
    "    target = tf.one_hot(tf.cast(target,tf.int64),10)\n",
    "    target = tf.cast(target,tf.float32)\n",
    "    return(row_dict,target)\n",
    "\n",
    "client = BigQueryClient()\n",
    "session = client.read_session(\"projects/\"+PROJECT_ID,PROJECT_ID,BQTABLE_ID,BQDATASET_ID,selected_fields,output_types,row_restriction=\"SPLITS='TRAIN'\",requested_streams=3)\n",
    "table = session.parallel_read_rows()\n",
    "table = table.map(transTable)\n",
    "train = table.shuffle(100000).batch(BATCH_SIZE)\n",
    "\n",
    "client = BigQueryClient()\n",
    "session = client.read_session(\"projects/\"+PROJECT_ID,PROJECT_ID,BQTABLE_ID,BQDATASET_ID,selected_fields,output_types,row_restriction=\"SPLITS='TEST'\",requested_streams=3)\n",
    "table = session.parallel_read_rows()\n",
    "table = table.map(transTable)\n",
    "test = table.batch(BATCH_SIZE)\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "feature_layer_outputs = feature_layer(feature_layer_inputs)\n",
    "model = tf.keras.Model(inputs=[v for v in feature_layer_inputs.values()],outputs=tf.keras.layers.Dense(10,activation=tf.nn.softmax)(feature_layer_outputs))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "tf.keras.utils.plot_model(model,show_shapes=True, show_dtype=True)\n",
    "\n",
    "history = model.fit(train,epochs=25)\n",
    "\n",
    "model.save(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f custom.tar custom.tar.gz\n",
    "!tar cvf custom.tar custom\n",
    "!gzip custom.tar\n",
    "!gsutil cp custom.tar.gz gs://$BUCKET_NAME/trainer_cifar.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_ID = create_custom_job(CUSTOM_JOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_custom_job(JOB_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-office",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-donna",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
